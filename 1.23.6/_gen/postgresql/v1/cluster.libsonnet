{
  local d = (import 'doc-util/main.libsonnet'),
  '#':: d.pkg(name='cluster', url='', help='"Cluster is the Schema for the PostgreSQL API"'),
  '#metadata':: d.obj(help='"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create."'),
  metadata: {
    '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotations(annotations): { metadata+: { annotations: annotations } },
    '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotationsMixin(annotations): { metadata+: { annotations+: annotations } },
    '#withClusterName':: d.fn(help='"The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request."', args=[d.arg(name='clusterName', type=d.T.string)]),
    withClusterName(clusterName): { metadata+: { clusterName: clusterName } },
    '#withCreationTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='creationTimestamp', type=d.T.string)]),
    withCreationTimestamp(creationTimestamp): { metadata+: { creationTimestamp: creationTimestamp } },
    '#withDeletionGracePeriodSeconds':: d.fn(help='"Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."', args=[d.arg(name='deletionGracePeriodSeconds', type=d.T.integer)]),
    withDeletionGracePeriodSeconds(deletionGracePeriodSeconds): { metadata+: { deletionGracePeriodSeconds: deletionGracePeriodSeconds } },
    '#withDeletionTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='deletionTimestamp', type=d.T.string)]),
    withDeletionTimestamp(deletionTimestamp): { metadata+: { deletionTimestamp: deletionTimestamp } },
    '#withFinalizers':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizers(finalizers): { metadata+: { finalizers: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withFinalizersMixin':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizersMixin(finalizers): { metadata+: { finalizers+: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withGenerateName':: d.fn(help='"GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.\\n\\nIf this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).\\n\\nApplied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"', args=[d.arg(name='generateName', type=d.T.string)]),
    withGenerateName(generateName): { metadata+: { generateName: generateName } },
    '#withGeneration':: d.fn(help='"A sequence number representing a specific generation of the desired state. Populated by the system. Read-only."', args=[d.arg(name='generation', type=d.T.integer)]),
    withGeneration(generation): { metadata+: { generation: generation } },
    '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
    withLabels(labels): { metadata+: { labels: labels } },
    '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
    withLabelsMixin(labels): { metadata+: { labels+: labels } },
    '#withName':: d.fn(help='"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names"', args=[d.arg(name='name', type=d.T.string)]),
    withName(name): { metadata+: { name: name } },
    '#withNamespace':: d.fn(help='"Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the \\"default\\" namespace, but \\"default\\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.\\n\\nMust be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"', args=[d.arg(name='namespace', type=d.T.string)]),
    withNamespace(namespace): { metadata+: { namespace: namespace } },
    '#withOwnerReferences':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferences(ownerReferences): { metadata+: { ownerReferences: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withOwnerReferencesMixin':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferencesMixin(ownerReferences): { metadata+: { ownerReferences+: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withResourceVersion':: d.fn(help='"An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.\\n\\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"', args=[d.arg(name='resourceVersion', type=d.T.string)]),
    withResourceVersion(resourceVersion): { metadata+: { resourceVersion: resourceVersion } },
    '#withSelfLink':: d.fn(help='"SelfLink is a URL representing this object. Populated by the system. Read-only.\\n\\nDEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."', args=[d.arg(name='selfLink', type=d.T.string)]),
    withSelfLink(selfLink): { metadata+: { selfLink: selfLink } },
    '#withUid':: d.fn(help='"UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.\\n\\nPopulated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"', args=[d.arg(name='uid', type=d.T.string)]),
    withUid(uid): { metadata+: { uid: uid } },
  },
  '#new':: d.fn(help='new returns an instance of Cluster', args=[d.arg(name='name', type=d.T.string)]),
  new(name): {
    apiVersion: 'postgresql.cnpg.io/v1',
    kind: 'Cluster',
  } + self.metadata.withName(name=name),
  '#spec':: d.obj(help='"Specification of the desired behavior of the cluster.\\nMore info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"'),
  spec: {
    '#affinity':: d.obj(help='"Affinity/Anti-affinity rules for Pods"'),
    affinity: {
      '#additionalPodAffinity':: d.obj(help="\"AdditionalPodAffinity allows to specify pod affinity terms to be passed to all the cluster's pods.\""),
      additionalPodAffinity: {
        '#preferredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."'),
        preferredDuringSchedulingIgnoredDuringExecution: {
          '#podAffinityTerm':: d.obj(help='"Required. A pod affinity term, associated with the corresponding weight."'),
          podAffinityTerm: {
            '#labelSelector':: d.obj(help="\"A label query over a set of resources, in this case pods.\\nIf it's null, this PodAffinityTerm matches with no Pods.\""),
            labelSelector: {
              '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
              matchExpressions: {
                '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { key: key },
                '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
                withOperator(operator): { operator: operator },
                '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
                withValues(values): { values: if std.isArray(v=values) then values else [values] },
                '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
                withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
              },
              '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressions(matchExpressions): { podAffinityTerm+: { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressionsMixin(matchExpressions): { podAffinityTerm+: { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { podAffinityTerm+: { labelSelector+: { matchLabels: matchLabels } } },
              '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { podAffinityTerm+: { labelSelector+: { matchLabels+: matchLabels } } },
            },
            '#namespaceSelector':: d.obj(help="\"A label query over the set of namespaces that the term applies to.\\nThe term is applied to the union of the namespaces selected by this field\\nand the ones listed in the namespaces field.\\nnull selector and null or empty namespaces list means \\\"this pod's namespace\\\".\\nAn empty selector ({}) matches all namespaces.\""),
            namespaceSelector: {
              '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
              matchExpressions: {
                '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { key: key },
                '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
                withOperator(operator): { operator: operator },
                '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
                withValues(values): { values: if std.isArray(v=values) then values else [values] },
                '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
                withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
              },
              '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressions(matchExpressions): { podAffinityTerm+: { namespaceSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressionsMixin(matchExpressions): { podAffinityTerm+: { namespaceSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { podAffinityTerm+: { namespaceSelector+: { matchLabels: matchLabels } } },
              '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { podAffinityTerm+: { namespaceSelector+: { matchLabels+: matchLabels } } },
            },
            '#withMatchLabelKeys':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
            withMatchLabelKeys(matchLabelKeys): { podAffinityTerm+: { matchLabelKeys: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] } },
            '#withMatchLabelKeysMixin':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
            withMatchLabelKeysMixin(matchLabelKeys): { podAffinityTerm+: { matchLabelKeys+: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] } },
            '#withMismatchLabelKeys':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
            withMismatchLabelKeys(mismatchLabelKeys): { podAffinityTerm+: { mismatchLabelKeys: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] } },
            '#withMismatchLabelKeysMixin':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
            withMismatchLabelKeysMixin(mismatchLabelKeys): { podAffinityTerm+: { mismatchLabelKeys+: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] } },
            '#withNamespaces':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"", args=[d.arg(name='namespaces', type=d.T.array)]),
            withNamespaces(namespaces): { podAffinityTerm+: { namespaces: if std.isArray(v=namespaces) then namespaces else [namespaces] } },
            '#withNamespacesMixin':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='namespaces', type=d.T.array)]),
            withNamespacesMixin(namespaces): { podAffinityTerm+: { namespaces+: if std.isArray(v=namespaces) then namespaces else [namespaces] } },
            '#withTopologyKey':: d.fn(help='"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching\\nthe labelSelector in the specified namespaces, where co-located is defined as running on a node\\nwhose value of the label with key topologyKey matches that of any node on which any of the\\nselected pods is running.\\nEmpty topologyKey is not allowed."', args=[d.arg(name='topologyKey', type=d.T.string)]),
            withTopologyKey(topologyKey): { podAffinityTerm+: { topologyKey: topologyKey } },
          },
          '#withWeight':: d.fn(help='"weight associated with matching the corresponding podAffinityTerm,\\nin the range 1-100."', args=[d.arg(name='weight', type=d.T.integer)]),
          withWeight(weight): { weight: weight },
        },
        '#requiredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"If the affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."'),
        requiredDuringSchedulingIgnoredDuringExecution: {
          '#labelSelector':: d.obj(help="\"A label query over a set of resources, in this case pods.\\nIf it's null, this PodAffinityTerm matches with no Pods.\""),
          labelSelector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { labelSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { labelSelector+: { matchLabels+: matchLabels } },
          },
          '#namespaceSelector':: d.obj(help="\"A label query over the set of namespaces that the term applies to.\\nThe term is applied to the union of the namespaces selected by this field\\nand the ones listed in the namespaces field.\\nnull selector and null or empty namespaces list means \\\"this pod's namespace\\\".\\nAn empty selector ({}) matches all namespaces.\""),
          namespaceSelector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { namespaceSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { namespaceSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { namespaceSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { namespaceSelector+: { matchLabels+: matchLabels } },
          },
          '#withMatchLabelKeys':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
          withMatchLabelKeys(matchLabelKeys): { matchLabelKeys: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
          '#withMatchLabelKeysMixin':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
          withMatchLabelKeysMixin(matchLabelKeys): { matchLabelKeys+: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
          '#withMismatchLabelKeys':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
          withMismatchLabelKeys(mismatchLabelKeys): { mismatchLabelKeys: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] },
          '#withMismatchLabelKeysMixin':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
          withMismatchLabelKeysMixin(mismatchLabelKeys): { mismatchLabelKeys+: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] },
          '#withNamespaces':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"", args=[d.arg(name='namespaces', type=d.T.array)]),
          withNamespaces(namespaces): { namespaces: if std.isArray(v=namespaces) then namespaces else [namespaces] },
          '#withNamespacesMixin':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='namespaces', type=d.T.array)]),
          withNamespacesMixin(namespaces): { namespaces+: if std.isArray(v=namespaces) then namespaces else [namespaces] },
          '#withTopologyKey':: d.fn(help='"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching\\nthe labelSelector in the specified namespaces, where co-located is defined as running on a node\\nwhose value of the label with key topologyKey matches that of any node on which any of the\\nselected pods is running.\\nEmpty topologyKey is not allowed."', args=[d.arg(name='topologyKey', type=d.T.string)]),
          withTopologyKey(topologyKey): { topologyKey: topologyKey },
        },
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
      },
      '#additionalPodAntiAffinity':: d.obj(help='"AdditionalPodAntiAffinity allows to specify pod anti-affinity terms to be added to the ones generated\\nby the operator if EnablePodAntiAffinity is set to true (default) or to be used exclusively if set to false."'),
      additionalPodAntiAffinity: {
        '#preferredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe anti-affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling anti-affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."'),
        preferredDuringSchedulingIgnoredDuringExecution: {
          '#podAffinityTerm':: d.obj(help='"Required. A pod affinity term, associated with the corresponding weight."'),
          podAffinityTerm: {
            '#labelSelector':: d.obj(help="\"A label query over a set of resources, in this case pods.\\nIf it's null, this PodAffinityTerm matches with no Pods.\""),
            labelSelector: {
              '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
              matchExpressions: {
                '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { key: key },
                '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
                withOperator(operator): { operator: operator },
                '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
                withValues(values): { values: if std.isArray(v=values) then values else [values] },
                '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
                withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
              },
              '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressions(matchExpressions): { podAffinityTerm+: { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressionsMixin(matchExpressions): { podAffinityTerm+: { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { podAffinityTerm+: { labelSelector+: { matchLabels: matchLabels } } },
              '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { podAffinityTerm+: { labelSelector+: { matchLabels+: matchLabels } } },
            },
            '#namespaceSelector':: d.obj(help="\"A label query over the set of namespaces that the term applies to.\\nThe term is applied to the union of the namespaces selected by this field\\nand the ones listed in the namespaces field.\\nnull selector and null or empty namespaces list means \\\"this pod's namespace\\\".\\nAn empty selector ({}) matches all namespaces.\""),
            namespaceSelector: {
              '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
              matchExpressions: {
                '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
                withKey(key): { key: key },
                '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
                withOperator(operator): { operator: operator },
                '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
                withValues(values): { values: if std.isArray(v=values) then values else [values] },
                '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
                withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
              },
              '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressions(matchExpressions): { podAffinityTerm+: { namespaceSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
              withMatchExpressionsMixin(matchExpressions): { podAffinityTerm+: { namespaceSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
              '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabels(matchLabels): { podAffinityTerm+: { namespaceSelector+: { matchLabels: matchLabels } } },
              '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
              withMatchLabelsMixin(matchLabels): { podAffinityTerm+: { namespaceSelector+: { matchLabels+: matchLabels } } },
            },
            '#withMatchLabelKeys':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
            withMatchLabelKeys(matchLabelKeys): { podAffinityTerm+: { matchLabelKeys: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] } },
            '#withMatchLabelKeysMixin':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
            withMatchLabelKeysMixin(matchLabelKeys): { podAffinityTerm+: { matchLabelKeys+: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] } },
            '#withMismatchLabelKeys':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
            withMismatchLabelKeys(mismatchLabelKeys): { podAffinityTerm+: { mismatchLabelKeys: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] } },
            '#withMismatchLabelKeysMixin':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
            withMismatchLabelKeysMixin(mismatchLabelKeys): { podAffinityTerm+: { mismatchLabelKeys+: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] } },
            '#withNamespaces':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"", args=[d.arg(name='namespaces', type=d.T.array)]),
            withNamespaces(namespaces): { podAffinityTerm+: { namespaces: if std.isArray(v=namespaces) then namespaces else [namespaces] } },
            '#withNamespacesMixin':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='namespaces', type=d.T.array)]),
            withNamespacesMixin(namespaces): { podAffinityTerm+: { namespaces+: if std.isArray(v=namespaces) then namespaces else [namespaces] } },
            '#withTopologyKey':: d.fn(help='"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching\\nthe labelSelector in the specified namespaces, where co-located is defined as running on a node\\nwhose value of the label with key topologyKey matches that of any node on which any of the\\nselected pods is running.\\nEmpty topologyKey is not allowed."', args=[d.arg(name='topologyKey', type=d.T.string)]),
            withTopologyKey(topologyKey): { podAffinityTerm+: { topologyKey: topologyKey } },
          },
          '#withWeight':: d.fn(help='"weight associated with matching the corresponding podAffinityTerm,\\nin the range 1-100."', args=[d.arg(name='weight', type=d.T.integer)]),
          withWeight(weight): { weight: weight },
        },
        '#requiredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"If the anti-affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the anti-affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."'),
        requiredDuringSchedulingIgnoredDuringExecution: {
          '#labelSelector':: d.obj(help="\"A label query over a set of resources, in this case pods.\\nIf it's null, this PodAffinityTerm matches with no Pods.\""),
          labelSelector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { labelSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { labelSelector+: { matchLabels+: matchLabels } },
          },
          '#namespaceSelector':: d.obj(help="\"A label query over the set of namespaces that the term applies to.\\nThe term is applied to the union of the namespaces selected by this field\\nand the ones listed in the namespaces field.\\nnull selector and null or empty namespaces list means \\\"this pod's namespace\\\".\\nAn empty selector ({}) matches all namespaces.\""),
          namespaceSelector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { namespaceSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { namespaceSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { namespaceSelector+: { matchLabels: matchLabels } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { namespaceSelector+: { matchLabels+: matchLabels } },
          },
          '#withMatchLabelKeys':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
          withMatchLabelKeys(matchLabelKeys): { matchLabelKeys: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
          '#withMatchLabelKeysMixin':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key in (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both matchLabelKeys and labelSelector.\\nAlso, matchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
          withMatchLabelKeysMixin(matchLabelKeys): { matchLabelKeys+: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
          '#withMismatchLabelKeys':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
          withMismatchLabelKeys(mismatchLabelKeys): { mismatchLabelKeys: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] },
          '#withMismatchLabelKeysMixin':: d.fn(help="\"MismatchLabelKeys is a set of pod label keys to select which pods will\\nbe taken into consideration. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are merged with `labelSelector` as `key notin (value)`\\nto select the group of existing pods which pods will be taken into consideration\\nfor the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming\\npod labels will be ignored. The default value is empty.\\nThe same key is forbidden to exist in both mismatchLabelKeys and labelSelector.\\nAlso, mismatchLabelKeys cannot be set when labelSelector isn't set.\\nThis is a beta field and requires enabling MatchLabelKeysInPodAffinity feature gate (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='mismatchLabelKeys', type=d.T.array)]),
          withMismatchLabelKeysMixin(mismatchLabelKeys): { mismatchLabelKeys+: if std.isArray(v=mismatchLabelKeys) then mismatchLabelKeys else [mismatchLabelKeys] },
          '#withNamespaces':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"", args=[d.arg(name='namespaces', type=d.T.array)]),
          withNamespaces(namespaces): { namespaces: if std.isArray(v=namespaces) then namespaces else [namespaces] },
          '#withNamespacesMixin':: d.fn(help="\"namespaces specifies a static list of namespace names that the term applies to.\\nThe term is applied to the union of the namespaces listed in this field\\nand the ones selected by namespaceSelector.\\nnull or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='namespaces', type=d.T.array)]),
          withNamespacesMixin(namespaces): { namespaces+: if std.isArray(v=namespaces) then namespaces else [namespaces] },
          '#withTopologyKey':: d.fn(help='"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching\\nthe labelSelector in the specified namespaces, where co-located is defined as running on a node\\nwhose value of the label with key topologyKey matches that of any node on which any of the\\nselected pods is running.\\nEmpty topologyKey is not allowed."', args=[d.arg(name='topologyKey', type=d.T.string)]),
          withTopologyKey(topologyKey): { topologyKey: topologyKey },
        },
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe anti-affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling anti-affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe anti-affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling anti-affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the\\nnode(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAntiAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the anti-affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecution(requiredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withRequiredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"If the anti-affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the anti-affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to a pod label update), the\\nsystem may or may not try to eventually evict the pod from its node.\\nWhen there are multiple elements, the lists of nodes corresponding to each\\npodAffinityTerm are intersected, i.e. all terms must be satisfied."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requiredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withRequiredDuringSchedulingIgnoredDuringExecutionMixin(requiredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { additionalPodAntiAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=requiredDuringSchedulingIgnoredDuringExecution) then requiredDuringSchedulingIgnoredDuringExecution else [requiredDuringSchedulingIgnoredDuringExecution] } } } },
      },
      '#nodeAffinity':: d.obj(help='"NodeAffinity describes node affinity scheduling rules for the pod.\\nMore info: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity"'),
      nodeAffinity: {
        '#preferredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node matches the corresponding matchExpressions; the\\nnode(s) with the highest sum are the most preferred."'),
        preferredDuringSchedulingIgnoredDuringExecution: {
          '#preference':: d.obj(help='"A node selector term, associated with the corresponding weight."'),
          preference: {
            '#matchExpressions':: d.obj(help="\"A list of node selector requirements by node's labels.\""),
            matchExpressions: {
              '#withKey':: d.fn(help='"The label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"Represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#matchFields':: d.obj(help="\"A list of node selector requirements by node's fields.\""),
            matchFields: {
              '#withKey':: d.fn(help='"The label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"Represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help="\"A list of node selector requirements by node's labels.\"", args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { preference+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchExpressionsMixin':: d.fn(help="\"A list of node selector requirements by node's labels.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { preference+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
            '#withMatchFields':: d.fn(help="\"A list of node selector requirements by node's fields.\"", args=[d.arg(name='matchFields', type=d.T.array)]),
            withMatchFields(matchFields): { preference+: { matchFields: if std.isArray(v=matchFields) then matchFields else [matchFields] } },
            '#withMatchFieldsMixin':: d.fn(help="\"A list of node selector requirements by node's fields.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchFields', type=d.T.array)]),
            withMatchFieldsMixin(matchFields): { preference+: { matchFields+: if std.isArray(v=matchFields) then matchFields else [matchFields] } },
          },
          '#withWeight':: d.fn(help='"Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100."', args=[d.arg(name='weight', type=d.T.integer)]),
          withWeight(weight): { weight: weight },
        },
        '#requiredDuringSchedulingIgnoredDuringExecution':: d.obj(help='"If the affinity requirements specified by this field are not met at\\nscheduling time, the pod will not be scheduled onto the node.\\nIf the affinity requirements specified by this field cease to be met\\nat some point during pod execution (e.g. due to an update), the system\\nmay or may not try to eventually evict the pod from its node."'),
        requiredDuringSchedulingIgnoredDuringExecution: {
          '#nodeSelectorTerms':: d.obj(help='"Required. A list of node selector terms. The terms are ORed."'),
          nodeSelectorTerms: {
            '#matchExpressions':: d.obj(help="\"A list of node selector requirements by node's labels.\""),
            matchExpressions: {
              '#withKey':: d.fn(help='"The label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"Represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#matchFields':: d.obj(help="\"A list of node selector requirements by node's fields.\""),
            matchFields: {
              '#withKey':: d.fn(help='"The label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"Represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"An array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. If the operator is Gt or Lt, the values\\narray must have a single element, which will be interpreted as an integer.\\nThis array is replaced during a strategic merge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help="\"A list of node selector requirements by node's labels.\"", args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] },
            '#withMatchExpressionsMixin':: d.fn(help="\"A list of node selector requirements by node's labels.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] },
            '#withMatchFields':: d.fn(help="\"A list of node selector requirements by node's fields.\"", args=[d.arg(name='matchFields', type=d.T.array)]),
            withMatchFields(matchFields): { matchFields: if std.isArray(v=matchFields) then matchFields else [matchFields] },
            '#withMatchFieldsMixin':: d.fn(help="\"A list of node selector requirements by node's fields.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchFields', type=d.T.array)]),
            withMatchFieldsMixin(matchFields): { matchFields+: if std.isArray(v=matchFields) then matchFields else [matchFields] },
          },
          '#withNodeSelectorTerms':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
          withNodeSelectorTerms(nodeSelectorTerms): { spec+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } },
          '#withNodeSelectorTermsMixin':: d.fn(help='"Required. A list of node selector terms. The terms are ORed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelectorTerms', type=d.T.array)]),
          withNodeSelectorTermsMixin(nodeSelectorTerms): { spec+: { affinity+: { nodeAffinity+: { requiredDuringSchedulingIgnoredDuringExecution+: { nodeSelectorTerms+: if std.isArray(v=nodeSelectorTerms) then nodeSelectorTerms else [nodeSelectorTerms] } } } } },
        },
        '#withPreferredDuringSchedulingIgnoredDuringExecution':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node matches the corresponding matchExpressions; the\\nnode(s) with the highest sum are the most preferred."', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecution(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
        '#withPreferredDuringSchedulingIgnoredDuringExecutionMixin':: d.fn(help='"The scheduler will prefer to schedule pods to nodes that satisfy\\nthe affinity expressions specified by this field, but it may choose\\na node that violates one or more of the expressions. The node that is\\nmost preferred is the one with the greatest sum of weights, i.e.\\nfor each node that meets all of the scheduling requirements (resource\\nrequest, requiredDuringScheduling affinity expressions, etc.),\\ncompute a sum by iterating through the elements of this field and adding\\n\\"weight\\" to the sum if the node matches the corresponding matchExpressions; the\\nnode(s) with the highest sum are the most preferred."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='preferredDuringSchedulingIgnoredDuringExecution', type=d.T.array)]),
        withPreferredDuringSchedulingIgnoredDuringExecutionMixin(preferredDuringSchedulingIgnoredDuringExecution): { spec+: { affinity+: { nodeAffinity+: { preferredDuringSchedulingIgnoredDuringExecution+: if std.isArray(v=preferredDuringSchedulingIgnoredDuringExecution) then preferredDuringSchedulingIgnoredDuringExecution else [preferredDuringSchedulingIgnoredDuringExecution] } } } },
      },
      '#tolerations':: d.obj(help='"Tolerations is a list of Tolerations that should be set for all the pods, in order to allow them to run\\non tainted nodes.\\nMore info: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/"'),
      tolerations: {
        '#withEffect':: d.fn(help='"Effect indicates the taint effect to match. Empty means match all taint effects.\\nWhen specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute."', args=[d.arg(name='effect', type=d.T.string)]),
        withEffect(effect): { effect: effect },
        '#withKey':: d.fn(help='"Key is the taint key that the toleration applies to. Empty means match all taint keys.\\nIf the key is empty, operator must be Exists; this combination means to match all values and all keys."', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { key: key },
        '#withOperator':: d.fn(help="\"Operator represents a key's relationship to the value.\\nValid operators are Exists and Equal. Defaults to Equal.\\nExists is equivalent to wildcard for value, so that a pod can\\ntolerate all taints of a particular category.\"", args=[d.arg(name='operator', type=d.T.string)]),
        withOperator(operator): { operator: operator },
        '#withTolerationSeconds':: d.fn(help='"TolerationSeconds represents the period of time the toleration (which must be\\nof effect NoExecute, otherwise this field is ignored) tolerates the taint. By default,\\nit is not set, which means tolerate the taint forever (do not evict). Zero and\\nnegative values will be treated as 0 (evict immediately) by the system."', args=[d.arg(name='tolerationSeconds', type=d.T.integer)]),
        withTolerationSeconds(tolerationSeconds): { tolerationSeconds: tolerationSeconds },
        '#withValue':: d.fn(help='"Value is the taint value the toleration matches to.\\nIf the operator is Exists, the value should be empty, otherwise just a regular string."', args=[d.arg(name='value', type=d.T.string)]),
        withValue(value): { value: value },
      },
      '#withEnablePodAntiAffinity':: d.fn(help='"Activates anti-affinity for the pods. The operator will define pods\\nanti-affinity unless this field is explicitly set to false"', args=[d.arg(name='enablePodAntiAffinity', type=d.T.boolean)]),
      withEnablePodAntiAffinity(enablePodAntiAffinity): { spec+: { affinity+: { enablePodAntiAffinity: enablePodAntiAffinity } } },
      '#withNodeSelector':: d.fn(help='"NodeSelector is map of key-value pairs used to define the nodes on which\\nthe pods can run.\\nMore info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/"', args=[d.arg(name='nodeSelector', type=d.T.object)]),
      withNodeSelector(nodeSelector): { spec+: { affinity+: { nodeSelector: nodeSelector } } },
      '#withNodeSelectorMixin':: d.fn(help='"NodeSelector is map of key-value pairs used to define the nodes on which\\nthe pods can run.\\nMore info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeSelector', type=d.T.object)]),
      withNodeSelectorMixin(nodeSelector): { spec+: { affinity+: { nodeSelector+: nodeSelector } } },
      '#withPodAntiAffinityType':: d.fn(help="\"PodAntiAffinityType allows the user to decide whether pod anti-affinity between cluster instance has to be\\nconsidered a strong requirement during scheduling or not. Allowed values are: \\\"preferred\\\" (default if empty) or\\n\\\"required\\\". Setting it to \\\"required\\\", could lead to instances remaining pending until new kubernetes nodes are\\nadded if all the existing nodes don't match the required pod anti-affinity rule.\\nMore info:\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\"", args=[d.arg(name='podAntiAffinityType', type=d.T.string)]),
      withPodAntiAffinityType(podAntiAffinityType): { spec+: { affinity+: { podAntiAffinityType: podAntiAffinityType } } },
      '#withTolerations':: d.fn(help='"Tolerations is a list of Tolerations that should be set for all the pods, in order to allow them to run\\non tainted nodes.\\nMore info: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/"', args=[d.arg(name='tolerations', type=d.T.array)]),
      withTolerations(tolerations): { spec+: { affinity+: { tolerations: if std.isArray(v=tolerations) then tolerations else [tolerations] } } },
      '#withTolerationsMixin':: d.fn(help='"Tolerations is a list of Tolerations that should be set for all the pods, in order to allow them to run\\non tainted nodes.\\nMore info: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tolerations', type=d.T.array)]),
      withTolerationsMixin(tolerations): { spec+: { affinity+: { tolerations+: if std.isArray(v=tolerations) then tolerations else [tolerations] } } },
      '#withTopologyKey':: d.fn(help='"TopologyKey to use for anti-affinity configuration. See k8s documentation\\nfor more info on that"', args=[d.arg(name='topologyKey', type=d.T.string)]),
      withTopologyKey(topologyKey): { spec+: { affinity+: { topologyKey: topologyKey } } },
    },
    '#backup':: d.obj(help='"The configuration to be used for backups"'),
    backup: {
      '#barmanObjectStore':: d.obj(help='"The configuration for the barman-cloud tool suite"'),
      barmanObjectStore: {
        '#azureCredentials':: d.obj(help='"The credentials to use to upload data to Azure Blob Storage"'),
        azureCredentials: {
          '#connectionString':: d.obj(help='"The connection string to be used"'),
          connectionString: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { connectionString+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { connectionString+: { name: name } } } } } },
          },
          '#storageAccount':: d.obj(help='"The storage account where to upload data"'),
          storageAccount: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageAccount+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageAccount+: { name: name } } } } } },
          },
          '#storageKey':: d.obj(help='"The storage account key to be used in conjunction\\nwith the storage account name"'),
          storageKey: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageKey+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageKey+: { name: name } } } } } },
          },
          '#storageSasToken':: d.obj(help='"A shared-access-signature to be used in conjunction with\\nthe storage account name"'),
          storageSasToken: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageSasToken+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { storageSasToken+: { name: name } } } } } },
          },
          '#withInheritFromAzureAD':: d.fn(help='"Use the Azure AD based authentication without providing explicitly the keys."', args=[d.arg(name='inheritFromAzureAD', type=d.T.boolean)]),
          withInheritFromAzureAD(inheritFromAzureAD): { spec+: { backup+: { barmanObjectStore+: { azureCredentials+: { inheritFromAzureAD: inheritFromAzureAD } } } } },
        },
        '#data':: d.obj(help='"The configuration to be used to backup the data files\\nWhen not defined, base backups files will be stored uncompressed and may\\nbe unencrypted in the object store, according to the bucket default\\npolicy."'),
        data: {
          '#withAdditionalCommandArgs':: d.fn(help="\"AdditionalCommandArgs represents additional arguments that can be appended\\nto the 'barman-cloud-backup' command-line invocation. These arguments\\nprovide flexibility to customize the backup process further according to\\nspecific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-backup' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='additionalCommandArgs', type=d.T.array)]),
          withAdditionalCommandArgs(additionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { data+: { additionalCommandArgs: if std.isArray(v=additionalCommandArgs) then additionalCommandArgs else [additionalCommandArgs] } } } } },
          '#withAdditionalCommandArgsMixin':: d.fn(help="\"AdditionalCommandArgs represents additional arguments that can be appended\\nto the 'barman-cloud-backup' command-line invocation. These arguments\\nprovide flexibility to customize the backup process further according to\\nspecific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-backup' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='additionalCommandArgs', type=d.T.array)]),
          withAdditionalCommandArgsMixin(additionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { data+: { additionalCommandArgs+: if std.isArray(v=additionalCommandArgs) then additionalCommandArgs else [additionalCommandArgs] } } } } },
          '#withCompression':: d.fn(help='"Compress a backup file (a tar file per tablespace) while streaming it\\nto the object store. Available options are empty string (no\\ncompression, default), `gzip`, `bzip2` or `snappy`."', args=[d.arg(name='compression', type=d.T.string)]),
          withCompression(compression): { spec+: { backup+: { barmanObjectStore+: { data+: { compression: compression } } } } },
          '#withEncryption':: d.fn(help='"Whenever to force the encryption of files (if the bucket is\\nnot already configured for that).\\nAllowed options are empty string (use the bucket policy, default),\\n`AES256` and `aws:kms`"', args=[d.arg(name='encryption', type=d.T.string)]),
          withEncryption(encryption): { spec+: { backup+: { barmanObjectStore+: { data+: { encryption: encryption } } } } },
          '#withImmediateCheckpoint':: d.fn(help='"Control whether the I/O workload for the backup initial checkpoint will\\nbe limited, according to the `checkpoint_completion_target` setting on\\nthe PostgreSQL server. If set to true, an immediate checkpoint will be\\nused, meaning PostgreSQL will complete the checkpoint as soon as\\npossible. `false` by default."', args=[d.arg(name='immediateCheckpoint', type=d.T.boolean)]),
          withImmediateCheckpoint(immediateCheckpoint): { spec+: { backup+: { barmanObjectStore+: { data+: { immediateCheckpoint: immediateCheckpoint } } } } },
          '#withJobs':: d.fn(help='"The number of parallel jobs to be used to upload the backup, defaults\\nto 2"', args=[d.arg(name='jobs', type=d.T.integer)]),
          withJobs(jobs): { spec+: { backup+: { barmanObjectStore+: { data+: { jobs: jobs } } } } },
        },
        '#endpointCA':: d.obj(help='"EndpointCA store the CA bundle of the barman endpoint.\\nUseful when using self-signed certificates to avoid\\nerrors with certificate issuer and barman-cloud-wal-archive"'),
        endpointCA: {
          '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { spec+: { backup+: { barmanObjectStore+: { endpointCA+: { key: key } } } } },
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { backup+: { barmanObjectStore+: { endpointCA+: { name: name } } } } },
        },
        '#googleCredentials':: d.obj(help='"The credentials to use to upload data to Google Cloud Storage"'),
        googleCredentials: {
          '#applicationCredentials':: d.obj(help='"The secret containing the Google Cloud Storage JSON file with the credentials"'),
          applicationCredentials: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { googleCredentials+: { applicationCredentials+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { googleCredentials+: { applicationCredentials+: { name: name } } } } } },
          },
          '#withGkeEnvironment':: d.fn(help="\"If set to true, will presume that it's running inside a GKE environment,\\ndefault to false.\"", args=[d.arg(name='gkeEnvironment', type=d.T.boolean)]),
          withGkeEnvironment(gkeEnvironment): { spec+: { backup+: { barmanObjectStore+: { googleCredentials+: { gkeEnvironment: gkeEnvironment } } } } },
        },
        '#s3Credentials':: d.obj(help='"The credentials to use to upload data to S3"'),
        s3Credentials: {
          '#accessKeyId':: d.obj(help='"The reference to the access key id"'),
          accessKeyId: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { accessKeyId+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { accessKeyId+: { name: name } } } } } },
          },
          '#region':: d.obj(help='"The reference to the secret containing the region name"'),
          region: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { region+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { region+: { name: name } } } } } },
          },
          '#secretAccessKey':: d.obj(help='"The reference to the secret access key"'),
          secretAccessKey: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { secretAccessKey+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { secretAccessKey+: { name: name } } } } } },
          },
          '#sessionToken':: d.obj(help='"The references to the session key"'),
          sessionToken: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { sessionToken+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { sessionToken+: { name: name } } } } } },
          },
          '#withInheritFromIAMRole':: d.fn(help='"Use the role based authentication without providing explicitly the keys."', args=[d.arg(name='inheritFromIAMRole', type=d.T.boolean)]),
          withInheritFromIAMRole(inheritFromIAMRole): { spec+: { backup+: { barmanObjectStore+: { s3Credentials+: { inheritFromIAMRole: inheritFromIAMRole } } } } },
        },
        '#wal':: d.obj(help='"The configuration for the backup of the WAL stream.\\nWhen not defined, WAL files will be stored uncompressed and may be\\nunencrypted in the object store, according to the bucket default policy."'),
        wal: {
          '#withArchiveAdditionalCommandArgs':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-archive'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL archive process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-archive' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='archiveAdditionalCommandArgs', type=d.T.array)]),
          withArchiveAdditionalCommandArgs(archiveAdditionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { wal+: { archiveAdditionalCommandArgs: if std.isArray(v=archiveAdditionalCommandArgs) then archiveAdditionalCommandArgs else [archiveAdditionalCommandArgs] } } } } },
          '#withArchiveAdditionalCommandArgsMixin':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-archive'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL archive process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-archive' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='archiveAdditionalCommandArgs', type=d.T.array)]),
          withArchiveAdditionalCommandArgsMixin(archiveAdditionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { wal+: { archiveAdditionalCommandArgs+: if std.isArray(v=archiveAdditionalCommandArgs) then archiveAdditionalCommandArgs else [archiveAdditionalCommandArgs] } } } } },
          '#withCompression':: d.fn(help='"Compress a WAL file before sending it to the object store. Available\\noptions are empty string (no compression, default), `gzip`, `bzip2` or `snappy`."', args=[d.arg(name='compression', type=d.T.string)]),
          withCompression(compression): { spec+: { backup+: { barmanObjectStore+: { wal+: { compression: compression } } } } },
          '#withEncryption':: d.fn(help='"Whenever to force the encryption of files (if the bucket is\\nnot already configured for that).\\nAllowed options are empty string (use the bucket policy, default),\\n`AES256` and `aws:kms`"', args=[d.arg(name='encryption', type=d.T.string)]),
          withEncryption(encryption): { spec+: { backup+: { barmanObjectStore+: { wal+: { encryption: encryption } } } } },
          '#withMaxParallel':: d.fn(help='"Number of WAL files to be either archived in parallel (when the\\nPostgreSQL instance is archiving to a backup object store) or\\nrestored in parallel (when a PostgreSQL standby is fetching WAL\\nfiles from a recovery object store). If not specified, WAL files\\nwill be processed one at a time. It accepts a positive integer as a\\nvalue - with 1 being the minimum accepted value."', args=[d.arg(name='maxParallel', type=d.T.integer)]),
          withMaxParallel(maxParallel): { spec+: { backup+: { barmanObjectStore+: { wal+: { maxParallel: maxParallel } } } } },
          '#withRestoreAdditionalCommandArgs':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-restore'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL restore process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-restore' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='restoreAdditionalCommandArgs', type=d.T.array)]),
          withRestoreAdditionalCommandArgs(restoreAdditionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { wal+: { restoreAdditionalCommandArgs: if std.isArray(v=restoreAdditionalCommandArgs) then restoreAdditionalCommandArgs else [restoreAdditionalCommandArgs] } } } } },
          '#withRestoreAdditionalCommandArgsMixin':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-restore'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL restore process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-restore' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='restoreAdditionalCommandArgs', type=d.T.array)]),
          withRestoreAdditionalCommandArgsMixin(restoreAdditionalCommandArgs): { spec+: { backup+: { barmanObjectStore+: { wal+: { restoreAdditionalCommandArgs+: if std.isArray(v=restoreAdditionalCommandArgs) then restoreAdditionalCommandArgs else [restoreAdditionalCommandArgs] } } } } },
        },
        '#withDestinationPath':: d.fn(help='"The path where to store the backup (i.e. s3://bucket/path/to/folder)\\nthis path, with different destination folders, will be used for WALs\\nand for data"', args=[d.arg(name='destinationPath', type=d.T.string)]),
        withDestinationPath(destinationPath): { spec+: { backup+: { barmanObjectStore+: { destinationPath: destinationPath } } } },
        '#withEndpointURL':: d.fn(help='"Endpoint to be used to upload data to the cloud,\\noverriding the automatic endpoint discovery"', args=[d.arg(name='endpointURL', type=d.T.string)]),
        withEndpointURL(endpointURL): { spec+: { backup+: { barmanObjectStore+: { endpointURL: endpointURL } } } },
        '#withHistoryTags':: d.fn(help='"HistoryTags is a list of key value pairs that will be passed to the\\nBarman --history-tags option."', args=[d.arg(name='historyTags', type=d.T.object)]),
        withHistoryTags(historyTags): { spec+: { backup+: { barmanObjectStore+: { historyTags: historyTags } } } },
        '#withHistoryTagsMixin':: d.fn(help='"HistoryTags is a list of key value pairs that will be passed to the\\nBarman --history-tags option."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='historyTags', type=d.T.object)]),
        withHistoryTagsMixin(historyTags): { spec+: { backup+: { barmanObjectStore+: { historyTags+: historyTags } } } },
        '#withServerName':: d.fn(help='"The server name on S3, the cluster name is used if this\\nparameter is omitted"', args=[d.arg(name='serverName', type=d.T.string)]),
        withServerName(serverName): { spec+: { backup+: { barmanObjectStore+: { serverName: serverName } } } },
        '#withTags':: d.fn(help='"Tags is a list of key value pairs that will be passed to the\\nBarman --tags option."', args=[d.arg(name='tags', type=d.T.object)]),
        withTags(tags): { spec+: { backup+: { barmanObjectStore+: { tags: tags } } } },
        '#withTagsMixin':: d.fn(help='"Tags is a list of key value pairs that will be passed to the\\nBarman --tags option."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
        withTagsMixin(tags): { spec+: { backup+: { barmanObjectStore+: { tags+: tags } } } },
      },
      '#volumeSnapshot':: d.obj(help='"VolumeSnapshot provides the configuration for the execution of volume snapshot backups."'),
      volumeSnapshot: {
        '#onlineConfiguration':: d.obj(help='"Configuration parameters to control the online/hot backup with volume snapshots"'),
        onlineConfiguration: {
          '#withImmediateCheckpoint':: d.fn(help='"Control whether the I/O workload for the backup initial checkpoint will\\nbe limited, according to the `checkpoint_completion_target` setting on\\nthe PostgreSQL server. If set to true, an immediate checkpoint will be\\nused, meaning PostgreSQL will complete the checkpoint as soon as\\npossible. `false` by default."', args=[d.arg(name='immediateCheckpoint', type=d.T.boolean)]),
          withImmediateCheckpoint(immediateCheckpoint): { spec+: { backup+: { volumeSnapshot+: { onlineConfiguration+: { immediateCheckpoint: immediateCheckpoint } } } } },
          '#withWaitForArchive':: d.fn(help='"If false, the function will return immediately after the backup is completed,\\nwithout waiting for WAL to be archived.\\nThis behavior is only useful with backup software that independently monitors WAL archiving.\\nOtherwise, WAL required to make the backup consistent might be missing and make the backup useless.\\nBy default, or when this parameter is true, pg_backup_stop will wait for WAL to be archived when archiving is\\nenabled.\\nOn a standby, this means that it will wait only when archive_mode = always.\\nIf write activity on the primary is low, it may be useful to run pg_switch_wal on the primary in order to trigger\\nan immediate segment switch."', args=[d.arg(name='waitForArchive', type=d.T.boolean)]),
          withWaitForArchive(waitForArchive): { spec+: { backup+: { volumeSnapshot+: { onlineConfiguration+: { waitForArchive: waitForArchive } } } } },
        },
        '#withAnnotations':: d.fn(help='"Annotations key-value pairs that will be added to .metadata.annotations snapshot resources."', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotations(annotations): { spec+: { backup+: { volumeSnapshot+: { annotations: annotations } } } },
        '#withAnnotationsMixin':: d.fn(help='"Annotations key-value pairs that will be added to .metadata.annotations snapshot resources."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotationsMixin(annotations): { spec+: { backup+: { volumeSnapshot+: { annotations+: annotations } } } },
        '#withClassName':: d.fn(help='"ClassName specifies the Snapshot Class to be used for PG_DATA PersistentVolumeClaim.\\nIt is the default class for the other types if no specific class is present"', args=[d.arg(name='className', type=d.T.string)]),
        withClassName(className): { spec+: { backup+: { volumeSnapshot+: { className: className } } } },
        '#withLabels':: d.fn(help='"Labels are key-value pairs that will be added to .metadata.labels snapshot resources."', args=[d.arg(name='labels', type=d.T.object)]),
        withLabels(labels): { spec+: { backup+: { volumeSnapshot+: { labels: labels } } } },
        '#withLabelsMixin':: d.fn(help='"Labels are key-value pairs that will be added to .metadata.labels snapshot resources."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
        withLabelsMixin(labels): { spec+: { backup+: { volumeSnapshot+: { labels+: labels } } } },
        '#withOnline':: d.fn(help='"Whether the default type of backup with volume snapshots is\\nonline/hot (`true`, default) or offline/cold (`false`)"', args=[d.arg(name='online', type=d.T.boolean)]),
        withOnline(online): { spec+: { backup+: { volumeSnapshot+: { online: online } } } },
        '#withSnapshotOwnerReference':: d.fn(help='"SnapshotOwnerReference indicates the type of owner reference the snapshot should have"', args=[d.arg(name='snapshotOwnerReference', type=d.T.string)]),
        withSnapshotOwnerReference(snapshotOwnerReference): { spec+: { backup+: { volumeSnapshot+: { snapshotOwnerReference: snapshotOwnerReference } } } },
        '#withTablespaceClassName':: d.fn(help='"TablespaceClassName specifies the Snapshot Class to be used for the tablespaces.\\ndefaults to the PGDATA Snapshot Class, if set"', args=[d.arg(name='tablespaceClassName', type=d.T.object)]),
        withTablespaceClassName(tablespaceClassName): { spec+: { backup+: { volumeSnapshot+: { tablespaceClassName: tablespaceClassName } } } },
        '#withTablespaceClassNameMixin':: d.fn(help='"TablespaceClassName specifies the Snapshot Class to be used for the tablespaces.\\ndefaults to the PGDATA Snapshot Class, if set"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tablespaceClassName', type=d.T.object)]),
        withTablespaceClassNameMixin(tablespaceClassName): { spec+: { backup+: { volumeSnapshot+: { tablespaceClassName+: tablespaceClassName } } } },
        '#withWalClassName':: d.fn(help='"WalClassName specifies the Snapshot Class to be used for the PG_WAL PersistentVolumeClaim."', args=[d.arg(name='walClassName', type=d.T.string)]),
        withWalClassName(walClassName): { spec+: { backup+: { volumeSnapshot+: { walClassName: walClassName } } } },
      },
      '#withRetentionPolicy':: d.fn(help="\"RetentionPolicy is the retention policy to be used for backups\\nand WALs (i.e. '60d'). The retention policy is expressed in the form\\nof `XXu` where `XX` is a positive integer and `u` is in `[dwm]` -\\ndays, weeks, months.\\nIt's currently only applicable when using the BarmanObjectStore method.\"", args=[d.arg(name='retentionPolicy', type=d.T.string)]),
      withRetentionPolicy(retentionPolicy): { spec+: { backup+: { retentionPolicy: retentionPolicy } } },
      '#withTarget':: d.fn(help='"The policy to decide which instance should perform backups. Available\\noptions are empty string, which will default to `prefer-standby` policy,\\n`primary` to have backups run always on primary instances, `prefer-standby`\\nto have backups run preferably on the most updated standby, if available."', args=[d.arg(name='target', type=d.T.string)]),
      withTarget(target): { spec+: { backup+: { target: target } } },
    },
    '#bootstrap':: d.obj(help='"Instructions to bootstrap this cluster"'),
    bootstrap: {
      '#initdb':: d.obj(help='"Bootstrap the cluster via initdb"'),
      initdb: {
        '#import':: d.obj(help='"Bootstraps the new cluster by importing data from an existing PostgreSQL\\ninstance using logical backup (`pg_dump` and `pg_restore`)"'),
        'import': {
          '#source':: d.obj(help='"The source of the import"'),
          source: {
            '#withExternalCluster':: d.fn(help='"The name of the externalCluster used for import"', args=[d.arg(name='externalCluster', type=d.T.string)]),
            withExternalCluster(externalCluster): { spec+: { bootstrap+: { initdb+: { 'import'+: { source+: { externalCluster: externalCluster } } } } } },
          },
          '#withDatabases':: d.fn(help='"The databases to import"', args=[d.arg(name='databases', type=d.T.array)]),
          withDatabases(databases): { spec+: { bootstrap+: { initdb+: { 'import'+: { databases: if std.isArray(v=databases) then databases else [databases] } } } } },
          '#withDatabasesMixin':: d.fn(help='"The databases to import"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='databases', type=d.T.array)]),
          withDatabasesMixin(databases): { spec+: { bootstrap+: { initdb+: { 'import'+: { databases+: if std.isArray(v=databases) then databases else [databases] } } } } },
          '#withPgDumpExtraOptions':: d.fn(help="\"List of custom options to pass to the `pg_dump` command. IMPORTANT:\\nUse these options with caution and at your own risk, as the operator\\ndoes not validate their content. Be aware that certain options may\\nconflict with the operator's intended functionality or design.\"", args=[d.arg(name='pgDumpExtraOptions', type=d.T.array)]),
          withPgDumpExtraOptions(pgDumpExtraOptions): { spec+: { bootstrap+: { initdb+: { 'import'+: { pgDumpExtraOptions: if std.isArray(v=pgDumpExtraOptions) then pgDumpExtraOptions else [pgDumpExtraOptions] } } } } },
          '#withPgDumpExtraOptionsMixin':: d.fn(help="\"List of custom options to pass to the `pg_dump` command. IMPORTANT:\\nUse these options with caution and at your own risk, as the operator\\ndoes not validate their content. Be aware that certain options may\\nconflict with the operator's intended functionality or design.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='pgDumpExtraOptions', type=d.T.array)]),
          withPgDumpExtraOptionsMixin(pgDumpExtraOptions): { spec+: { bootstrap+: { initdb+: { 'import'+: { pgDumpExtraOptions+: if std.isArray(v=pgDumpExtraOptions) then pgDumpExtraOptions else [pgDumpExtraOptions] } } } } },
          '#withPgRestoreExtraOptions':: d.fn(help="\"List of custom options to pass to the `pg_restore` command. IMPORTANT:\\nUse these options with caution and at your own risk, as the operator\\ndoes not validate their content. Be aware that certain options may\\nconflict with the operator's intended functionality or design.\"", args=[d.arg(name='pgRestoreExtraOptions', type=d.T.array)]),
          withPgRestoreExtraOptions(pgRestoreExtraOptions): { spec+: { bootstrap+: { initdb+: { 'import'+: { pgRestoreExtraOptions: if std.isArray(v=pgRestoreExtraOptions) then pgRestoreExtraOptions else [pgRestoreExtraOptions] } } } } },
          '#withPgRestoreExtraOptionsMixin':: d.fn(help="\"List of custom options to pass to the `pg_restore` command. IMPORTANT:\\nUse these options with caution and at your own risk, as the operator\\ndoes not validate their content. Be aware that certain options may\\nconflict with the operator's intended functionality or design.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='pgRestoreExtraOptions', type=d.T.array)]),
          withPgRestoreExtraOptionsMixin(pgRestoreExtraOptions): { spec+: { bootstrap+: { initdb+: { 'import'+: { pgRestoreExtraOptions+: if std.isArray(v=pgRestoreExtraOptions) then pgRestoreExtraOptions else [pgRestoreExtraOptions] } } } } },
          '#withPostImportApplicationSQL':: d.fn(help='"List of SQL queries to be executed as a superuser in the application\\ndatabase right after is imported - to be used with extreme care\\n(by default empty). Only available in microservice type."', args=[d.arg(name='postImportApplicationSQL', type=d.T.array)]),
          withPostImportApplicationSQL(postImportApplicationSQL): { spec+: { bootstrap+: { initdb+: { 'import'+: { postImportApplicationSQL: if std.isArray(v=postImportApplicationSQL) then postImportApplicationSQL else [postImportApplicationSQL] } } } } },
          '#withPostImportApplicationSQLMixin':: d.fn(help='"List of SQL queries to be executed as a superuser in the application\\ndatabase right after is imported - to be used with extreme care\\n(by default empty). Only available in microservice type."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='postImportApplicationSQL', type=d.T.array)]),
          withPostImportApplicationSQLMixin(postImportApplicationSQL): { spec+: { bootstrap+: { initdb+: { 'import'+: { postImportApplicationSQL+: if std.isArray(v=postImportApplicationSQL) then postImportApplicationSQL else [postImportApplicationSQL] } } } } },
          '#withRoles':: d.fn(help='"The roles to import"', args=[d.arg(name='roles', type=d.T.array)]),
          withRoles(roles): { spec+: { bootstrap+: { initdb+: { 'import'+: { roles: if std.isArray(v=roles) then roles else [roles] } } } } },
          '#withRolesMixin':: d.fn(help='"The roles to import"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='roles', type=d.T.array)]),
          withRolesMixin(roles): { spec+: { bootstrap+: { initdb+: { 'import'+: { roles+: if std.isArray(v=roles) then roles else [roles] } } } } },
          '#withSchemaOnly':: d.fn(help='"When set to true, only the `pre-data` and `post-data` sections of\\n`pg_restore` are invoked, avoiding data import. Default: `false`."', args=[d.arg(name='schemaOnly', type=d.T.boolean)]),
          withSchemaOnly(schemaOnly): { spec+: { bootstrap+: { initdb+: { 'import'+: { schemaOnly: schemaOnly } } } } },
          '#withType':: d.fn(help='"The import type. Can be `microservice` or `monolith`."', args=[d.arg(name='type', type=d.T.string)]),
          withType(type): { spec+: { bootstrap+: { initdb+: { 'import'+: { type: type } } } } },
        },
        '#postInitApplicationSQLRefs':: d.obj(help='"List of references to ConfigMaps or Secrets containing SQL files\\nto be executed as a superuser in the application database right after\\nthe cluster has been created. The references are processed in a specific order:\\nfirst, all Secrets are processed, followed by all ConfigMaps.\\nWithin each group, the processing order follows the sequence specified\\nin their respective arrays.\\n(by default empty)"'),
        postInitApplicationSQLRefs: {
          '#configMapRefs':: d.obj(help='"ConfigMapRefs holds a list of references to ConfigMaps"'),
          configMapRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#secretRefs':: d.obj(help='"SecretRefs holds a list of references to Secrets"'),
          secretRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#withConfigMapRefs':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefs(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQLRefs+: { configMapRefs: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withConfigMapRefsMixin':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefsMixin(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQLRefs+: { configMapRefs+: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withSecretRefs':: d.fn(help='"SecretRefs holds a list of references to Secrets"', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefs(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQLRefs+: { secretRefs: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
          '#withSecretRefsMixin':: d.fn(help='"SecretRefs holds a list of references to Secrets"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefsMixin(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQLRefs+: { secretRefs+: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
        },
        '#postInitSQLRefs':: d.obj(help='"List of references to ConfigMaps or Secrets containing SQL files\\nto be executed as a superuser in the `postgres` database right after\\nthe cluster has been created. The references are processed in a specific order:\\nfirst, all Secrets are processed, followed by all ConfigMaps.\\nWithin each group, the processing order follows the sequence specified\\nin their respective arrays.\\n(by default empty)"'),
        postInitSQLRefs: {
          '#configMapRefs':: d.obj(help='"ConfigMapRefs holds a list of references to ConfigMaps"'),
          configMapRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#secretRefs':: d.obj(help='"SecretRefs holds a list of references to Secrets"'),
          secretRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#withConfigMapRefs':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefs(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitSQLRefs+: { configMapRefs: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withConfigMapRefsMixin':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefsMixin(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitSQLRefs+: { configMapRefs+: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withSecretRefs':: d.fn(help='"SecretRefs holds a list of references to Secrets"', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefs(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitSQLRefs+: { secretRefs: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
          '#withSecretRefsMixin':: d.fn(help='"SecretRefs holds a list of references to Secrets"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefsMixin(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitSQLRefs+: { secretRefs+: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
        },
        '#postInitTemplateSQLRefs':: d.obj(help='"List of references to ConfigMaps or Secrets containing SQL files\\nto be executed as a superuser in the `template1` database right after\\nthe cluster has been created. The references are processed in a specific order:\\nfirst, all Secrets are processed, followed by all ConfigMaps.\\nWithin each group, the processing order follows the sequence specified\\nin their respective arrays.\\n(by default empty)"'),
        postInitTemplateSQLRefs: {
          '#configMapRefs':: d.obj(help='"ConfigMapRefs holds a list of references to ConfigMaps"'),
          configMapRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#secretRefs':: d.obj(help='"SecretRefs holds a list of references to Secrets"'),
          secretRefs: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
          },
          '#withConfigMapRefs':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefs(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQLRefs+: { configMapRefs: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withConfigMapRefsMixin':: d.fn(help='"ConfigMapRefs holds a list of references to ConfigMaps"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='configMapRefs', type=d.T.array)]),
          withConfigMapRefsMixin(configMapRefs): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQLRefs+: { configMapRefs+: if std.isArray(v=configMapRefs) then configMapRefs else [configMapRefs] } } } } },
          '#withSecretRefs':: d.fn(help='"SecretRefs holds a list of references to Secrets"', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefs(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQLRefs+: { secretRefs: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
          '#withSecretRefsMixin':: d.fn(help='"SecretRefs holds a list of references to Secrets"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='secretRefs', type=d.T.array)]),
          withSecretRefsMixin(secretRefs): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQLRefs+: { secretRefs+: if std.isArray(v=secretRefs) then secretRefs else [secretRefs] } } } } },
        },
        '#secret':: d.obj(help='"Name of the secret containing the initial credentials for the\\nowner of the user database. If empty a new secret will be\\ncreated from scratch"'),
        secret: {
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { bootstrap+: { initdb+: { secret+: { name: name } } } } },
        },
        '#withBuiltinLocale':: d.fn(help='"Specifies the locale name when the builtin provider is used.\\nThis option requires `localeProvider` to be set to `builtin`.\\nAvailable from PostgreSQL 17."', args=[d.arg(name='builtinLocale', type=d.T.string)]),
        withBuiltinLocale(builtinLocale): { spec+: { bootstrap+: { initdb+: { builtinLocale: builtinLocale } } } },
        '#withDataChecksums':: d.fn(help='"Whether the `-k` option should be passed to initdb,\\nenabling checksums on data pages (default: `false`)"', args=[d.arg(name='dataChecksums', type=d.T.boolean)]),
        withDataChecksums(dataChecksums): { spec+: { bootstrap+: { initdb+: { dataChecksums: dataChecksums } } } },
        '#withDatabase':: d.fn(help='"Name of the database used by the application. Default: `app`."', args=[d.arg(name='database', type=d.T.string)]),
        withDatabase(database): { spec+: { bootstrap+: { initdb+: { database: database } } } },
        '#withEncoding':: d.fn(help='"The value to be passed as option `--encoding` for initdb (default:`UTF8`)"', args=[d.arg(name='encoding', type=d.T.string)]),
        withEncoding(encoding): { spec+: { bootstrap+: { initdb+: { encoding: encoding } } } },
        '#withIcuLocale':: d.fn(help='"Specifies the ICU locale when the ICU provider is used.\\nThis option requires `localeProvider` to be set to `icu`.\\nAvailable from PostgreSQL 15."', args=[d.arg(name='icuLocale', type=d.T.string)]),
        withIcuLocale(icuLocale): { spec+: { bootstrap+: { initdb+: { icuLocale: icuLocale } } } },
        '#withIcuRules':: d.fn(help='"Specifies additional collation rules to customize the behavior of the default collation.\\nThis option requires `localeProvider` to be set to `icu`.\\nAvailable from PostgreSQL 16."', args=[d.arg(name='icuRules', type=d.T.string)]),
        withIcuRules(icuRules): { spec+: { bootstrap+: { initdb+: { icuRules: icuRules } } } },
        '#withLocale':: d.fn(help='"Sets the default collation order and character classification in the new database."', args=[d.arg(name='locale', type=d.T.string)]),
        withLocale(locale): { spec+: { bootstrap+: { initdb+: { locale: locale } } } },
        '#withLocaleCType':: d.fn(help='"The value to be passed as option `--lc-ctype` for initdb (default:`C`)"', args=[d.arg(name='localeCType', type=d.T.string)]),
        withLocaleCType(localeCType): { spec+: { bootstrap+: { initdb+: { localeCType: localeCType } } } },
        '#withLocaleCollate':: d.fn(help='"The value to be passed as option `--lc-collate` for initdb (default:`C`)"', args=[d.arg(name='localeCollate', type=d.T.string)]),
        withLocaleCollate(localeCollate): { spec+: { bootstrap+: { initdb+: { localeCollate: localeCollate } } } },
        '#withLocaleProvider':: d.fn(help='"This option sets the locale provider for databases created in the new cluster.\\nAvailable from PostgreSQL 16."', args=[d.arg(name='localeProvider', type=d.T.string)]),
        withLocaleProvider(localeProvider): { spec+: { bootstrap+: { initdb+: { localeProvider: localeProvider } } } },
        '#withOptions':: d.fn(help='"The list of options that must be passed to initdb when creating the cluster.\\nDeprecated: This could lead to inconsistent configurations,\\nplease use the explicit provided parameters instead.\\nIf defined, explicit values will be ignored."', args=[d.arg(name='options', type=d.T.array)]),
        withOptions(options): { spec+: { bootstrap+: { initdb+: { options: if std.isArray(v=options) then options else [options] } } } },
        '#withOptionsMixin':: d.fn(help='"The list of options that must be passed to initdb when creating the cluster.\\nDeprecated: This could lead to inconsistent configurations,\\nplease use the explicit provided parameters instead.\\nIf defined, explicit values will be ignored."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='options', type=d.T.array)]),
        withOptionsMixin(options): { spec+: { bootstrap+: { initdb+: { options+: if std.isArray(v=options) then options else [options] } } } },
        '#withOwner':: d.fn(help='"Name of the owner of the database in the instance to be used\\nby applications. Defaults to the value of the `database` key."', args=[d.arg(name='owner', type=d.T.string)]),
        withOwner(owner): { spec+: { bootstrap+: { initdb+: { owner: owner } } } },
        '#withPostInitApplicationSQL':: d.fn(help='"List of SQL queries to be executed as a superuser in the application\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"', args=[d.arg(name='postInitApplicationSQL', type=d.T.array)]),
        withPostInitApplicationSQL(postInitApplicationSQL): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQL: if std.isArray(v=postInitApplicationSQL) then postInitApplicationSQL else [postInitApplicationSQL] } } } },
        '#withPostInitApplicationSQLMixin':: d.fn(help='"List of SQL queries to be executed as a superuser in the application\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='postInitApplicationSQL', type=d.T.array)]),
        withPostInitApplicationSQLMixin(postInitApplicationSQL): { spec+: { bootstrap+: { initdb+: { postInitApplicationSQL+: if std.isArray(v=postInitApplicationSQL) then postInitApplicationSQL else [postInitApplicationSQL] } } } },
        '#withPostInitSQL':: d.fn(help='"List of SQL queries to be executed as a superuser in the `postgres`\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"', args=[d.arg(name='postInitSQL', type=d.T.array)]),
        withPostInitSQL(postInitSQL): { spec+: { bootstrap+: { initdb+: { postInitSQL: if std.isArray(v=postInitSQL) then postInitSQL else [postInitSQL] } } } },
        '#withPostInitSQLMixin':: d.fn(help='"List of SQL queries to be executed as a superuser in the `postgres`\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='postInitSQL', type=d.T.array)]),
        withPostInitSQLMixin(postInitSQL): { spec+: { bootstrap+: { initdb+: { postInitSQL+: if std.isArray(v=postInitSQL) then postInitSQL else [postInitSQL] } } } },
        '#withPostInitTemplateSQL':: d.fn(help='"List of SQL queries to be executed as a superuser in the `template1`\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"', args=[d.arg(name='postInitTemplateSQL', type=d.T.array)]),
        withPostInitTemplateSQL(postInitTemplateSQL): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQL: if std.isArray(v=postInitTemplateSQL) then postInitTemplateSQL else [postInitTemplateSQL] } } } },
        '#withPostInitTemplateSQLMixin':: d.fn(help='"List of SQL queries to be executed as a superuser in the `template1`\\ndatabase right after the cluster has been created - to be used with extreme care\\n(by default empty)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='postInitTemplateSQL', type=d.T.array)]),
        withPostInitTemplateSQLMixin(postInitTemplateSQL): { spec+: { bootstrap+: { initdb+: { postInitTemplateSQL+: if std.isArray(v=postInitTemplateSQL) then postInitTemplateSQL else [postInitTemplateSQL] } } } },
        '#withWalSegmentSize':: d.fn(help='"The value in megabytes (1 to 1024) to be passed to the `--wal-segsize`\\noption for initdb (default: empty, resulting in PostgreSQL default: 16MB)"', args=[d.arg(name='walSegmentSize', type=d.T.integer)]),
        withWalSegmentSize(walSegmentSize): { spec+: { bootstrap+: { initdb+: { walSegmentSize: walSegmentSize } } } },
      },
      '#pg_basebackup':: d.obj(help='"Bootstrap the cluster taking a physical backup of another compatible\\nPostgreSQL instance"'),
      pg_basebackup: {
        '#secret':: d.obj(help='"Name of the secret containing the initial credentials for the\\nowner of the user database. If empty a new secret will be\\ncreated from scratch"'),
        secret: {
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { bootstrap+: { pg_basebackup+: { secret+: { name: name } } } } },
        },
        '#withDatabase':: d.fn(help='"Name of the database used by the application. Default: `app`."', args=[d.arg(name='database', type=d.T.string)]),
        withDatabase(database): { spec+: { bootstrap+: { pg_basebackup+: { database: database } } } },
        '#withOwner':: d.fn(help='"Name of the owner of the database in the instance to be used\\nby applications. Defaults to the value of the `database` key."', args=[d.arg(name='owner', type=d.T.string)]),
        withOwner(owner): { spec+: { bootstrap+: { pg_basebackup+: { owner: owner } } } },
        '#withSource':: d.fn(help='"The name of the server of which we need to take a physical backup"', args=[d.arg(name='source', type=d.T.string)]),
        withSource(source): { spec+: { bootstrap+: { pg_basebackup+: { source: source } } } },
      },
      '#recovery':: d.obj(help='"Bootstrap the cluster from a backup"'),
      recovery: {
        '#backup':: d.obj(help='"The backup object containing the physical base backup from which to\\ninitiate the recovery procedure.\\nMutually exclusive with `source` and `volumeSnapshots`."'),
        backup: {
          '#endpointCA':: d.obj(help='"EndpointCA store the CA bundle of the barman endpoint.\\nUseful when using self-signed certificates to avoid\\nerrors with certificate issuer and barman-cloud-wal-archive."'),
          endpointCA: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { bootstrap+: { recovery+: { backup+: { endpointCA+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { bootstrap+: { recovery+: { backup+: { endpointCA+: { name: name } } } } } },
          },
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { bootstrap+: { recovery+: { backup+: { name: name } } } } },
        },
        '#recoveryTarget':: d.obj(help='"By default, the recovery process applies all the available\\nWAL files in the archive (full recovery). However, you can also\\nend the recovery as soon as a consistent state is reached or\\nrecover to a point-in-time (PITR) by specifying a `RecoveryTarget` object,\\nas expected by PostgreSQL (i.e., timestamp, transaction Id, LSN, ...).\\nMore info: https://www.postgresql.org/docs/current/runtime-config-wal.html#RUNTIME-CONFIG-WAL-RECOVERY-TARGET"'),
        recoveryTarget: {
          '#withBackupID':: d.fn(help='"The ID of the backup from which to start the recovery process.\\nIf empty (default) the operator will automatically detect the backup\\nbased on targetTime or targetLSN if specified. Otherwise use the\\nlatest available backup in chronological order."', args=[d.arg(name='backupID', type=d.T.string)]),
          withBackupID(backupID): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { backupID: backupID } } } } },
          '#withExclusive':: d.fn(help='"Set the target to be exclusive. If omitted, defaults to false, so that\\nin Postgres, `recovery_target_inclusive` will be true"', args=[d.arg(name='exclusive', type=d.T.boolean)]),
          withExclusive(exclusive): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { exclusive: exclusive } } } } },
          '#withTargetImmediate':: d.fn(help='"End recovery as soon as a consistent state is reached"', args=[d.arg(name='targetImmediate', type=d.T.boolean)]),
          withTargetImmediate(targetImmediate): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetImmediate: targetImmediate } } } } },
          '#withTargetLSN':: d.fn(help='"The target LSN (Log Sequence Number)"', args=[d.arg(name='targetLSN', type=d.T.string)]),
          withTargetLSN(targetLSN): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetLSN: targetLSN } } } } },
          '#withTargetName':: d.fn(help='"The target name (to be previously created\\nwith `pg_create_restore_point`)"', args=[d.arg(name='targetName', type=d.T.string)]),
          withTargetName(targetName): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetName: targetName } } } } },
          '#withTargetTLI':: d.fn(help='"The target timeline (\\"latest\\" or a positive integer)"', args=[d.arg(name='targetTLI', type=d.T.string)]),
          withTargetTLI(targetTLI): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetTLI: targetTLI } } } } },
          '#withTargetTime':: d.fn(help='"The target time as a timestamp in the RFC3339 standard"', args=[d.arg(name='targetTime', type=d.T.string)]),
          withTargetTime(targetTime): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetTime: targetTime } } } } },
          '#withTargetXID':: d.fn(help='"The target transaction ID"', args=[d.arg(name='targetXID', type=d.T.string)]),
          withTargetXID(targetXID): { spec+: { bootstrap+: { recovery+: { recoveryTarget+: { targetXID: targetXID } } } } },
        },
        '#secret':: d.obj(help='"Name of the secret containing the initial credentials for the\\nowner of the user database. If empty a new secret will be\\ncreated from scratch"'),
        secret: {
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { bootstrap+: { recovery+: { secret+: { name: name } } } } },
        },
        '#volumeSnapshots':: d.obj(help='"The static PVC data source(s) from which to initiate the\\nrecovery procedure. Currently supporting `VolumeSnapshot`\\nand `PersistentVolumeClaim` resources that map an existing\\nPVC group, compatible with CloudNativePG, and taken with\\na cold backup copy on a fenced Postgres instance (limitation\\nwhich will be removed in the future when online backup\\nwill be implemented).\\nMutually exclusive with `backup`."'),
        volumeSnapshots: {
          '#storage':: d.obj(help='"Configuration of the storage of the instances"'),
          storage: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { storage+: { apiGroup: apiGroup } } } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { storage+: { kind: kind } } } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { storage+: { name: name } } } } } },
          },
          '#walStorage':: d.obj(help='"Configuration of the storage for PostgreSQL WAL (Write-Ahead Log)"'),
          walStorage: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { walStorage+: { apiGroup: apiGroup } } } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { walStorage+: { kind: kind } } } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { walStorage+: { name: name } } } } } },
          },
          '#withTablespaceStorage':: d.fn(help='"Configuration of the storage for PostgreSQL tablespaces"', args=[d.arg(name='tablespaceStorage', type=d.T.object)]),
          withTablespaceStorage(tablespaceStorage): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { tablespaceStorage: tablespaceStorage } } } } },
          '#withTablespaceStorageMixin':: d.fn(help='"Configuration of the storage for PostgreSQL tablespaces"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tablespaceStorage', type=d.T.object)]),
          withTablespaceStorageMixin(tablespaceStorage): { spec+: { bootstrap+: { recovery+: { volumeSnapshots+: { tablespaceStorage+: tablespaceStorage } } } } },
        },
        '#withDatabase':: d.fn(help='"Name of the database used by the application. Default: `app`."', args=[d.arg(name='database', type=d.T.string)]),
        withDatabase(database): { spec+: { bootstrap+: { recovery+: { database: database } } } },
        '#withOwner':: d.fn(help='"Name of the owner of the database in the instance to be used\\nby applications. Defaults to the value of the `database` key."', args=[d.arg(name='owner', type=d.T.string)]),
        withOwner(owner): { spec+: { bootstrap+: { recovery+: { owner: owner } } } },
        '#withSource':: d.fn(help='"The external cluster whose backup we will restore. This is also\\nused as the name of the folder under which the backup is stored,\\nso it must be set to the name of the source cluster\\nMutually exclusive with `backup`."', args=[d.arg(name='source', type=d.T.string)]),
        withSource(source): { spec+: { bootstrap+: { recovery+: { source: source } } } },
      },
    },
    '#certificates':: d.obj(help='"The configuration for the CA and related certificates"'),
    certificates: {
      '#withClientCASecret':: d.fn(help='"The secret containing the Client CA certificate. If not defined, a new secret will be created\\nwith a self-signed CA and will be used to generate all the client certificates.<br />\\n<br />\\nContains:<br />\\n<br />\\n- `ca.crt`: CA that should be used to validate the client certificates,\\nused as `ssl_ca_file` of all the instances.<br />\\n- `ca.key`: key used to generate client certificates, if ReplicationTLSSecret is provided,\\nthis can be omitted.<br />"', args=[d.arg(name='clientCASecret', type=d.T.string)]),
      withClientCASecret(clientCASecret): { spec+: { certificates+: { clientCASecret: clientCASecret } } },
      '#withReplicationTLSSecret':: d.fn(help='"The secret of type kubernetes.io/tls containing the client certificate to authenticate as\\nthe `streaming_replica` user.\\nIf not defined, ClientCASecret must provide also `ca.key`, and a new secret will be\\ncreated using the provided CA."', args=[d.arg(name='replicationTLSSecret', type=d.T.string)]),
      withReplicationTLSSecret(replicationTLSSecret): { spec+: { certificates+: { replicationTLSSecret: replicationTLSSecret } } },
      '#withServerAltDNSNames':: d.fn(help='"The list of the server alternative DNS names to be added to the generated server TLS certificates, when required."', args=[d.arg(name='serverAltDNSNames', type=d.T.array)]),
      withServerAltDNSNames(serverAltDNSNames): { spec+: { certificates+: { serverAltDNSNames: if std.isArray(v=serverAltDNSNames) then serverAltDNSNames else [serverAltDNSNames] } } },
      '#withServerAltDNSNamesMixin':: d.fn(help='"The list of the server alternative DNS names to be added to the generated server TLS certificates, when required."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='serverAltDNSNames', type=d.T.array)]),
      withServerAltDNSNamesMixin(serverAltDNSNames): { spec+: { certificates+: { serverAltDNSNames+: if std.isArray(v=serverAltDNSNames) then serverAltDNSNames else [serverAltDNSNames] } } },
      '#withServerCASecret':: d.fn(help='"The secret containing the Server CA certificate. If not defined, a new secret will be created\\nwith a self-signed CA and will be used to generate the TLS certificate ServerTLSSecret.<br />\\n<br />\\nContains:<br />\\n<br />\\n- `ca.crt`: CA that should be used to validate the server certificate,\\nused as `sslrootcert` in client connection strings.<br />\\n- `ca.key`: key used to generate Server SSL certs, if ServerTLSSecret is provided,\\nthis can be omitted.<br />"', args=[d.arg(name='serverCASecret', type=d.T.string)]),
      withServerCASecret(serverCASecret): { spec+: { certificates+: { serverCASecret: serverCASecret } } },
      '#withServerTLSSecret':: d.fn(help='"The secret of type kubernetes.io/tls containing the server TLS certificate and key that will be set as\\n`ssl_cert_file` and `ssl_key_file` so that clients can connect to postgres securely.\\nIf not defined, ServerCASecret must provide also `ca.key` and a new secret will be\\ncreated using the provided CA."', args=[d.arg(name='serverTLSSecret', type=d.T.string)]),
      withServerTLSSecret(serverTLSSecret): { spec+: { certificates+: { serverTLSSecret: serverTLSSecret } } },
    },
    '#env':: d.obj(help='"Env follows the Env format to pass environment variables\\nto the pods created in the cluster"'),
    env: {
      '#valueFrom':: d.obj(help="\"Source for the environment variable's value. Cannot be used if value is not empty.\""),
      valueFrom: {
        '#configMapKeyRef':: d.obj(help='"Selects a key of a ConfigMap."'),
        configMapKeyRef: {
          '#withKey':: d.fn(help='"The key to select."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { valueFrom+: { configMapKeyRef+: { key: key } } },
          '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { valueFrom+: { configMapKeyRef+: { name: name } } },
          '#withOptional':: d.fn(help='"Specify whether the ConfigMap or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { valueFrom+: { configMapKeyRef+: { optional: optional } } },
        },
        '#fieldRef':: d.obj(help="\"Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['\u003cKEY\u003e']`, `metadata.annotations['\u003cKEY\u003e']`,\\nspec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.\""),
        fieldRef: {
          '#withApiVersion':: d.fn(help='"Version of the schema the FieldPath is written in terms of, defaults to \\"v1\\"."', args=[d.arg(name='apiVersion', type=d.T.string)]),
          withApiVersion(apiVersion): { valueFrom+: { fieldRef+: { apiVersion: apiVersion } } },
          '#withFieldPath':: d.fn(help='"Path of the field to select in the specified API version."', args=[d.arg(name='fieldPath', type=d.T.string)]),
          withFieldPath(fieldPath): { valueFrom+: { fieldRef+: { fieldPath: fieldPath } } },
        },
        '#resourceFieldRef':: d.obj(help='"Selects a resource of the container: only resources limits and requests\\n(limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported."'),
        resourceFieldRef: {
          '#withContainerName':: d.fn(help='"Container name: required for volumes, optional for env vars"', args=[d.arg(name='containerName', type=d.T.string)]),
          withContainerName(containerName): { valueFrom+: { resourceFieldRef+: { containerName: containerName } } },
          '#withDivisor':: d.fn(help='"Specifies the output format of the exposed resources, defaults to \\"1\\', args=[d.arg(name='divisor', type=d.T.any)]),
          withDivisor(divisor): { valueFrom+: { resourceFieldRef+: { divisor: divisor } } },
          '#withResource':: d.fn(help='"Required: resource to select"', args=[d.arg(name='resource', type=d.T.string)]),
          withResource(resource): { valueFrom+: { resourceFieldRef+: { resource: resource } } },
        },
        '#secretKeyRef':: d.obj(help="\"Selects a key of a secret in the pod's namespace\""),
        secretKeyRef: {
          '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { valueFrom+: { secretKeyRef+: { key: key } } },
          '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { valueFrom+: { secretKeyRef+: { name: name } } },
          '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { valueFrom+: { secretKeyRef+: { optional: optional } } },
        },
      },
      '#withName':: d.fn(help='"Name of the environment variable. Must be a C_IDENTIFIER."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { name: name },
      '#withValue':: d.fn(help='"Variable references $(VAR_NAME) are expanded\\nusing the previously defined environment variables in the container and\\nany service environment variables. If a variable cannot be resolved,\\nthe reference in the input string will be unchanged. Double $$ are reduced\\nto a single $, which allows for escaping the $(VAR_NAME) syntax: i.e.\\n\\"$$(VAR_NAME)\\" will produce the string literal \\"$(VAR_NAME)\\".\\nEscaped references will never be expanded, regardless of whether the variable\\nexists or not.\\nDefaults to \\"\\"."', args=[d.arg(name='value', type=d.T.string)]),
      withValue(value): { value: value },
    },
    '#envFrom':: d.obj(help='"EnvFrom follows the EnvFrom format to pass environment variables\\nsources to the pods to be used by Env"'),
    envFrom: {
      '#configMapRef':: d.obj(help='"The ConfigMap to select from"'),
      configMapRef: {
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { configMapRef+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the ConfigMap must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { configMapRef+: { optional: optional } },
      },
      '#secretRef':: d.obj(help='"The Secret to select from"'),
      secretRef: {
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { secretRef+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the Secret must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { secretRef+: { optional: optional } },
      },
      '#withPrefix':: d.fn(help='"An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER."', args=[d.arg(name='prefix', type=d.T.string)]),
      withPrefix(prefix): { prefix: prefix },
    },
    '#ephemeralVolumeSource':: d.obj(help='"EphemeralVolumeSource allows the user to configure the source of ephemeral volumes."'),
    ephemeralVolumeSource: {
      '#volumeClaimTemplate':: d.obj(help='"Will be used to create a stand-alone PVC to provision the volume.\\nThe pod in which this EphemeralVolumeSource is embedded will be the\\nowner of the PVC, i.e. the PVC will be deleted together with the\\npod.  The name of the PVC will be `<pod name>-<volume name>` where\\n`<volume name>` is the name from the `PodSpec.Volumes` array\\nentry. Pod validation will reject the pod if the concatenated name\\nis not valid for a PVC (for example, too long).\\n\\nAn existing PVC with that name that is not owned by the pod\\nwill *not* be used for the pod to avoid using an unrelated\\nvolume by mistake. Starting the pod is then blocked until\\nthe unrelated PVC is removed. If such a pre-created PVC is\\nmeant to be used by the pod, the PVC has to updated with an\\nowner reference to the pod once the pod exists. Normally\\nthis should not be necessary, but it may be useful when\\nmanually reconstructing a broken cluster.\\n\\nThis field is read-only and no changes will be made by Kubernetes\\nto the PVC after it has been created.\\n\\nRequired, must not be nil."'),
      volumeClaimTemplate: {
        '#spec':: d.obj(help='"The specification for the PersistentVolumeClaim. The entire content is\\ncopied unchanged into the PVC that gets created from this\\ntemplate. The same fields as in a PersistentVolumeClaim\\nare also valid here."'),
        spec: {
          '#dataSource':: d.obj(help='"dataSource field can be used to specify either:\\n* An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)\\n* An existing PVC (PersistentVolumeClaim)\\nIf the provisioner or an external controller can support the specified data source,\\nit will create a new volume based on the contents of the specified data source.\\nWhen the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,\\nand dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.\\nIf the namespace is specified, then dataSourceRef will not be copied to dataSource."'),
          dataSource: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSource+: { apiGroup: apiGroup } } } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSource+: { kind: kind } } } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSource+: { name: name } } } } } },
          },
          '#dataSourceRef':: d.obj(help="\"dataSourceRef specifies the object from which to populate the volume with data, if a non-empty\\nvolume is desired. This may be any object from a non-empty API group (non\\ncore object) or a PersistentVolumeClaim object.\\nWhen this field is specified, volume binding will only succeed if the type of\\nthe specified object matches some installed volume populator or dynamic\\nprovisioner.\\nThis field will replace the functionality of the dataSource field and as such\\nif both fields are non-empty, they must have the same value. For backwards\\ncompatibility, when namespace isn't specified in dataSourceRef,\\nboth fields (dataSource and dataSourceRef) will be set to the same\\nvalue automatically if one of them is empty and the other is non-empty.\\nWhen namespace is specified in dataSourceRef,\\ndataSource isn't set to the same value and must be empty.\\nThere are three important differences between dataSource and dataSourceRef:\\n* While dataSource only allows two specific types of objects, dataSourceRef\\n  allows any non-core object, as well as PersistentVolumeClaim objects.\\n* While dataSource ignores disallowed values (dropping them), dataSourceRef\\n  preserves all values, and generates an error if a disallowed value is\\n  specified.\\n* While dataSource only allows local objects, dataSourceRef allows objects\\n  in any namespaces.\\n(Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.\\n(Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\""),
          dataSourceRef: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSourceRef+: { apiGroup: apiGroup } } } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSourceRef+: { kind: kind } } } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSourceRef+: { name: name } } } } } },
            '#withNamespace':: d.fn(help="\"Namespace is the namespace of resource being referenced\\nNote that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.\\n(Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\"", args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { dataSourceRef+: { namespace: namespace } } } } } },
          },
          '#resources':: d.obj(help='"resources represents the minimum resources the volume should have.\\nIf RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements\\nthat are lower than previous value but must still be higher than capacity recorded in the\\nstatus field of the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"'),
          resources: {
            '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='limits', type=d.T.object)]),
            withLimits(limits): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { resources+: { limits: limits } } } } } },
            '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
            withLimitsMixin(limits): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { resources+: { limits+: limits } } } } } },
            '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='requests', type=d.T.object)]),
            withRequests(requests): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { resources+: { requests: requests } } } } } },
            '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
            withRequestsMixin(requests): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { resources+: { requests+: requests } } } } } },
          },
          '#selector':: d.obj(help='"selector is a label query over volumes to consider for binding."'),
          selector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { selector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { selector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { selector+: { matchLabels: matchLabels } } } } } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { selector+: { matchLabels+: matchLabels } } } } } },
          },
          '#withAccessModes':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"', args=[d.arg(name='accessModes', type=d.T.array)]),
          withAccessModes(accessModes): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { accessModes: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } } },
          '#withAccessModesMixin':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='accessModes', type=d.T.array)]),
          withAccessModesMixin(accessModes): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { accessModes+: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } } },
          '#withStorageClassName':: d.fn(help='"storageClassName is the name of the StorageClass required by the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"', args=[d.arg(name='storageClassName', type=d.T.string)]),
          withStorageClassName(storageClassName): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { storageClassName: storageClassName } } } } },
          '#withVolumeAttributesClassName':: d.fn(help="\"volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.\\nIf specified, the CSI driver will create or update the volume with the attributes defined\\nin the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,\\nit can be changed after the claim is created. An empty string value means that no VolumeAttributesClass\\nwill be applied to the claim but it's not allowed to reset this field to empty string once it is set.\\nIf unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass\\nwill be set by the persistentvolume controller if it exists.\\nIf the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be\\nset to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource\\nexists.\\nMore info: https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\\n(Beta) Using this field requires the VolumeAttributesClass feature gate to be enabled (off by default).\"", args=[d.arg(name='volumeAttributesClassName', type=d.T.string)]),
          withVolumeAttributesClassName(volumeAttributesClassName): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { volumeAttributesClassName: volumeAttributesClassName } } } } },
          '#withVolumeMode':: d.fn(help='"volumeMode defines what type of volume is required by the claim.\\nValue of Filesystem is implied when not included in claim spec."', args=[d.arg(name='volumeMode', type=d.T.string)]),
          withVolumeMode(volumeMode): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { volumeMode: volumeMode } } } } },
          '#withVolumeName':: d.fn(help='"volumeName is the binding reference to the PersistentVolume backing this claim."', args=[d.arg(name='volumeName', type=d.T.string)]),
          withVolumeName(volumeName): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { spec+: { volumeName: volumeName } } } } },
        },
        '#withMetadata':: d.fn(help='"May contain labels and annotations that will be copied into the PVC\\nwhen creating it. No other fields are allowed and will be rejected during\\nvalidation."', args=[d.arg(name='metadata', type=d.T.object)]),
        withMetadata(metadata): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { metadata: metadata } } } },
        '#withMetadataMixin':: d.fn(help='"May contain labels and annotations that will be copied into the PVC\\nwhen creating it. No other fields are allowed and will be rejected during\\nvalidation."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='metadata', type=d.T.object)]),
        withMetadataMixin(metadata): { spec+: { ephemeralVolumeSource+: { volumeClaimTemplate+: { metadata+: metadata } } } },
      },
    },
    '#ephemeralVolumesSizeLimit':: d.obj(help='"EphemeralVolumesSizeLimit allows the user to set the limits for the ephemeral\\nvolumes"'),
    ephemeralVolumesSizeLimit: {
      '#withShm':: d.fn(help='"Shm is the size limit of the shared memory volume"', args=[d.arg(name='shm', type=d.T.any)]),
      withShm(shm): { spec+: { ephemeralVolumesSizeLimit+: { shm: shm } } },
      '#withTemporaryData':: d.fn(help='"TemporaryData is the size limit of the temporary data volume"', args=[d.arg(name='temporaryData', type=d.T.any)]),
      withTemporaryData(temporaryData): { spec+: { ephemeralVolumesSizeLimit+: { temporaryData: temporaryData } } },
    },
    '#externalClusters':: d.obj(help='"The list of external clusters which are used in the configuration"'),
    externalClusters: {
      '#barmanObjectStore':: d.obj(help='"The configuration for the barman-cloud tool suite"'),
      barmanObjectStore: {
        '#azureCredentials':: d.obj(help='"The credentials to use to upload data to Azure Blob Storage"'),
        azureCredentials: {
          '#connectionString':: d.obj(help='"The connection string to be used"'),
          connectionString: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { azureCredentials+: { connectionString+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { azureCredentials+: { connectionString+: { name: name } } } },
          },
          '#storageAccount':: d.obj(help='"The storage account where to upload data"'),
          storageAccount: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { azureCredentials+: { storageAccount+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { azureCredentials+: { storageAccount+: { name: name } } } },
          },
          '#storageKey':: d.obj(help='"The storage account key to be used in conjunction\\nwith the storage account name"'),
          storageKey: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { azureCredentials+: { storageKey+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { azureCredentials+: { storageKey+: { name: name } } } },
          },
          '#storageSasToken':: d.obj(help='"A shared-access-signature to be used in conjunction with\\nthe storage account name"'),
          storageSasToken: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { azureCredentials+: { storageSasToken+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { azureCredentials+: { storageSasToken+: { name: name } } } },
          },
          '#withInheritFromAzureAD':: d.fn(help='"Use the Azure AD based authentication without providing explicitly the keys."', args=[d.arg(name='inheritFromAzureAD', type=d.T.boolean)]),
          withInheritFromAzureAD(inheritFromAzureAD): { barmanObjectStore+: { azureCredentials+: { inheritFromAzureAD: inheritFromAzureAD } } },
        },
        '#data':: d.obj(help='"The configuration to be used to backup the data files\\nWhen not defined, base backups files will be stored uncompressed and may\\nbe unencrypted in the object store, according to the bucket default\\npolicy."'),
        data: {
          '#withAdditionalCommandArgs':: d.fn(help="\"AdditionalCommandArgs represents additional arguments that can be appended\\nto the 'barman-cloud-backup' command-line invocation. These arguments\\nprovide flexibility to customize the backup process further according to\\nspecific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-backup' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='additionalCommandArgs', type=d.T.array)]),
          withAdditionalCommandArgs(additionalCommandArgs): { barmanObjectStore+: { data+: { additionalCommandArgs: if std.isArray(v=additionalCommandArgs) then additionalCommandArgs else [additionalCommandArgs] } } },
          '#withAdditionalCommandArgsMixin':: d.fn(help="\"AdditionalCommandArgs represents additional arguments that can be appended\\nto the 'barman-cloud-backup' command-line invocation. These arguments\\nprovide flexibility to customize the backup process further according to\\nspecific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-backup' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='additionalCommandArgs', type=d.T.array)]),
          withAdditionalCommandArgsMixin(additionalCommandArgs): { barmanObjectStore+: { data+: { additionalCommandArgs+: if std.isArray(v=additionalCommandArgs) then additionalCommandArgs else [additionalCommandArgs] } } },
          '#withCompression':: d.fn(help='"Compress a backup file (a tar file per tablespace) while streaming it\\nto the object store. Available options are empty string (no\\ncompression, default), `gzip`, `bzip2` or `snappy`."', args=[d.arg(name='compression', type=d.T.string)]),
          withCompression(compression): { barmanObjectStore+: { data+: { compression: compression } } },
          '#withEncryption':: d.fn(help='"Whenever to force the encryption of files (if the bucket is\\nnot already configured for that).\\nAllowed options are empty string (use the bucket policy, default),\\n`AES256` and `aws:kms`"', args=[d.arg(name='encryption', type=d.T.string)]),
          withEncryption(encryption): { barmanObjectStore+: { data+: { encryption: encryption } } },
          '#withImmediateCheckpoint':: d.fn(help='"Control whether the I/O workload for the backup initial checkpoint will\\nbe limited, according to the `checkpoint_completion_target` setting on\\nthe PostgreSQL server. If set to true, an immediate checkpoint will be\\nused, meaning PostgreSQL will complete the checkpoint as soon as\\npossible. `false` by default."', args=[d.arg(name='immediateCheckpoint', type=d.T.boolean)]),
          withImmediateCheckpoint(immediateCheckpoint): { barmanObjectStore+: { data+: { immediateCheckpoint: immediateCheckpoint } } },
          '#withJobs':: d.fn(help='"The number of parallel jobs to be used to upload the backup, defaults\\nto 2"', args=[d.arg(name='jobs', type=d.T.integer)]),
          withJobs(jobs): { barmanObjectStore+: { data+: { jobs: jobs } } },
        },
        '#endpointCA':: d.obj(help='"EndpointCA store the CA bundle of the barman endpoint.\\nUseful when using self-signed certificates to avoid\\nerrors with certificate issuer and barman-cloud-wal-archive"'),
        endpointCA: {
          '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { barmanObjectStore+: { endpointCA+: { key: key } } },
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { barmanObjectStore+: { endpointCA+: { name: name } } },
        },
        '#googleCredentials':: d.obj(help='"The credentials to use to upload data to Google Cloud Storage"'),
        googleCredentials: {
          '#applicationCredentials':: d.obj(help='"The secret containing the Google Cloud Storage JSON file with the credentials"'),
          applicationCredentials: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { googleCredentials+: { applicationCredentials+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { googleCredentials+: { applicationCredentials+: { name: name } } } },
          },
          '#withGkeEnvironment':: d.fn(help="\"If set to true, will presume that it's running inside a GKE environment,\\ndefault to false.\"", args=[d.arg(name='gkeEnvironment', type=d.T.boolean)]),
          withGkeEnvironment(gkeEnvironment): { barmanObjectStore+: { googleCredentials+: { gkeEnvironment: gkeEnvironment } } },
        },
        '#s3Credentials':: d.obj(help='"The credentials to use to upload data to S3"'),
        s3Credentials: {
          '#accessKeyId':: d.obj(help='"The reference to the access key id"'),
          accessKeyId: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { s3Credentials+: { accessKeyId+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { s3Credentials+: { accessKeyId+: { name: name } } } },
          },
          '#region':: d.obj(help='"The reference to the secret containing the region name"'),
          region: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { s3Credentials+: { region+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { s3Credentials+: { region+: { name: name } } } },
          },
          '#secretAccessKey':: d.obj(help='"The reference to the secret access key"'),
          secretAccessKey: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { s3Credentials+: { secretAccessKey+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { s3Credentials+: { secretAccessKey+: { name: name } } } },
          },
          '#sessionToken':: d.obj(help='"The references to the session key"'),
          sessionToken: {
            '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { barmanObjectStore+: { s3Credentials+: { sessionToken+: { key: key } } } },
            '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { barmanObjectStore+: { s3Credentials+: { sessionToken+: { name: name } } } },
          },
          '#withInheritFromIAMRole':: d.fn(help='"Use the role based authentication without providing explicitly the keys."', args=[d.arg(name='inheritFromIAMRole', type=d.T.boolean)]),
          withInheritFromIAMRole(inheritFromIAMRole): { barmanObjectStore+: { s3Credentials+: { inheritFromIAMRole: inheritFromIAMRole } } },
        },
        '#wal':: d.obj(help='"The configuration for the backup of the WAL stream.\\nWhen not defined, WAL files will be stored uncompressed and may be\\nunencrypted in the object store, according to the bucket default policy."'),
        wal: {
          '#withArchiveAdditionalCommandArgs':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-archive'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL archive process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-archive' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='archiveAdditionalCommandArgs', type=d.T.array)]),
          withArchiveAdditionalCommandArgs(archiveAdditionalCommandArgs): { barmanObjectStore+: { wal+: { archiveAdditionalCommandArgs: if std.isArray(v=archiveAdditionalCommandArgs) then archiveAdditionalCommandArgs else [archiveAdditionalCommandArgs] } } },
          '#withArchiveAdditionalCommandArgsMixin':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-archive'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL archive process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-archive' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='archiveAdditionalCommandArgs', type=d.T.array)]),
          withArchiveAdditionalCommandArgsMixin(archiveAdditionalCommandArgs): { barmanObjectStore+: { wal+: { archiveAdditionalCommandArgs+: if std.isArray(v=archiveAdditionalCommandArgs) then archiveAdditionalCommandArgs else [archiveAdditionalCommandArgs] } } },
          '#withCompression':: d.fn(help='"Compress a WAL file before sending it to the object store. Available\\noptions are empty string (no compression, default), `gzip`, `bzip2` or `snappy`."', args=[d.arg(name='compression', type=d.T.string)]),
          withCompression(compression): { barmanObjectStore+: { wal+: { compression: compression } } },
          '#withEncryption':: d.fn(help='"Whenever to force the encryption of files (if the bucket is\\nnot already configured for that).\\nAllowed options are empty string (use the bucket policy, default),\\n`AES256` and `aws:kms`"', args=[d.arg(name='encryption', type=d.T.string)]),
          withEncryption(encryption): { barmanObjectStore+: { wal+: { encryption: encryption } } },
          '#withMaxParallel':: d.fn(help='"Number of WAL files to be either archived in parallel (when the\\nPostgreSQL instance is archiving to a backup object store) or\\nrestored in parallel (when a PostgreSQL standby is fetching WAL\\nfiles from a recovery object store). If not specified, WAL files\\nwill be processed one at a time. It accepts a positive integer as a\\nvalue - with 1 being the minimum accepted value."', args=[d.arg(name='maxParallel', type=d.T.integer)]),
          withMaxParallel(maxParallel): { barmanObjectStore+: { wal+: { maxParallel: maxParallel } } },
          '#withRestoreAdditionalCommandArgs':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-restore'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL restore process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-restore' command, to avoid potential errors or unintended\\nbehavior during execution.\"", args=[d.arg(name='restoreAdditionalCommandArgs', type=d.T.array)]),
          withRestoreAdditionalCommandArgs(restoreAdditionalCommandArgs): { barmanObjectStore+: { wal+: { restoreAdditionalCommandArgs: if std.isArray(v=restoreAdditionalCommandArgs) then restoreAdditionalCommandArgs else [restoreAdditionalCommandArgs] } } },
          '#withRestoreAdditionalCommandArgsMixin':: d.fn(help="\"Additional arguments that can be appended to the 'barman-cloud-wal-restore'\\ncommand-line invocation. These arguments provide flexibility to customize\\nthe WAL restore process further, according to specific requirements or configurations.\\n\\nExample:\\nIn a scenario where specialized backup options are required, such as setting\\na specific timeout or defining custom behavior, users can use this field\\nto specify additional command arguments.\\n\\nNote:\\nIt's essential to ensure that the provided arguments are valid and supported\\nby the 'barman-cloud-wal-restore' command, to avoid potential errors or unintended\\nbehavior during execution.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='restoreAdditionalCommandArgs', type=d.T.array)]),
          withRestoreAdditionalCommandArgsMixin(restoreAdditionalCommandArgs): { barmanObjectStore+: { wal+: { restoreAdditionalCommandArgs+: if std.isArray(v=restoreAdditionalCommandArgs) then restoreAdditionalCommandArgs else [restoreAdditionalCommandArgs] } } },
        },
        '#withDestinationPath':: d.fn(help='"The path where to store the backup (i.e. s3://bucket/path/to/folder)\\nthis path, with different destination folders, will be used for WALs\\nand for data"', args=[d.arg(name='destinationPath', type=d.T.string)]),
        withDestinationPath(destinationPath): { barmanObjectStore+: { destinationPath: destinationPath } },
        '#withEndpointURL':: d.fn(help='"Endpoint to be used to upload data to the cloud,\\noverriding the automatic endpoint discovery"', args=[d.arg(name='endpointURL', type=d.T.string)]),
        withEndpointURL(endpointURL): { barmanObjectStore+: { endpointURL: endpointURL } },
        '#withHistoryTags':: d.fn(help='"HistoryTags is a list of key value pairs that will be passed to the\\nBarman --history-tags option."', args=[d.arg(name='historyTags', type=d.T.object)]),
        withHistoryTags(historyTags): { barmanObjectStore+: { historyTags: historyTags } },
        '#withHistoryTagsMixin':: d.fn(help='"HistoryTags is a list of key value pairs that will be passed to the\\nBarman --history-tags option."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='historyTags', type=d.T.object)]),
        withHistoryTagsMixin(historyTags): { barmanObjectStore+: { historyTags+: historyTags } },
        '#withServerName':: d.fn(help='"The server name on S3, the cluster name is used if this\\nparameter is omitted"', args=[d.arg(name='serverName', type=d.T.string)]),
        withServerName(serverName): { barmanObjectStore+: { serverName: serverName } },
        '#withTags':: d.fn(help='"Tags is a list of key value pairs that will be passed to the\\nBarman --tags option."', args=[d.arg(name='tags', type=d.T.object)]),
        withTags(tags): { barmanObjectStore+: { tags: tags } },
        '#withTagsMixin':: d.fn(help='"Tags is a list of key value pairs that will be passed to the\\nBarman --tags option."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
        withTagsMixin(tags): { barmanObjectStore+: { tags+: tags } },
      },
      '#password':: d.obj(help="\"The reference to the password to be used to connect to the server.\\nIf a password is provided, CloudNativePG creates a PostgreSQL\\npassfile at `/controller/external/NAME/pass` (where \\\"NAME\\\" is the\\ncluster's name). This passfile is automatically referenced in the\\nconnection string when establishing a connection to the remote\\nPostgreSQL server from the current PostgreSQL `Cluster`. This ensures\\nsecure and efficient password management for external clusters.\""),
      password: {
        '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { password+: { key: key } },
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { password+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { password+: { optional: optional } },
      },
      '#plugin':: d.obj(help='"The configuration of the plugin that is taking care\\nof WAL archiving and backups for this external cluster"'),
      plugin: {
        '#withEnabled':: d.fn(help='"Enabled is true if this plugin will be used"', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { plugin+: { enabled: enabled } },
        '#withIsWALArchiver':: d.fn(help='"Only one plugin can be declared as WALArchiver.\\nCannot be active if \\".spec.backup.barmanObjectStore\\" configuration is present."', args=[d.arg(name='isWALArchiver', type=d.T.boolean)]),
        withIsWALArchiver(isWALArchiver): { plugin+: { isWALArchiver: isWALArchiver } },
        '#withName':: d.fn(help='"Name is the plugin name"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { plugin+: { name: name } },
        '#withParameters':: d.fn(help='"Parameters is the configuration of the plugin"', args=[d.arg(name='parameters', type=d.T.object)]),
        withParameters(parameters): { plugin+: { parameters: parameters } },
        '#withParametersMixin':: d.fn(help='"Parameters is the configuration of the plugin"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.object)]),
        withParametersMixin(parameters): { plugin+: { parameters+: parameters } },
      },
      '#sslCert':: d.obj(help='"The reference to an SSL certificate to be used to connect to this\\ninstance"'),
      sslCert: {
        '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { sslCert+: { key: key } },
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { sslCert+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { sslCert+: { optional: optional } },
      },
      '#sslKey':: d.obj(help='"The reference to an SSL private key to be used to connect to this\\ninstance"'),
      sslKey: {
        '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { sslKey+: { key: key } },
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { sslKey+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { sslKey+: { optional: optional } },
      },
      '#sslRootCert':: d.obj(help='"The reference to an SSL CA public key to be used to connect to this\\ninstance"'),
      sslRootCert: {
        '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { sslRootCert+: { key: key } },
        '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { sslRootCert+: { name: name } },
        '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
        withOptional(optional): { sslRootCert+: { optional: optional } },
      },
      '#withConnectionParameters':: d.fn(help='"The list of connection parameters, such as dbname, host, username, etc"', args=[d.arg(name='connectionParameters', type=d.T.object)]),
      withConnectionParameters(connectionParameters): { connectionParameters: connectionParameters },
      '#withConnectionParametersMixin':: d.fn(help='"The list of connection parameters, such as dbname, host, username, etc"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='connectionParameters', type=d.T.object)]),
      withConnectionParametersMixin(connectionParameters): { connectionParameters+: connectionParameters },
      '#withName':: d.fn(help='"The server name, required"', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { name: name },
    },
    '#imageCatalogRef':: d.obj(help='"Defines the major PostgreSQL version we want to use within an ImageCatalog"'),
    imageCatalogRef: {
      '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
      withApiGroup(apiGroup): { spec+: { imageCatalogRef+: { apiGroup: apiGroup } } },
      '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
      withKind(kind): { spec+: { imageCatalogRef+: { kind: kind } } },
      '#withMajor':: d.fn(help='"The major version of PostgreSQL we want to use from the ImageCatalog"', args=[d.arg(name='major', type=d.T.integer)]),
      withMajor(major): { spec+: { imageCatalogRef+: { major: major } } },
      '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { imageCatalogRef+: { name: name } } },
    },
    '#imagePullSecrets':: d.obj(help='"The list of pull secrets to be used to pull the images"'),
    imagePullSecrets: {
      '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { name: name },
    },
    '#inheritedMetadata':: d.obj(help='"Metadata that will be inherited by all objects related to the Cluster"'),
    inheritedMetadata: {
      '#withAnnotations':: d.fn(help='', args=[d.arg(name='annotations', type=d.T.object)]),
      withAnnotations(annotations): { spec+: { inheritedMetadata+: { annotations: annotations } } },
      '#withAnnotationsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
      withAnnotationsMixin(annotations): { spec+: { inheritedMetadata+: { annotations+: annotations } } },
      '#withLabels':: d.fn(help='', args=[d.arg(name='labels', type=d.T.object)]),
      withLabels(labels): { spec+: { inheritedMetadata+: { labels: labels } } },
      '#withLabelsMixin':: d.fn(help='\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
      withLabelsMixin(labels): { spec+: { inheritedMetadata+: { labels+: labels } } },
    },
    '#managed':: d.obj(help='"The configuration that is used by the portions of PostgreSQL that are managed by the instance manager"'),
    managed: {
      '#roles':: d.obj(help='"Database roles managed by the `Cluster`"'),
      roles: {
        '#passwordSecret':: d.obj(help='"Secret containing the password of the role (if present)\\nIf null, the password will be ignored unless DisablePassword is set"'),
        passwordSecret: {
          '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { passwordSecret+: { name: name } },
        },
        '#withBypassrls':: d.fn(help='"Whether a role bypasses every row-level security (RLS) policy.\\nDefault is `false`."', args=[d.arg(name='bypassrls', type=d.T.boolean)]),
        withBypassrls(bypassrls): { bypassrls: bypassrls },
        '#withComment':: d.fn(help='"Description of the role"', args=[d.arg(name='comment', type=d.T.string)]),
        withComment(comment): { comment: comment },
        '#withConnectionLimit':: d.fn(help='"If the role can log in, this specifies how many concurrent\\nconnections the role can make. `-1` (the default) means no limit."', args=[d.arg(name='connectionLimit', type=d.T.integer)]),
        withConnectionLimit(connectionLimit): { connectionLimit: connectionLimit },
        '#withCreatedb':: d.fn(help='"When set to `true`, the role being defined will be allowed to create\\nnew databases. Specifying `false` (default) will deny a role the\\nability to create databases."', args=[d.arg(name='createdb', type=d.T.boolean)]),
        withCreatedb(createdb): { createdb: createdb },
        '#withCreaterole':: d.fn(help='"Whether the role will be permitted to create, alter, drop, comment\\non, change the security label for, and grant or revoke membership in\\nother roles. Default is `false`."', args=[d.arg(name='createrole', type=d.T.boolean)]),
        withCreaterole(createrole): { createrole: createrole },
        '#withDisablePassword':: d.fn(help="\"DisablePassword indicates that a role's password should be set to NULL in Postgres\"", args=[d.arg(name='disablePassword', type=d.T.boolean)]),
        withDisablePassword(disablePassword): { disablePassword: disablePassword },
        '#withEnsure':: d.fn(help='"Ensure the role is `present` or `absent` - defaults to \\"present\\', args=[d.arg(name='ensure', type=d.T.string)]),
        withEnsure(ensure): { ensure: ensure },
        '#withInRoles':: d.fn(help='"List of one or more existing roles to which this role will be\\nimmediately added as a new member. Default empty."', args=[d.arg(name='inRoles', type=d.T.array)]),
        withInRoles(inRoles): { inRoles: if std.isArray(v=inRoles) then inRoles else [inRoles] },
        '#withInRolesMixin':: d.fn(help='"List of one or more existing roles to which this role will be\\nimmediately added as a new member. Default empty."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inRoles', type=d.T.array)]),
        withInRolesMixin(inRoles): { inRoles+: if std.isArray(v=inRoles) then inRoles else [inRoles] },
        '#withInherit':: d.fn(help='"Whether a role \\"inherits\\" the privileges of roles it is a member of.\\nDefaults is `true`."', args=[d.arg(name='inherit', type=d.T.boolean)]),
        withInherit(inherit): { inherit: inherit },
        '#withLogin':: d.fn(help='"Whether the role is allowed to log in. A role having the `login`\\nattribute can be thought of as a user. Roles without this attribute\\nare useful for managing database privileges, but are not users in\\nthe usual sense of the word. Default is `false`."', args=[d.arg(name='login', type=d.T.boolean)]),
        withLogin(login): { login: login },
        '#withName':: d.fn(help='"Name of the role"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withReplication':: d.fn(help='"Whether a role is a replication role. A role must have this\\nattribute (or be a superuser) in order to be able to connect to the\\nserver in replication mode (physical or logical replication) and in\\norder to be able to create or drop replication slots. A role having\\nthe `replication` attribute is a very highly privileged role, and\\nshould only be used on roles actually used for replication. Default\\nis `false`."', args=[d.arg(name='replication', type=d.T.boolean)]),
        withReplication(replication): { replication: replication },
        '#withSuperuser':: d.fn(help='"Whether the role is a `superuser` who can override all access\\nrestrictions within the database - superuser status is dangerous and\\nshould be used only when really needed. You must yourself be a\\nsuperuser to create a new superuser. Defaults is `false`."', args=[d.arg(name='superuser', type=d.T.boolean)]),
        withSuperuser(superuser): { superuser: superuser },
        '#withValidUntil':: d.fn(help="\"Date and time after which the role's password is no longer valid.\\nWhen omitted, the password will never expire (default).\"", args=[d.arg(name='validUntil', type=d.T.string)]),
        withValidUntil(validUntil): { validUntil: validUntil },
      },
      '#services':: d.obj(help='"Services roles managed by the `Cluster`"'),
      services: {
        '#additional':: d.obj(help='"Additional is a list of additional managed services specified by the user."'),
        additional: {
          '#serviceTemplate':: d.obj(help='"ServiceTemplate is the template specification for the service."'),
          serviceTemplate: {
            '#metadata':: d.obj(help="\"Standard object's metadata.\\nMore info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\""),
            metadata: {
              '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be\\nset by external tools to store and retrieve arbitrary metadata. They are not\\nqueryable and should be preserved when modifying objects.\\nMore info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
              withAnnotations(annotations): { serviceTemplate+: { metadata+: { annotations: annotations } } },
              '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be\\nset by external tools to store and retrieve arbitrary metadata. They are not\\nqueryable and should be preserved when modifying objects.\\nMore info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
              withAnnotationsMixin(annotations): { serviceTemplate+: { metadata+: { annotations+: annotations } } },
              '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize\\n(scope and select) objects. May match selectors of replication controllers\\nand services.\\nMore info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
              withLabels(labels): { serviceTemplate+: { metadata+: { labels: labels } } },
              '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize\\n(scope and select) objects. May match selectors of replication controllers\\nand services.\\nMore info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
              withLabelsMixin(labels): { serviceTemplate+: { metadata+: { labels+: labels } } },
              '#withName':: d.fn(help='"The name of the resource. Only supported for certain types"', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { serviceTemplate+: { metadata+: { name: name } } },
            },
            '#spec':: d.obj(help='"Specification of the desired behavior of the service.\\nMore info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"'),
            spec: {
              '#ports':: d.obj(help='"The list of ports that are exposed by this service.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"'),
              ports: {
                '#withAppProtocol':: d.fn(help="\"The application protocol for this port.\\nThis is used as a hint for implementations to offer richer behavior for protocols that they understand.\\nThis field follows standard Kubernetes label syntax.\\nValid values are either:\\n\\n* Un-prefixed protocol names - reserved for IANA standard service names (as per\\nRFC-6335 and https://www.iana.org/assignments/service-names).\\n\\n* Kubernetes-defined prefixed names:\\n  * 'kubernetes.io/h2c' - HTTP/2 prior knowledge over cleartext as described in https://www.rfc-editor.org/rfc/rfc9113.html#name-starting-http-2-with-prior-\\n  * 'kubernetes.io/ws'  - WebSocket over cleartext as described in https://www.rfc-editor.org/rfc/rfc6455\\n  * 'kubernetes.io/wss' - WebSocket over TLS as described in https://www.rfc-editor.org/rfc/rfc6455\\n\\n* Other protocols should use implementation-defined prefixed names such as\\nmycompany.com/my-custom-protocol.\"", args=[d.arg(name='appProtocol', type=d.T.string)]),
                withAppProtocol(appProtocol): { appProtocol: appProtocol },
                '#withName':: d.fn(help="\"The name of this port within the service. This must be a DNS_LABEL.\\nAll ports within a ServiceSpec must have unique names. When considering\\nthe endpoints for a Service, this must match the 'name' field in the\\nEndpointPort.\\nOptional if only one ServicePort is defined on this service.\"", args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { name: name },
                '#withNodePort':: d.fn(help='"The port on each node on which this service is exposed when type is\\nNodePort or LoadBalancer.  Usually assigned by the system. If a value is\\nspecified, in-range, and not in use it will be used, otherwise the\\noperation will fail.  If not specified, a port will be allocated if this\\nService requires one.  If this field is specified when creating a\\nService which does not need it, creation will fail. This field will be\\nwiped when updating a Service to no longer need it (e.g. changing type\\nfrom NodePort to ClusterIP).\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport"', args=[d.arg(name='nodePort', type=d.T.integer)]),
                withNodePort(nodePort): { nodePort: nodePort },
                '#withPort':: d.fn(help='"The port that will be exposed by this service."', args=[d.arg(name='port', type=d.T.integer)]),
                withPort(port): { port: port },
                '#withProtocol':: d.fn(help='"The IP protocol for this port. Supports \\"TCP\\", \\"UDP\\", and \\"SCTP\\".\\nDefault is TCP."', args=[d.arg(name='protocol', type=d.T.string)]),
                withProtocol(protocol): { protocol: protocol },
                '#withTargetPort':: d.fn(help="\"Number or name of the port to access on the pods targeted by the service.\\nNumber must be in the range 1 to 65535. Name must be an IANA_SVC_NAME.\\nIf this is a string, it will be looked up as a named port in the\\ntarget Pod's container ports. If this is not specified, the value\\nof the 'port' field is used (an identity map).\\nThis field is ignored for services with clusterIP=None, and should be\\nomitted or set equal to the 'port' field.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service\"", args=[d.arg(name='targetPort', type=d.T.any)]),
                withTargetPort(targetPort): { targetPort: targetPort },
              },
              '#sessionAffinityConfig':: d.obj(help='"sessionAffinityConfig contains the configurations of session affinity."'),
              sessionAffinityConfig: {
                '#clientIP':: d.obj(help='"clientIP contains the configurations of Client IP based session affinity."'),
                clientIP: {
                  '#withTimeoutSeconds':: d.fn(help='"timeoutSeconds specifies the seconds of ClientIP type session sticky time.\\nThe value must be >0 && <=86400(for 1 day) if ServiceAffinity == \\"ClientIP\\".\\nDefault value is 10800(for 3 hours)."', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
                  withTimeoutSeconds(timeoutSeconds): { serviceTemplate+: { spec+: { sessionAffinityConfig+: { clientIP+: { timeoutSeconds: timeoutSeconds } } } } },
                },
              },
              '#withAllocateLoadBalancerNodePorts':: d.fn(help='"allocateLoadBalancerNodePorts defines if NodePorts will be automatically\\nallocated for services with type LoadBalancer.  Default is \\"true\\". It\\nmay be set to \\"false\\" if the cluster load-balancer does not rely on\\nNodePorts.  If the caller requests specific NodePorts (by specifying a\\nvalue), those requests will be respected, regardless of this field.\\nThis field may only be set for services with type LoadBalancer and will\\nbe cleared if the type is changed to any other type."', args=[d.arg(name='allocateLoadBalancerNodePorts', type=d.T.boolean)]),
              withAllocateLoadBalancerNodePorts(allocateLoadBalancerNodePorts): { serviceTemplate+: { spec+: { allocateLoadBalancerNodePorts: allocateLoadBalancerNodePorts } } },
              '#withClusterIP':: d.fn(help='"clusterIP is the IP address of the service and is usually assigned\\nrandomly. If an address is specified manually, is in-range (as per\\nsystem configuration), and is not in use, it will be allocated to the\\nservice; otherwise creation of the service will fail. This field may not\\nbe changed through updates unless the type field is also being changed\\nto ExternalName (which requires this field to be blank) or the type\\nfield is being changed from ExternalName (in which case this field may\\noptionally be specified, as describe above).  Valid values are \\"None\\",\\nempty string (\\"\\"), or a valid IP address. Setting this to \\"None\\" makes a\\n\\"headless service\\" (no virtual IP), which is useful when direct endpoint\\nconnections are preferred and proxying is not required.  Only applies to\\ntypes ClusterIP, NodePort, and LoadBalancer. If this field is specified\\nwhen creating a Service of type ExternalName, creation will fail. This\\nfield will be wiped when updating a Service to type ExternalName.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"', args=[d.arg(name='clusterIP', type=d.T.string)]),
              withClusterIP(clusterIP): { serviceTemplate+: { spec+: { clusterIP: clusterIP } } },
              '#withClusterIPs':: d.fn(help='"ClusterIPs is a list of IP addresses assigned to this service, and are\\nusually assigned randomly.  If an address is specified manually, is\\nin-range (as per system configuration), and is not in use, it will be\\nallocated to the service; otherwise creation of the service will fail.\\nThis field may not be changed through updates unless the type field is\\nalso being changed to ExternalName (which requires this field to be\\nempty) or the type field is being changed from ExternalName (in which\\ncase this field may optionally be specified, as describe above).  Valid\\nvalues are \\"None\\", empty string (\\"\\"), or a valid IP address.  Setting\\nthis to \\"None\\" makes a \\"headless service\\" (no virtual IP), which is\\nuseful when direct endpoint connections are preferred and proxying is\\nnot required.  Only applies to types ClusterIP, NodePort, and\\nLoadBalancer. If this field is specified when creating a Service of type\\nExternalName, creation will fail. This field will be wiped when updating\\na Service to type ExternalName.  If this field is not specified, it will\\nbe initialized from the clusterIP field.  If this field is specified,\\nclients must ensure that clusterIPs[0] and clusterIP have the same\\nvalue.\\n\\nThis field may hold a maximum of two entries (dual-stack IPs, in either order).\\nThese IPs must correspond to the values of the ipFamilies field. Both\\nclusterIPs and ipFamilies are governed by the ipFamilyPolicy field.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"', args=[d.arg(name='clusterIPs', type=d.T.array)]),
              withClusterIPs(clusterIPs): { serviceTemplate+: { spec+: { clusterIPs: if std.isArray(v=clusterIPs) then clusterIPs else [clusterIPs] } } },
              '#withClusterIPsMixin':: d.fn(help='"ClusterIPs is a list of IP addresses assigned to this service, and are\\nusually assigned randomly.  If an address is specified manually, is\\nin-range (as per system configuration), and is not in use, it will be\\nallocated to the service; otherwise creation of the service will fail.\\nThis field may not be changed through updates unless the type field is\\nalso being changed to ExternalName (which requires this field to be\\nempty) or the type field is being changed from ExternalName (in which\\ncase this field may optionally be specified, as describe above).  Valid\\nvalues are \\"None\\", empty string (\\"\\"), or a valid IP address.  Setting\\nthis to \\"None\\" makes a \\"headless service\\" (no virtual IP), which is\\nuseful when direct endpoint connections are preferred and proxying is\\nnot required.  Only applies to types ClusterIP, NodePort, and\\nLoadBalancer. If this field is specified when creating a Service of type\\nExternalName, creation will fail. This field will be wiped when updating\\na Service to type ExternalName.  If this field is not specified, it will\\nbe initialized from the clusterIP field.  If this field is specified,\\nclients must ensure that clusterIPs[0] and clusterIP have the same\\nvalue.\\n\\nThis field may hold a maximum of two entries (dual-stack IPs, in either order).\\nThese IPs must correspond to the values of the ipFamilies field. Both\\nclusterIPs and ipFamilies are governed by the ipFamilyPolicy field.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='clusterIPs', type=d.T.array)]),
              withClusterIPsMixin(clusterIPs): { serviceTemplate+: { spec+: { clusterIPs+: if std.isArray(v=clusterIPs) then clusterIPs else [clusterIPs] } } },
              '#withExternalIPs':: d.fn(help='"externalIPs is a list of IP addresses for which nodes in the cluster\\nwill also accept traffic for this service.  These IPs are not managed by\\nKubernetes.  The user is responsible for ensuring that traffic arrives\\nat a node with this IP.  A common example is external load-balancers\\nthat are not part of the Kubernetes system."', args=[d.arg(name='externalIPs', type=d.T.array)]),
              withExternalIPs(externalIPs): { serviceTemplate+: { spec+: { externalIPs: if std.isArray(v=externalIPs) then externalIPs else [externalIPs] } } },
              '#withExternalIPsMixin':: d.fn(help='"externalIPs is a list of IP addresses for which nodes in the cluster\\nwill also accept traffic for this service.  These IPs are not managed by\\nKubernetes.  The user is responsible for ensuring that traffic arrives\\nat a node with this IP.  A common example is external load-balancers\\nthat are not part of the Kubernetes system."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='externalIPs', type=d.T.array)]),
              withExternalIPsMixin(externalIPs): { serviceTemplate+: { spec+: { externalIPs+: if std.isArray(v=externalIPs) then externalIPs else [externalIPs] } } },
              '#withExternalName':: d.fn(help='"externalName is the external reference that discovery mechanisms will\\nreturn as an alias for this service (e.g. a DNS CNAME record). No\\nproxying will be involved.  Must be a lowercase RFC-1123 hostname\\n(https://tools.ietf.org/html/rfc1123) and requires `type` to be \\"ExternalName\\"."', args=[d.arg(name='externalName', type=d.T.string)]),
              withExternalName(externalName): { serviceTemplate+: { spec+: { externalName: externalName } } },
              '#withExternalTrafficPolicy':: d.fn(help="\"externalTrafficPolicy describes how nodes distribute service traffic they\\nreceive on one of the Service's \\\"externally-facing\\\" addresses (NodePorts,\\nExternalIPs, and LoadBalancer IPs). If set to \\\"Local\\\", the proxy will configure\\nthe service in a way that assumes that external load balancers will take care\\nof balancing the service traffic between nodes, and so each node will deliver\\ntraffic only to the node-local endpoints of the service, without masquerading\\nthe client source IP. (Traffic mistakenly sent to a node with no endpoints will\\nbe dropped.) The default value, \\\"Cluster\\\", uses the standard behavior of\\nrouting to all endpoints evenly (possibly modified by topology and other\\nfeatures). Note that traffic sent to an External IP or LoadBalancer IP from\\nwithin the cluster will always get \\\"Cluster\\\" semantics, but clients sending to\\na NodePort from within the cluster may need to take traffic policy into account\\nwhen picking a node.\"", args=[d.arg(name='externalTrafficPolicy', type=d.T.string)]),
              withExternalTrafficPolicy(externalTrafficPolicy): { serviceTemplate+: { spec+: { externalTrafficPolicy: externalTrafficPolicy } } },
              '#withHealthCheckNodePort':: d.fn(help='"healthCheckNodePort specifies the healthcheck nodePort for the service.\\nThis only applies when type is set to LoadBalancer and\\nexternalTrafficPolicy is set to Local. If a value is specified, is\\nin-range, and is not in use, it will be used.  If not specified, a value\\nwill be automatically allocated.  External systems (e.g. load-balancers)\\ncan use this port to determine if a given node holds endpoints for this\\nservice or not.  If this field is specified when creating a Service\\nwhich does not need it, creation will fail. This field will be wiped\\nwhen updating a Service to no longer need it (e.g. changing type).\\nThis field cannot be updated once set."', args=[d.arg(name='healthCheckNodePort', type=d.T.integer)]),
              withHealthCheckNodePort(healthCheckNodePort): { serviceTemplate+: { spec+: { healthCheckNodePort: healthCheckNodePort } } },
              '#withInternalTrafficPolicy':: d.fn(help='"InternalTrafficPolicy describes how nodes distribute service traffic they\\nreceive on the ClusterIP. If set to \\"Local\\", the proxy will assume that pods\\nonly want to talk to endpoints of the service on the same node as the pod,\\ndropping the traffic if there are no local endpoints. The default value,\\n\\"Cluster\\", uses the standard behavior of routing to all endpoints evenly\\n(possibly modified by topology and other features)."', args=[d.arg(name='internalTrafficPolicy', type=d.T.string)]),
              withInternalTrafficPolicy(internalTrafficPolicy): { serviceTemplate+: { spec+: { internalTrafficPolicy: internalTrafficPolicy } } },
              '#withIpFamilies':: d.fn(help='"IPFamilies is a list of IP families (e.g. IPv4, IPv6) assigned to this\\nservice. This field is usually assigned automatically based on cluster\\nconfiguration and the ipFamilyPolicy field. If this field is specified\\nmanually, the requested family is available in the cluster,\\nand ipFamilyPolicy allows it, it will be used; otherwise creation of\\nthe service will fail. This field is conditionally mutable: it allows\\nfor adding or removing a secondary IP family, but it does not allow\\nchanging the primary IP family of the Service. Valid values are \\"IPv4\\"\\nand \\"IPv6\\".  This field only applies to Services of types ClusterIP,\\nNodePort, and LoadBalancer, and does apply to \\"headless\\" services.\\nThis field will be wiped when updating a Service to type ExternalName.\\n\\nThis field may hold a maximum of two entries (dual-stack families, in\\neither order).  These families must correspond to the values of the\\nclusterIPs field, if specified. Both clusterIPs and ipFamilies are\\ngoverned by the ipFamilyPolicy field."', args=[d.arg(name='ipFamilies', type=d.T.array)]),
              withIpFamilies(ipFamilies): { serviceTemplate+: { spec+: { ipFamilies: if std.isArray(v=ipFamilies) then ipFamilies else [ipFamilies] } } },
              '#withIpFamiliesMixin':: d.fn(help='"IPFamilies is a list of IP families (e.g. IPv4, IPv6) assigned to this\\nservice. This field is usually assigned automatically based on cluster\\nconfiguration and the ipFamilyPolicy field. If this field is specified\\nmanually, the requested family is available in the cluster,\\nand ipFamilyPolicy allows it, it will be used; otherwise creation of\\nthe service will fail. This field is conditionally mutable: it allows\\nfor adding or removing a secondary IP family, but it does not allow\\nchanging the primary IP family of the Service. Valid values are \\"IPv4\\"\\nand \\"IPv6\\".  This field only applies to Services of types ClusterIP,\\nNodePort, and LoadBalancer, and does apply to \\"headless\\" services.\\nThis field will be wiped when updating a Service to type ExternalName.\\n\\nThis field may hold a maximum of two entries (dual-stack families, in\\neither order).  These families must correspond to the values of the\\nclusterIPs field, if specified. Both clusterIPs and ipFamilies are\\ngoverned by the ipFamilyPolicy field."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ipFamilies', type=d.T.array)]),
              withIpFamiliesMixin(ipFamilies): { serviceTemplate+: { spec+: { ipFamilies+: if std.isArray(v=ipFamilies) then ipFamilies else [ipFamilies] } } },
              '#withIpFamilyPolicy':: d.fn(help='"IPFamilyPolicy represents the dual-stack-ness requested or required by\\nthis Service. If there is no value provided, then this field will be set\\nto SingleStack. Services can be \\"SingleStack\\" (a single IP family),\\n\\"PreferDualStack\\" (two IP families on dual-stack configured clusters or\\na single IP family on single-stack clusters), or \\"RequireDualStack\\"\\n(two IP families on dual-stack configured clusters, otherwise fail). The\\nipFamilies and clusterIPs fields depend on the value of this field. This\\nfield will be wiped when updating a service to type ExternalName."', args=[d.arg(name='ipFamilyPolicy', type=d.T.string)]),
              withIpFamilyPolicy(ipFamilyPolicy): { serviceTemplate+: { spec+: { ipFamilyPolicy: ipFamilyPolicy } } },
              '#withLoadBalancerClass':: d.fn(help="\"loadBalancerClass is the class of the load balancer implementation this Service belongs to.\\nIf specified, the value of this field must be a label-style identifier, with an optional prefix,\\ne.g. \\\"internal-vip\\\" or \\\"example.com/internal-vip\\\". Unprefixed names are reserved for end-users.\\nThis field can only be set when the Service type is 'LoadBalancer'. If not set, the default load\\nbalancer implementation is used, today this is typically done through the cloud provider integration,\\nbut should apply for any default implementation. If set, it is assumed that a load balancer\\nimplementation is watching for Services with a matching class. Any default load balancer\\nimplementation (e.g. cloud providers) should ignore Services that set this field.\\nThis field can only be set when creating or updating a Service to type 'LoadBalancer'.\\nOnce set, it can not be changed. This field will be wiped when a service is updated to a non 'LoadBalancer' type.\"", args=[d.arg(name='loadBalancerClass', type=d.T.string)]),
              withLoadBalancerClass(loadBalancerClass): { serviceTemplate+: { spec+: { loadBalancerClass: loadBalancerClass } } },
              '#withLoadBalancerIP':: d.fn(help='"Only applies to Service Type: LoadBalancer.\\nThis feature depends on whether the underlying cloud-provider supports specifying\\nthe loadBalancerIP when a load balancer is created.\\nThis field will be ignored if the cloud-provider does not support the feature.\\nDeprecated: This field was under-specified and its meaning varies across implementations.\\nUsing it is non-portable and it may not support dual-stack.\\nUsers are encouraged to use implementation-specific annotations when available."', args=[d.arg(name='loadBalancerIP', type=d.T.string)]),
              withLoadBalancerIP(loadBalancerIP): { serviceTemplate+: { spec+: { loadBalancerIP: loadBalancerIP } } },
              '#withLoadBalancerSourceRanges':: d.fn(help='"If specified and supported by the platform, this will restrict traffic through the cloud-provider\\nload-balancer will be restricted to the specified client IPs. This field will be ignored if the\\ncloud-provider does not support the feature.\\"\\nMore info: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/"', args=[d.arg(name='loadBalancerSourceRanges', type=d.T.array)]),
              withLoadBalancerSourceRanges(loadBalancerSourceRanges): { serviceTemplate+: { spec+: { loadBalancerSourceRanges: if std.isArray(v=loadBalancerSourceRanges) then loadBalancerSourceRanges else [loadBalancerSourceRanges] } } },
              '#withLoadBalancerSourceRangesMixin':: d.fn(help='"If specified and supported by the platform, this will restrict traffic through the cloud-provider\\nload-balancer will be restricted to the specified client IPs. This field will be ignored if the\\ncloud-provider does not support the feature.\\"\\nMore info: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='loadBalancerSourceRanges', type=d.T.array)]),
              withLoadBalancerSourceRangesMixin(loadBalancerSourceRanges): { serviceTemplate+: { spec+: { loadBalancerSourceRanges+: if std.isArray(v=loadBalancerSourceRanges) then loadBalancerSourceRanges else [loadBalancerSourceRanges] } } },
              '#withPorts':: d.fn(help='"The list of ports that are exposed by this service.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"', args=[d.arg(name='ports', type=d.T.array)]),
              withPorts(ports): { serviceTemplate+: { spec+: { ports: if std.isArray(v=ports) then ports else [ports] } } },
              '#withPortsMixin':: d.fn(help='"The list of ports that are exposed by this service.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ports', type=d.T.array)]),
              withPortsMixin(ports): { serviceTemplate+: { spec+: { ports+: if std.isArray(v=ports) then ports else [ports] } } },
              '#withPublishNotReadyAddresses':: d.fn(help="\"publishNotReadyAddresses indicates that any agent which deals with endpoints for this\\nService should disregard any indications of ready/not-ready.\\nThe primary use case for setting this field is for a StatefulSet's Headless Service to\\npropagate SRV DNS records for its Pods for the purpose of peer discovery.\\nThe Kubernetes controllers that generate Endpoints and EndpointSlice resources for\\nServices interpret this to mean that all endpoints are considered \\\"ready\\\" even if the\\nPods themselves are not. Agents which consume only Kubernetes generated endpoints\\nthrough the Endpoints or EndpointSlice resources can safely assume this behavior.\"", args=[d.arg(name='publishNotReadyAddresses', type=d.T.boolean)]),
              withPublishNotReadyAddresses(publishNotReadyAddresses): { serviceTemplate+: { spec+: { publishNotReadyAddresses: publishNotReadyAddresses } } },
              '#withSelector':: d.fn(help='"Route service traffic to pods with label keys and values matching this\\nselector. If empty or not present, the service is assumed to have an\\nexternal process managing its endpoints, which Kubernetes will not\\nmodify. Only applies to types ClusterIP, NodePort, and LoadBalancer.\\nIgnored if type is ExternalName.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/"', args=[d.arg(name='selector', type=d.T.object)]),
              withSelector(selector): { serviceTemplate+: { spec+: { selector: selector } } },
              '#withSelectorMixin':: d.fn(help='"Route service traffic to pods with label keys and values matching this\\nselector. If empty or not present, the service is assumed to have an\\nexternal process managing its endpoints, which Kubernetes will not\\nmodify. Only applies to types ClusterIP, NodePort, and LoadBalancer.\\nIgnored if type is ExternalName.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='selector', type=d.T.object)]),
              withSelectorMixin(selector): { serviceTemplate+: { spec+: { selector+: selector } } },
              '#withSessionAffinity':: d.fn(help='"Supports \\"ClientIP\\" and \\"None\\". Used to maintain session affinity.\\nEnable client IP based session affinity.\\nMust be ClientIP or None.\\nDefaults to None.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies"', args=[d.arg(name='sessionAffinity', type=d.T.string)]),
              withSessionAffinity(sessionAffinity): { serviceTemplate+: { spec+: { sessionAffinity: sessionAffinity } } },
              '#withTrafficDistribution':: d.fn(help='"TrafficDistribution offers a way to express preferences for how traffic is\\ndistributed to Service endpoints. Implementations can use this field as a\\nhint, but are not required to guarantee strict adherence. If the field is\\nnot set, the implementation will apply its default routing strategy. If set\\nto \\"PreferClose\\", implementations should prioritize endpoints that are\\ntopologically close (e.g., same zone).\\nThis is a beta field and requires enabling ServiceTrafficDistribution feature."', args=[d.arg(name='trafficDistribution', type=d.T.string)]),
              withTrafficDistribution(trafficDistribution): { serviceTemplate+: { spec+: { trafficDistribution: trafficDistribution } } },
              '#withType':: d.fn(help='"type determines how the Service is exposed. Defaults to ClusterIP. Valid\\noptions are ExternalName, ClusterIP, NodePort, and LoadBalancer.\\n\\"ClusterIP\\" allocates a cluster-internal IP address for load-balancing\\nto endpoints. Endpoints are determined by the selector or if that is not\\nspecified, by manual construction of an Endpoints object or\\nEndpointSlice objects. If clusterIP is \\"None\\", no virtual IP is\\nallocated and the endpoints are published as a set of endpoints rather\\nthan a virtual IP.\\n\\"NodePort\\" builds on ClusterIP and allocates a port on every node which\\nroutes to the same endpoints as the clusterIP.\\n\\"LoadBalancer\\" builds on NodePort and creates an external load-balancer\\n(if supported in the current cloud) which routes to the same endpoints\\nas the clusterIP.\\n\\"ExternalName\\" aliases this service to the specified externalName.\\nSeveral other fields do not apply to ExternalName services.\\nMore info: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types"', args=[d.arg(name='type', type=d.T.string)]),
              withType(type): { serviceTemplate+: { spec+: { type: type } } },
            },
          },
          '#withSelectorType':: d.fn(help='"SelectorType specifies the type of selectors that the service will have.\\nValid values are \\"rw\\", \\"r\\", and \\"ro\\", representing read-write, read, and read-only services."', args=[d.arg(name='selectorType', type=d.T.string)]),
          withSelectorType(selectorType): { selectorType: selectorType },
          '#withUpdateStrategy':: d.fn(help='"UpdateStrategy describes how the service differences should be reconciled"', args=[d.arg(name='updateStrategy', type=d.T.string)]),
          withUpdateStrategy(updateStrategy): { updateStrategy: updateStrategy },
        },
        '#withAdditional':: d.fn(help='"Additional is a list of additional managed services specified by the user."', args=[d.arg(name='additional', type=d.T.array)]),
        withAdditional(additional): { spec+: { managed+: { services+: { additional: if std.isArray(v=additional) then additional else [additional] } } } },
        '#withAdditionalMixin':: d.fn(help='"Additional is a list of additional managed services specified by the user."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='additional', type=d.T.array)]),
        withAdditionalMixin(additional): { spec+: { managed+: { services+: { additional+: if std.isArray(v=additional) then additional else [additional] } } } },
        '#withDisabledDefaultServices':: d.fn(help='"DisabledDefaultServices is a list of service types that are disabled by default.\\nValid values are \\"r\\", and \\"ro\\", representing read, and read-only services."', args=[d.arg(name='disabledDefaultServices', type=d.T.array)]),
        withDisabledDefaultServices(disabledDefaultServices): { spec+: { managed+: { services+: { disabledDefaultServices: if std.isArray(v=disabledDefaultServices) then disabledDefaultServices else [disabledDefaultServices] } } } },
        '#withDisabledDefaultServicesMixin':: d.fn(help='"DisabledDefaultServices is a list of service types that are disabled by default.\\nValid values are \\"r\\", and \\"ro\\", representing read, and read-only services."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='disabledDefaultServices', type=d.T.array)]),
        withDisabledDefaultServicesMixin(disabledDefaultServices): { spec+: { managed+: { services+: { disabledDefaultServices+: if std.isArray(v=disabledDefaultServices) then disabledDefaultServices else [disabledDefaultServices] } } } },
      },
      '#withRoles':: d.fn(help='"Database roles managed by the `Cluster`"', args=[d.arg(name='roles', type=d.T.array)]),
      withRoles(roles): { spec+: { managed+: { roles: if std.isArray(v=roles) then roles else [roles] } } },
      '#withRolesMixin':: d.fn(help='"Database roles managed by the `Cluster`"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='roles', type=d.T.array)]),
      withRolesMixin(roles): { spec+: { managed+: { roles+: if std.isArray(v=roles) then roles else [roles] } } },
    },
    '#monitoring':: d.obj(help='"The configuration of the monitoring infrastructure of this cluster"'),
    monitoring: {
      '#customQueriesConfigMap':: d.obj(help='"The list of config maps containing the custom queries"'),
      customQueriesConfigMap: {
        '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { key: key },
        '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
      },
      '#customQueriesSecret':: d.obj(help='"The list of secrets containing the custom queries"'),
      customQueriesSecret: {
        '#withKey':: d.fn(help='"The key to select"', args=[d.arg(name='key', type=d.T.string)]),
        withKey(key): { key: key },
        '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
      },
      '#podMonitorMetricRelabelings':: d.obj(help='"The list of metric relabelings for the `PodMonitor`. Applied to samples before ingestion."'),
      podMonitorMetricRelabelings: {
        '#withAction':: d.fn(help='"Action to perform based on the regex matching.\\n\\n`Uppercase` and `Lowercase` actions require Prometheus >= v2.36.0.\\n`DropEqual` and `KeepEqual` actions require Prometheus >= v2.41.0.\\n\\nDefault: \\"Replace\\', args=[d.arg(name='action', type=d.T.string)]),
        withAction(action): { action: action },
        '#withModulus':: d.fn(help='"Modulus to take of the hash of the source label values.\\n\\nOnly applicable when the action is `HashMod`."', args=[d.arg(name='modulus', type=d.T.integer)]),
        withModulus(modulus): { modulus: modulus },
        '#withRegex':: d.fn(help='"Regular expression against which the extracted value is matched."', args=[d.arg(name='regex', type=d.T.string)]),
        withRegex(regex): { regex: regex },
        '#withReplacement':: d.fn(help='"Replacement value against which a Replace action is performed if the\\nregular expression matches.\\n\\nRegex capture groups are available."', args=[d.arg(name='replacement', type=d.T.string)]),
        withReplacement(replacement): { replacement: replacement },
        '#withSeparator':: d.fn(help='"Separator is the string between concatenated SourceLabels."', args=[d.arg(name='separator', type=d.T.string)]),
        withSeparator(separator): { separator: separator },
        '#withSourceLabels':: d.fn(help='"The source labels select values from existing labels. Their content is\\nconcatenated using the configured Separator and matched against the\\nconfigured regular expression."', args=[d.arg(name='sourceLabels', type=d.T.array)]),
        withSourceLabels(sourceLabels): { sourceLabels: if std.isArray(v=sourceLabels) then sourceLabels else [sourceLabels] },
        '#withSourceLabelsMixin':: d.fn(help='"The source labels select values from existing labels. Their content is\\nconcatenated using the configured Separator and matched against the\\nconfigured regular expression."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sourceLabels', type=d.T.array)]),
        withSourceLabelsMixin(sourceLabels): { sourceLabels+: if std.isArray(v=sourceLabels) then sourceLabels else [sourceLabels] },
        '#withTargetLabel':: d.fn(help='"Label to which the resulting string is written in a replacement.\\n\\nIt is mandatory for `Replace`, `HashMod`, `Lowercase`, `Uppercase`,\\n`KeepEqual` and `DropEqual` actions.\\n\\nRegex capture groups are available."', args=[d.arg(name='targetLabel', type=d.T.string)]),
        withTargetLabel(targetLabel): { targetLabel: targetLabel },
      },
      '#podMonitorRelabelings':: d.obj(help='"The list of relabelings for the `PodMonitor`. Applied to samples before scraping."'),
      podMonitorRelabelings: {
        '#withAction':: d.fn(help='"Action to perform based on the regex matching.\\n\\n`Uppercase` and `Lowercase` actions require Prometheus >= v2.36.0.\\n`DropEqual` and `KeepEqual` actions require Prometheus >= v2.41.0.\\n\\nDefault: \\"Replace\\', args=[d.arg(name='action', type=d.T.string)]),
        withAction(action): { action: action },
        '#withModulus':: d.fn(help='"Modulus to take of the hash of the source label values.\\n\\nOnly applicable when the action is `HashMod`."', args=[d.arg(name='modulus', type=d.T.integer)]),
        withModulus(modulus): { modulus: modulus },
        '#withRegex':: d.fn(help='"Regular expression against which the extracted value is matched."', args=[d.arg(name='regex', type=d.T.string)]),
        withRegex(regex): { regex: regex },
        '#withReplacement':: d.fn(help='"Replacement value against which a Replace action is performed if the\\nregular expression matches.\\n\\nRegex capture groups are available."', args=[d.arg(name='replacement', type=d.T.string)]),
        withReplacement(replacement): { replacement: replacement },
        '#withSeparator':: d.fn(help='"Separator is the string between concatenated SourceLabels."', args=[d.arg(name='separator', type=d.T.string)]),
        withSeparator(separator): { separator: separator },
        '#withSourceLabels':: d.fn(help='"The source labels select values from existing labels. Their content is\\nconcatenated using the configured Separator and matched against the\\nconfigured regular expression."', args=[d.arg(name='sourceLabels', type=d.T.array)]),
        withSourceLabels(sourceLabels): { sourceLabels: if std.isArray(v=sourceLabels) then sourceLabels else [sourceLabels] },
        '#withSourceLabelsMixin':: d.fn(help='"The source labels select values from existing labels. Their content is\\nconcatenated using the configured Separator and matched against the\\nconfigured regular expression."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sourceLabels', type=d.T.array)]),
        withSourceLabelsMixin(sourceLabels): { sourceLabels+: if std.isArray(v=sourceLabels) then sourceLabels else [sourceLabels] },
        '#withTargetLabel':: d.fn(help='"Label to which the resulting string is written in a replacement.\\n\\nIt is mandatory for `Replace`, `HashMod`, `Lowercase`, `Uppercase`,\\n`KeepEqual` and `DropEqual` actions.\\n\\nRegex capture groups are available."', args=[d.arg(name='targetLabel', type=d.T.string)]),
        withTargetLabel(targetLabel): { targetLabel: targetLabel },
      },
      '#tls':: d.obj(help='"Configure TLS communication for the metrics endpoint.\\nChanging tls.enabled option will force a rollout of all instances."'),
      tls: {
        '#withEnabled':: d.fn(help='"Enable TLS for the monitoring endpoint.\\nChanging this option will force a rollout of all instances."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { spec+: { monitoring+: { tls+: { enabled: enabled } } } },
      },
      '#withCustomQueriesConfigMap':: d.fn(help='"The list of config maps containing the custom queries"', args=[d.arg(name='customQueriesConfigMap', type=d.T.array)]),
      withCustomQueriesConfigMap(customQueriesConfigMap): { spec+: { monitoring+: { customQueriesConfigMap: if std.isArray(v=customQueriesConfigMap) then customQueriesConfigMap else [customQueriesConfigMap] } } },
      '#withCustomQueriesConfigMapMixin':: d.fn(help='"The list of config maps containing the custom queries"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='customQueriesConfigMap', type=d.T.array)]),
      withCustomQueriesConfigMapMixin(customQueriesConfigMap): { spec+: { monitoring+: { customQueriesConfigMap+: if std.isArray(v=customQueriesConfigMap) then customQueriesConfigMap else [customQueriesConfigMap] } } },
      '#withCustomQueriesSecret':: d.fn(help='"The list of secrets containing the custom queries"', args=[d.arg(name='customQueriesSecret', type=d.T.array)]),
      withCustomQueriesSecret(customQueriesSecret): { spec+: { monitoring+: { customQueriesSecret: if std.isArray(v=customQueriesSecret) then customQueriesSecret else [customQueriesSecret] } } },
      '#withCustomQueriesSecretMixin':: d.fn(help='"The list of secrets containing the custom queries"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='customQueriesSecret', type=d.T.array)]),
      withCustomQueriesSecretMixin(customQueriesSecret): { spec+: { monitoring+: { customQueriesSecret+: if std.isArray(v=customQueriesSecret) then customQueriesSecret else [customQueriesSecret] } } },
      '#withDisableDefaultQueries':: d.fn(help="\"Whether the default queries should be injected.\\nSet it to `true` if you don't want to inject default queries into the cluster.\\nDefault: false.\"", args=[d.arg(name='disableDefaultQueries', type=d.T.boolean)]),
      withDisableDefaultQueries(disableDefaultQueries): { spec+: { monitoring+: { disableDefaultQueries: disableDefaultQueries } } },
      '#withEnablePodMonitor':: d.fn(help='"Enable or disable the `PodMonitor`"', args=[d.arg(name='enablePodMonitor', type=d.T.boolean)]),
      withEnablePodMonitor(enablePodMonitor): { spec+: { monitoring+: { enablePodMonitor: enablePodMonitor } } },
      '#withPodMonitorMetricRelabelings':: d.fn(help='"The list of metric relabelings for the `PodMonitor`. Applied to samples before ingestion."', args=[d.arg(name='podMonitorMetricRelabelings', type=d.T.array)]),
      withPodMonitorMetricRelabelings(podMonitorMetricRelabelings): { spec+: { monitoring+: { podMonitorMetricRelabelings: if std.isArray(v=podMonitorMetricRelabelings) then podMonitorMetricRelabelings else [podMonitorMetricRelabelings] } } },
      '#withPodMonitorMetricRelabelingsMixin':: d.fn(help='"The list of metric relabelings for the `PodMonitor`. Applied to samples before ingestion."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='podMonitorMetricRelabelings', type=d.T.array)]),
      withPodMonitorMetricRelabelingsMixin(podMonitorMetricRelabelings): { spec+: { monitoring+: { podMonitorMetricRelabelings+: if std.isArray(v=podMonitorMetricRelabelings) then podMonitorMetricRelabelings else [podMonitorMetricRelabelings] } } },
      '#withPodMonitorRelabelings':: d.fn(help='"The list of relabelings for the `PodMonitor`. Applied to samples before scraping."', args=[d.arg(name='podMonitorRelabelings', type=d.T.array)]),
      withPodMonitorRelabelings(podMonitorRelabelings): { spec+: { monitoring+: { podMonitorRelabelings: if std.isArray(v=podMonitorRelabelings) then podMonitorRelabelings else [podMonitorRelabelings] } } },
      '#withPodMonitorRelabelingsMixin':: d.fn(help='"The list of relabelings for the `PodMonitor`. Applied to samples before scraping."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='podMonitorRelabelings', type=d.T.array)]),
      withPodMonitorRelabelingsMixin(podMonitorRelabelings): { spec+: { monitoring+: { podMonitorRelabelings+: if std.isArray(v=podMonitorRelabelings) then podMonitorRelabelings else [podMonitorRelabelings] } } },
    },
    '#nodeMaintenanceWindow':: d.obj(help='"Define a maintenance window for the Kubernetes nodes"'),
    nodeMaintenanceWindow: {
      '#withInProgress':: d.fn(help='"Is there a node maintenance activity in progress?"', args=[d.arg(name='inProgress', type=d.T.boolean)]),
      withInProgress(inProgress): { spec+: { nodeMaintenanceWindow+: { inProgress: inProgress } } },
      '#withReusePVC':: d.fn(help='"Reuse the existing PVC (wait for the node to come\\nup again) or not (recreate it elsewhere - when `instances` >1)"', args=[d.arg(name='reusePVC', type=d.T.boolean)]),
      withReusePVC(reusePVC): { spec+: { nodeMaintenanceWindow+: { reusePVC: reusePVC } } },
    },
    '#plugins':: d.obj(help='"The plugins configuration, containing\\nany plugin to be loaded with the corresponding configuration"'),
    plugins: {
      '#withEnabled':: d.fn(help='"Enabled is true if this plugin will be used"', args=[d.arg(name='enabled', type=d.T.boolean)]),
      withEnabled(enabled): { enabled: enabled },
      '#withIsWALArchiver':: d.fn(help='"Only one plugin can be declared as WALArchiver.\\nCannot be active if \\".spec.backup.barmanObjectStore\\" configuration is present."', args=[d.arg(name='isWALArchiver', type=d.T.boolean)]),
      withIsWALArchiver(isWALArchiver): { isWALArchiver: isWALArchiver },
      '#withName':: d.fn(help='"Name is the plugin name"', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { name: name },
      '#withParameters':: d.fn(help='"Parameters is the configuration of the plugin"', args=[d.arg(name='parameters', type=d.T.object)]),
      withParameters(parameters): { parameters: parameters },
      '#withParametersMixin':: d.fn(help='"Parameters is the configuration of the plugin"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.object)]),
      withParametersMixin(parameters): { parameters+: parameters },
    },
    '#postgresql':: d.obj(help='"Configuration of the PostgreSQL server"'),
    postgresql: {
      '#ldap':: d.obj(help='"Options to specify LDAP configuration"'),
      ldap: {
        '#bindAsAuth':: d.obj(help='"Bind as authentication configuration"'),
        bindAsAuth: {
          '#withPrefix':: d.fn(help='"Prefix for the bind authentication option"', args=[d.arg(name='prefix', type=d.T.string)]),
          withPrefix(prefix): { spec+: { postgresql+: { ldap+: { bindAsAuth+: { prefix: prefix } } } } },
          '#withSuffix':: d.fn(help='"Suffix for the bind authentication option"', args=[d.arg(name='suffix', type=d.T.string)]),
          withSuffix(suffix): { spec+: { postgresql+: { ldap+: { bindAsAuth+: { suffix: suffix } } } } },
        },
        '#bindSearchAuth':: d.obj(help='"Bind+Search authentication configuration"'),
        bindSearchAuth: {
          '#bindPassword':: d.obj(help='"Secret with the password for the user to bind to the directory"'),
          bindPassword: {
            '#withKey':: d.fn(help='"The key of the secret to select from.  Must be a valid secret key."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { bindPassword+: { key: key } } } } } },
            '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { bindPassword+: { name: name } } } } } },
            '#withOptional':: d.fn(help='"Specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
            withOptional(optional): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { bindPassword+: { optional: optional } } } } } },
          },
          '#withBaseDN':: d.fn(help='"Root DN to begin the user search"', args=[d.arg(name='baseDN', type=d.T.string)]),
          withBaseDN(baseDN): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { baseDN: baseDN } } } } },
          '#withBindDN':: d.fn(help='"DN of the user to bind to the directory"', args=[d.arg(name='bindDN', type=d.T.string)]),
          withBindDN(bindDN): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { bindDN: bindDN } } } } },
          '#withSearchAttribute':: d.fn(help='"Attribute to match against the username"', args=[d.arg(name='searchAttribute', type=d.T.string)]),
          withSearchAttribute(searchAttribute): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { searchAttribute: searchAttribute } } } } },
          '#withSearchFilter':: d.fn(help='"Search filter to use when doing the search+bind authentication"', args=[d.arg(name='searchFilter', type=d.T.string)]),
          withSearchFilter(searchFilter): { spec+: { postgresql+: { ldap+: { bindSearchAuth+: { searchFilter: searchFilter } } } } },
        },
        '#withPort':: d.fn(help='"LDAP server port"', args=[d.arg(name='port', type=d.T.integer)]),
        withPort(port): { spec+: { postgresql+: { ldap+: { port: port } } } },
        '#withScheme':: d.fn(help='"LDAP schema to be used, possible options are `ldap` and `ldaps`"', args=[d.arg(name='scheme', type=d.T.string)]),
        withScheme(scheme): { spec+: { postgresql+: { ldap+: { scheme: scheme } } } },
        '#withServer':: d.fn(help='"LDAP hostname or IP address"', args=[d.arg(name='server', type=d.T.string)]),
        withServer(server): { spec+: { postgresql+: { ldap+: { server: server } } } },
        '#withTls':: d.fn(help="\"Set to 'true' to enable LDAP over TLS. 'false' is default\"", args=[d.arg(name='tls', type=d.T.boolean)]),
        withTls(tls): { spec+: { postgresql+: { ldap+: { tls: tls } } } },
      },
      '#syncReplicaElectionConstraint':: d.obj(help='"Requirements to be met by sync replicas. This will affect how the \\"synchronous_standby_names\\" parameter will be\\nset up."'),
      syncReplicaElectionConstraint: {
        '#withEnabled':: d.fn(help='"This flag enables the constraints for sync replicas"', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { spec+: { postgresql+: { syncReplicaElectionConstraint+: { enabled: enabled } } } },
        '#withNodeLabelsAntiAffinity':: d.fn(help='"A list of node labels values to extract and compare to evaluate if the pods reside in the same topology or not"', args=[d.arg(name='nodeLabelsAntiAffinity', type=d.T.array)]),
        withNodeLabelsAntiAffinity(nodeLabelsAntiAffinity): { spec+: { postgresql+: { syncReplicaElectionConstraint+: { nodeLabelsAntiAffinity: if std.isArray(v=nodeLabelsAntiAffinity) then nodeLabelsAntiAffinity else [nodeLabelsAntiAffinity] } } } },
        '#withNodeLabelsAntiAffinityMixin':: d.fn(help='"A list of node labels values to extract and compare to evaluate if the pods reside in the same topology or not"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nodeLabelsAntiAffinity', type=d.T.array)]),
        withNodeLabelsAntiAffinityMixin(nodeLabelsAntiAffinity): { spec+: { postgresql+: { syncReplicaElectionConstraint+: { nodeLabelsAntiAffinity+: if std.isArray(v=nodeLabelsAntiAffinity) then nodeLabelsAntiAffinity else [nodeLabelsAntiAffinity] } } } },
      },
      '#synchronous':: d.obj(help='"Configuration of the PostgreSQL synchronous replication feature"'),
      synchronous: {
        '#withDataDurability':: d.fn(help='"If set to \\"required\\", data durability is strictly enforced. Write operations\\nwith synchronous commit settings (`on`, `remote_write`, or `remote_apply`) will\\nblock if there are insufficient healthy replicas, ensuring data persistence.\\nIf set to \\"preferred\\", data durability is maintained when healthy replicas\\nare available, but the required number of instances will adjust dynamically\\nif replicas become unavailable. This setting relaxes strict durability enforcement\\nto allow for operational continuity. This setting is only applicable if both\\n`standbyNamesPre` and `standbyNamesPost` are unset (empty)."', args=[d.arg(name='dataDurability', type=d.T.string)]),
        withDataDurability(dataDurability): { spec+: { postgresql+: { synchronous+: { dataDurability: dataDurability } } } },
        '#withMaxStandbyNamesFromCluster':: d.fn(help='"Specifies the maximum number of local cluster pods that can be\\nautomatically included in the `synchronous_standby_names` option in\\nPostgreSQL."', args=[d.arg(name='maxStandbyNamesFromCluster', type=d.T.integer)]),
        withMaxStandbyNamesFromCluster(maxStandbyNamesFromCluster): { spec+: { postgresql+: { synchronous+: { maxStandbyNamesFromCluster: maxStandbyNamesFromCluster } } } },
        '#withMethod':: d.fn(help="\"Method to select synchronous replication standbys from the listed\\nservers, accepting 'any' (quorum-based synchronous replication) or\\n'first' (priority-based synchronous replication) as values.\"", args=[d.arg(name='method', type=d.T.string)]),
        withMethod(method): { spec+: { postgresql+: { synchronous+: { method: method } } } },
        '#withNumber':: d.fn(help='"Specifies the number of synchronous standby servers that\\ntransactions must wait for responses from."', args=[d.arg(name='number', type=d.T.integer)]),
        withNumber(number): { spec+: { postgresql+: { synchronous+: { number: number } } } },
        '#withStandbyNamesPost':: d.fn(help='"A user-defined list of application names to be added to\\n`synchronous_standby_names` after local cluster pods (the order is\\nonly useful for priority-based synchronous replication)."', args=[d.arg(name='standbyNamesPost', type=d.T.array)]),
        withStandbyNamesPost(standbyNamesPost): { spec+: { postgresql+: { synchronous+: { standbyNamesPost: if std.isArray(v=standbyNamesPost) then standbyNamesPost else [standbyNamesPost] } } } },
        '#withStandbyNamesPostMixin':: d.fn(help='"A user-defined list of application names to be added to\\n`synchronous_standby_names` after local cluster pods (the order is\\nonly useful for priority-based synchronous replication)."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='standbyNamesPost', type=d.T.array)]),
        withStandbyNamesPostMixin(standbyNamesPost): { spec+: { postgresql+: { synchronous+: { standbyNamesPost+: if std.isArray(v=standbyNamesPost) then standbyNamesPost else [standbyNamesPost] } } } },
        '#withStandbyNamesPre':: d.fn(help='"A user-defined list of application names to be added to\\n`synchronous_standby_names` before local cluster pods (the order is\\nonly useful for priority-based synchronous replication)."', args=[d.arg(name='standbyNamesPre', type=d.T.array)]),
        withStandbyNamesPre(standbyNamesPre): { spec+: { postgresql+: { synchronous+: { standbyNamesPre: if std.isArray(v=standbyNamesPre) then standbyNamesPre else [standbyNamesPre] } } } },
        '#withStandbyNamesPreMixin':: d.fn(help='"A user-defined list of application names to be added to\\n`synchronous_standby_names` before local cluster pods (the order is\\nonly useful for priority-based synchronous replication)."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='standbyNamesPre', type=d.T.array)]),
        withStandbyNamesPreMixin(standbyNamesPre): { spec+: { postgresql+: { synchronous+: { standbyNamesPre+: if std.isArray(v=standbyNamesPre) then standbyNamesPre else [standbyNamesPre] } } } },
      },
      '#withEnableAlterSystem':: d.fn(help='"If this parameter is true, the user will be able to invoke `ALTER SYSTEM`\\non this CloudNativePG Cluster.\\nThis should only be used for debugging and troubleshooting.\\nDefaults to false."', args=[d.arg(name='enableAlterSystem', type=d.T.boolean)]),
      withEnableAlterSystem(enableAlterSystem): { spec+: { postgresql+: { enableAlterSystem: enableAlterSystem } } },
      '#withParameters':: d.fn(help='"PostgreSQL configuration options (postgresql.conf)"', args=[d.arg(name='parameters', type=d.T.object)]),
      withParameters(parameters): { spec+: { postgresql+: { parameters: parameters } } },
      '#withParametersMixin':: d.fn(help='"PostgreSQL configuration options (postgresql.conf)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.object)]),
      withParametersMixin(parameters): { spec+: { postgresql+: { parameters+: parameters } } },
      '#withPg_hba':: d.fn(help='"PostgreSQL Host Based Authentication rules (lines to be appended\\nto the pg_hba.conf file)"', args=[d.arg(name='pg_hba', type=d.T.array)]),
      withPg_hba(pg_hba): { spec+: { postgresql+: { pg_hba: if std.isArray(v=pg_hba) then pg_hba else [pg_hba] } } },
      '#withPg_hbaMixin':: d.fn(help='"PostgreSQL Host Based Authentication rules (lines to be appended\\nto the pg_hba.conf file)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pg_hba', type=d.T.array)]),
      withPg_hbaMixin(pg_hba): { spec+: { postgresql+: { pg_hba+: if std.isArray(v=pg_hba) then pg_hba else [pg_hba] } } },
      '#withPg_ident':: d.fn(help='"PostgreSQL User Name Maps rules (lines to be appended\\nto the pg_ident.conf file)"', args=[d.arg(name='pg_ident', type=d.T.array)]),
      withPg_ident(pg_ident): { spec+: { postgresql+: { pg_ident: if std.isArray(v=pg_ident) then pg_ident else [pg_ident] } } },
      '#withPg_identMixin':: d.fn(help='"PostgreSQL User Name Maps rules (lines to be appended\\nto the pg_ident.conf file)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pg_ident', type=d.T.array)]),
      withPg_identMixin(pg_ident): { spec+: { postgresql+: { pg_ident+: if std.isArray(v=pg_ident) then pg_ident else [pg_ident] } } },
      '#withPromotionTimeout':: d.fn(help='"Specifies the maximum number of seconds to wait when promoting an instance to primary.\\nDefault value is 40000000, greater than one year in seconds,\\nbig enough to simulate an infinite timeout"', args=[d.arg(name='promotionTimeout', type=d.T.integer)]),
      withPromotionTimeout(promotionTimeout): { spec+: { postgresql+: { promotionTimeout: promotionTimeout } } },
      '#withShared_preload_libraries':: d.fn(help='"Lists of shared preload libraries to add to the default ones"', args=[d.arg(name='shared_preload_libraries', type=d.T.array)]),
      withShared_preload_libraries(shared_preload_libraries): { spec+: { postgresql+: { shared_preload_libraries: if std.isArray(v=shared_preload_libraries) then shared_preload_libraries else [shared_preload_libraries] } } },
      '#withShared_preload_librariesMixin':: d.fn(help='"Lists of shared preload libraries to add to the default ones"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='shared_preload_libraries', type=d.T.array)]),
      withShared_preload_librariesMixin(shared_preload_libraries): { spec+: { postgresql+: { shared_preload_libraries+: if std.isArray(v=shared_preload_libraries) then shared_preload_libraries else [shared_preload_libraries] } } },
    },
    '#probes':: d.obj(help='"The configuration of the probes to be injected\\nin the PostgreSQL Pods."'),
    probes: {
      '#liveness':: d.obj(help='"The liveness probe configuration"'),
      liveness: {
        '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded.\\nDefaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
        withFailureThreshold(failureThreshold): { spec+: { probes+: { liveness+: { failureThreshold: failureThreshold } } } },
        '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
        withInitialDelaySeconds(initialDelaySeconds): { spec+: { probes+: { liveness+: { initialDelaySeconds: initialDelaySeconds } } } },
        '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe.\\nDefault to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
        withPeriodSeconds(periodSeconds): { spec+: { probes+: { liveness+: { periodSeconds: periodSeconds } } } },
        '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed.\\nDefaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
        withSuccessThreshold(successThreshold): { spec+: { probes+: { liveness+: { successThreshold: successThreshold } } } },
        '#withTerminationGracePeriodSeconds':: d.fn(help="\"Optional duration in seconds the pod needs to terminate gracefully upon probe failure.\\nThe grace period is the duration in seconds after the processes running in the pod are sent\\na termination signal and the time when the processes are forcibly halted with a kill signal.\\nSet this value longer than the expected cleanup time for your process.\\nIf this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this\\nvalue overrides the value provided by the pod spec.\\nValue must be non-negative integer. The value zero indicates stop immediately via\\nthe kill signal (no opportunity to shut down).\\nThis is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.\\nMinimum value is 1. spec.terminationGracePeriodSeconds is used if unset.\"", args=[d.arg(name='terminationGracePeriodSeconds', type=d.T.integer)]),
        withTerminationGracePeriodSeconds(terminationGracePeriodSeconds): { spec+: { probes+: { liveness+: { terminationGracePeriodSeconds: terminationGracePeriodSeconds } } } },
        '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out.\\nDefaults to 1 second. Minimum value is 1.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
        withTimeoutSeconds(timeoutSeconds): { spec+: { probes+: { liveness+: { timeoutSeconds: timeoutSeconds } } } },
      },
      '#readiness':: d.obj(help='"The readiness probe configuration"'),
      readiness: {
        '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded.\\nDefaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
        withFailureThreshold(failureThreshold): { spec+: { probes+: { readiness+: { failureThreshold: failureThreshold } } } },
        '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
        withInitialDelaySeconds(initialDelaySeconds): { spec+: { probes+: { readiness+: { initialDelaySeconds: initialDelaySeconds } } } },
        '#withMaximumLag':: d.fn(help='"Lag limit. Used only for `streaming` strategy"', args=[d.arg(name='maximumLag', type=d.T.any)]),
        withMaximumLag(maximumLag): { spec+: { probes+: { readiness+: { maximumLag: maximumLag } } } },
        '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe.\\nDefault to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
        withPeriodSeconds(periodSeconds): { spec+: { probes+: { readiness+: { periodSeconds: periodSeconds } } } },
        '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed.\\nDefaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
        withSuccessThreshold(successThreshold): { spec+: { probes+: { readiness+: { successThreshold: successThreshold } } } },
        '#withTerminationGracePeriodSeconds':: d.fn(help="\"Optional duration in seconds the pod needs to terminate gracefully upon probe failure.\\nThe grace period is the duration in seconds after the processes running in the pod are sent\\na termination signal and the time when the processes are forcibly halted with a kill signal.\\nSet this value longer than the expected cleanup time for your process.\\nIf this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this\\nvalue overrides the value provided by the pod spec.\\nValue must be non-negative integer. The value zero indicates stop immediately via\\nthe kill signal (no opportunity to shut down).\\nThis is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.\\nMinimum value is 1. spec.terminationGracePeriodSeconds is used if unset.\"", args=[d.arg(name='terminationGracePeriodSeconds', type=d.T.integer)]),
        withTerminationGracePeriodSeconds(terminationGracePeriodSeconds): { spec+: { probes+: { readiness+: { terminationGracePeriodSeconds: terminationGracePeriodSeconds } } } },
        '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out.\\nDefaults to 1 second. Minimum value is 1.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
        withTimeoutSeconds(timeoutSeconds): { spec+: { probes+: { readiness+: { timeoutSeconds: timeoutSeconds } } } },
        '#withType':: d.fn(help='"The probe strategy"', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { spec+: { probes+: { readiness+: { type: type } } } },
      },
      '#startup':: d.obj(help='"The startup probe configuration"'),
      startup: {
        '#withFailureThreshold':: d.fn(help='"Minimum consecutive failures for the probe to be considered failed after having succeeded.\\nDefaults to 3. Minimum value is 1."', args=[d.arg(name='failureThreshold', type=d.T.integer)]),
        withFailureThreshold(failureThreshold): { spec+: { probes+: { startup+: { failureThreshold: failureThreshold } } } },
        '#withInitialDelaySeconds':: d.fn(help='"Number of seconds after the container has started before liveness probes are initiated.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='initialDelaySeconds', type=d.T.integer)]),
        withInitialDelaySeconds(initialDelaySeconds): { spec+: { probes+: { startup+: { initialDelaySeconds: initialDelaySeconds } } } },
        '#withMaximumLag':: d.fn(help='"Lag limit. Used only for `streaming` strategy"', args=[d.arg(name='maximumLag', type=d.T.any)]),
        withMaximumLag(maximumLag): { spec+: { probes+: { startup+: { maximumLag: maximumLag } } } },
        '#withPeriodSeconds':: d.fn(help='"How often (in seconds) to perform the probe.\\nDefault to 10 seconds. Minimum value is 1."', args=[d.arg(name='periodSeconds', type=d.T.integer)]),
        withPeriodSeconds(periodSeconds): { spec+: { probes+: { startup+: { periodSeconds: periodSeconds } } } },
        '#withSuccessThreshold':: d.fn(help='"Minimum consecutive successes for the probe to be considered successful after having failed.\\nDefaults to 1. Must be 1 for liveness and startup. Minimum value is 1."', args=[d.arg(name='successThreshold', type=d.T.integer)]),
        withSuccessThreshold(successThreshold): { spec+: { probes+: { startup+: { successThreshold: successThreshold } } } },
        '#withTerminationGracePeriodSeconds':: d.fn(help="\"Optional duration in seconds the pod needs to terminate gracefully upon probe failure.\\nThe grace period is the duration in seconds after the processes running in the pod are sent\\na termination signal and the time when the processes are forcibly halted with a kill signal.\\nSet this value longer than the expected cleanup time for your process.\\nIf this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this\\nvalue overrides the value provided by the pod spec.\\nValue must be non-negative integer. The value zero indicates stop immediately via\\nthe kill signal (no opportunity to shut down).\\nThis is a beta field and requires enabling ProbeTerminationGracePeriod feature gate.\\nMinimum value is 1. spec.terminationGracePeriodSeconds is used if unset.\"", args=[d.arg(name='terminationGracePeriodSeconds', type=d.T.integer)]),
        withTerminationGracePeriodSeconds(terminationGracePeriodSeconds): { spec+: { probes+: { startup+: { terminationGracePeriodSeconds: terminationGracePeriodSeconds } } } },
        '#withTimeoutSeconds':: d.fn(help='"Number of seconds after which the probe times out.\\nDefaults to 1 second. Minimum value is 1.\\nMore info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"', args=[d.arg(name='timeoutSeconds', type=d.T.integer)]),
        withTimeoutSeconds(timeoutSeconds): { spec+: { probes+: { startup+: { timeoutSeconds: timeoutSeconds } } } },
        '#withType':: d.fn(help='"The probe strategy"', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { spec+: { probes+: { startup+: { type: type } } } },
      },
    },
    '#projectedVolumeTemplate':: d.obj(help='"Template to be used to define projected volumes, projected volumes will be mounted\\nunder `/projected` base folder"'),
    projectedVolumeTemplate: {
      '#sources':: d.obj(help='"sources is the list of volume projections. Each entry in this list\\nhandles one source."'),
      sources: {
        '#clusterTrustBundle':: d.obj(help='"ClusterTrustBundle allows a pod to access the `.spec.trustBundle` field\\nof ClusterTrustBundle objects in an auto-updating file.\\n\\nAlpha, gated by the ClusterTrustBundleProjection feature gate.\\n\\nClusterTrustBundle objects can either be selected by name, or by the\\ncombination of signer name and a label selector.\\n\\nKubelet performs aggressive normalization of the PEM contents written\\ninto the pod filesystem.  Esoteric PEM features such as inter-block\\ncomments and block headers are stripped.  Certificates are deduplicated.\\nThe ordering of certificates within the file is arbitrary, and Kubelet\\nmay change the order over time."'),
        clusterTrustBundle: {
          '#labelSelector':: d.obj(help='"Select all ClusterTrustBundles that match this label selector.  Only has\\neffect if signerName is set.  Mutually-exclusive with name.  If unset,\\ninterpreted as \\"match nothing\\".  If set but empty, interpreted as \\"match\\neverything\\"."'),
          labelSelector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { clusterTrustBundle+: { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { clusterTrustBundle+: { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { clusterTrustBundle+: { labelSelector+: { matchLabels: matchLabels } } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { clusterTrustBundle+: { labelSelector+: { matchLabels+: matchLabels } } },
          },
          '#withName':: d.fn(help='"Select a single ClusterTrustBundle by object name.  Mutually-exclusive\\nwith signerName and labelSelector."', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { clusterTrustBundle+: { name: name } },
          '#withOptional':: d.fn(help="\"If true, don't block pod startup if the referenced ClusterTrustBundle(s)\\naren't available.  If using name, then the named ClusterTrustBundle is\\nallowed not to exist.  If using signerName, then the combination of\\nsignerName and labelSelector is allowed to match zero\\nClusterTrustBundles.\"", args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { clusterTrustBundle+: { optional: optional } },
          '#withPath':: d.fn(help='"Relative path from the volume root to write the bundle."', args=[d.arg(name='path', type=d.T.string)]),
          withPath(path): { clusterTrustBundle+: { path: path } },
          '#withSignerName':: d.fn(help='"Select all ClusterTrustBundles that match this signer name.\\nMutually-exclusive with name.  The contents of all selected\\nClusterTrustBundles will be unified and deduplicated."', args=[d.arg(name='signerName', type=d.T.string)]),
          withSignerName(signerName): { clusterTrustBundle+: { signerName: signerName } },
        },
        '#configMap':: d.obj(help='"configMap information about the configMap data to project"'),
        configMap: {
          '#items':: d.obj(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nConfigMap will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the ConfigMap,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\""),
          items: {
            '#withKey':: d.fn(help='"key is the key to project."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withMode':: d.fn(help='"mode is Optional: mode bits used to set permissions on this file.\\nMust be an octal value between 0000 and 0777 or a decimal value between 0 and 511.\\nYAML accepts both octal and decimal values, JSON requires decimal values for mode bits.\\nIf not specified, the volume defaultMode will be used.\\nThis might be in conflict with other options that affect the file\\nmode, like fsGroup, and the result can be other mode bits set."', args=[d.arg(name='mode', type=d.T.integer)]),
            withMode(mode): { mode: mode },
            '#withPath':: d.fn(help="\"path is the relative path of the file to map the key to.\\nMay not be an absolute path.\\nMay not contain the path element '..'.\\nMay not start with the string '..'.\"", args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { path: path },
          },
          '#withItems':: d.fn(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nConfigMap will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the ConfigMap,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\"", args=[d.arg(name='items', type=d.T.array)]),
          withItems(items): { configMap+: { items: if std.isArray(v=items) then items else [items] } },
          '#withItemsMixin':: d.fn(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nConfigMap will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the ConfigMap,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='items', type=d.T.array)]),
          withItemsMixin(items): { configMap+: { items+: if std.isArray(v=items) then items else [items] } },
          '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { configMap+: { name: name } },
          '#withOptional':: d.fn(help='"optional specify whether the ConfigMap or its keys must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { configMap+: { optional: optional } },
        },
        '#downwardAPI':: d.obj(help='"downwardAPI information about the downwardAPI data to project"'),
        downwardAPI: {
          '#items':: d.obj(help='"Items is a list of DownwardAPIVolume file"'),
          items: {
            '#fieldRef':: d.obj(help='"Required: Selects a field of the pod: only annotations, labels, name, namespace and uid are supported."'),
            fieldRef: {
              '#withApiVersion':: d.fn(help='"Version of the schema the FieldPath is written in terms of, defaults to \\"v1\\"."', args=[d.arg(name='apiVersion', type=d.T.string)]),
              withApiVersion(apiVersion): { fieldRef+: { apiVersion: apiVersion } },
              '#withFieldPath':: d.fn(help='"Path of the field to select in the specified API version."', args=[d.arg(name='fieldPath', type=d.T.string)]),
              withFieldPath(fieldPath): { fieldRef+: { fieldPath: fieldPath } },
            },
            '#resourceFieldRef':: d.obj(help='"Selects a resource of the container: only resources limits and requests\\n(limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported."'),
            resourceFieldRef: {
              '#withContainerName':: d.fn(help='"Container name: required for volumes, optional for env vars"', args=[d.arg(name='containerName', type=d.T.string)]),
              withContainerName(containerName): { resourceFieldRef+: { containerName: containerName } },
              '#withDivisor':: d.fn(help='"Specifies the output format of the exposed resources, defaults to \\"1\\', args=[d.arg(name='divisor', type=d.T.any)]),
              withDivisor(divisor): { resourceFieldRef+: { divisor: divisor } },
              '#withResource':: d.fn(help='"Required: resource to select"', args=[d.arg(name='resource', type=d.T.string)]),
              withResource(resource): { resourceFieldRef+: { resource: resource } },
            },
            '#withMode':: d.fn(help='"Optional: mode bits used to set permissions on this file, must be an octal value\\nbetween 0000 and 0777 or a decimal value between 0 and 511.\\nYAML accepts both octal and decimal values, JSON requires decimal values for mode bits.\\nIf not specified, the volume defaultMode will be used.\\nThis might be in conflict with other options that affect the file\\nmode, like fsGroup, and the result can be other mode bits set."', args=[d.arg(name='mode', type=d.T.integer)]),
            withMode(mode): { mode: mode },
            '#withPath':: d.fn(help="\"Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'\"", args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { path: path },
          },
          '#withItems':: d.fn(help='"Items is a list of DownwardAPIVolume file"', args=[d.arg(name='items', type=d.T.array)]),
          withItems(items): { downwardAPI+: { items: if std.isArray(v=items) then items else [items] } },
          '#withItemsMixin':: d.fn(help='"Items is a list of DownwardAPIVolume file"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='items', type=d.T.array)]),
          withItemsMixin(items): { downwardAPI+: { items+: if std.isArray(v=items) then items else [items] } },
        },
        '#secret':: d.obj(help='"secret information about the secret data to project"'),
        secret: {
          '#items':: d.obj(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nSecret will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the Secret,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\""),
          items: {
            '#withKey':: d.fn(help='"key is the key to project."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withMode':: d.fn(help='"mode is Optional: mode bits used to set permissions on this file.\\nMust be an octal value between 0000 and 0777 or a decimal value between 0 and 511.\\nYAML accepts both octal and decimal values, JSON requires decimal values for mode bits.\\nIf not specified, the volume defaultMode will be used.\\nThis might be in conflict with other options that affect the file\\nmode, like fsGroup, and the result can be other mode bits set."', args=[d.arg(name='mode', type=d.T.integer)]),
            withMode(mode): { mode: mode },
            '#withPath':: d.fn(help="\"path is the relative path of the file to map the key to.\\nMay not be an absolute path.\\nMay not contain the path element '..'.\\nMay not start with the string '..'.\"", args=[d.arg(name='path', type=d.T.string)]),
            withPath(path): { path: path },
          },
          '#withItems':: d.fn(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nSecret will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the Secret,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\"", args=[d.arg(name='items', type=d.T.array)]),
          withItems(items): { secret+: { items: if std.isArray(v=items) then items else [items] } },
          '#withItemsMixin':: d.fn(help="\"items if unspecified, each key-value pair in the Data field of the referenced\\nSecret will be projected into the volume as a file whose name is the\\nkey and content is the value. If specified, the listed keys will be\\nprojected into the specified paths, and unlisted keys will not be\\npresent. If a key is specified which is not present in the Secret,\\nthe volume setup will error unless it is marked optional. Paths must be\\nrelative and may not contain the '..' path or start with '..'.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='items', type=d.T.array)]),
          withItemsMixin(items): { secret+: { items+: if std.isArray(v=items) then items else [items] } },
          '#withName':: d.fn(help='"Name of the referent.\\nThis field is effectively required, but due to backwards compatibility is\\nallowed to be empty. Instances of this type with an empty value here are\\nalmost certainly wrong.\\nMore info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { secret+: { name: name } },
          '#withOptional':: d.fn(help='"optional field specify whether the Secret or its key must be defined"', args=[d.arg(name='optional', type=d.T.boolean)]),
          withOptional(optional): { secret+: { optional: optional } },
        },
        '#serviceAccountToken':: d.obj(help='"serviceAccountToken is information about the serviceAccountToken data to project"'),
        serviceAccountToken: {
          '#withAudience':: d.fn(help='"audience is the intended audience of the token. A recipient of a token\\nmust identify itself with an identifier specified in the audience of the\\ntoken, and otherwise should reject the token. The audience defaults to the\\nidentifier of the apiserver."', args=[d.arg(name='audience', type=d.T.string)]),
          withAudience(audience): { serviceAccountToken+: { audience: audience } },
          '#withExpirationSeconds':: d.fn(help='"expirationSeconds is the requested duration of validity of the service\\naccount token. As the token approaches expiration, the kubelet volume\\nplugin will proactively rotate the service account token. The kubelet will\\nstart trying to rotate the token if the token is older than 80 percent of\\nits time to live or if the token is older than 24 hours.Defaults to 1 hour\\nand must be at least 10 minutes."', args=[d.arg(name='expirationSeconds', type=d.T.integer)]),
          withExpirationSeconds(expirationSeconds): { serviceAccountToken+: { expirationSeconds: expirationSeconds } },
          '#withPath':: d.fn(help='"path is the path relative to the mount point of the file to project the\\ntoken into."', args=[d.arg(name='path', type=d.T.string)]),
          withPath(path): { serviceAccountToken+: { path: path } },
        },
      },
      '#withDefaultMode':: d.fn(help='"defaultMode are the mode bits used to set permissions on created files by default.\\nMust be an octal value between 0000 and 0777 or a decimal value between 0 and 511.\\nYAML accepts both octal and decimal values, JSON requires decimal values for mode bits.\\nDirectories within the path are not affected by this setting.\\nThis might be in conflict with other options that affect the file\\nmode, like fsGroup, and the result can be other mode bits set."', args=[d.arg(name='defaultMode', type=d.T.integer)]),
      withDefaultMode(defaultMode): { spec+: { projectedVolumeTemplate+: { defaultMode: defaultMode } } },
      '#withSources':: d.fn(help='"sources is the list of volume projections. Each entry in this list\\nhandles one source."', args=[d.arg(name='sources', type=d.T.array)]),
      withSources(sources): { spec+: { projectedVolumeTemplate+: { sources: if std.isArray(v=sources) then sources else [sources] } } },
      '#withSourcesMixin':: d.fn(help='"sources is the list of volume projections. Each entry in this list\\nhandles one source."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sources', type=d.T.array)]),
      withSourcesMixin(sources): { spec+: { projectedVolumeTemplate+: { sources+: if std.isArray(v=sources) then sources else [sources] } } },
    },
    '#replica':: d.obj(help='"Replica cluster configuration"'),
    replica: {
      '#withEnabled':: d.fn(help='"If replica mode is enabled, this cluster will be a replica of an\\nexisting cluster. Replica cluster can be created from a recovery\\nobject store or via streaming through pg_basebackup.\\nRefer to the Replica clusters page of the documentation for more information."', args=[d.arg(name='enabled', type=d.T.boolean)]),
      withEnabled(enabled): { spec+: { replica+: { enabled: enabled } } },
      '#withMinApplyDelay':: d.fn(help='"When replica mode is enabled, this parameter allows you to replay\\ntransactions only when the system time is at least the configured\\ntime past the commit time. This provides an opportunity to correct\\ndata loss errors. Note that when this parameter is set, a promotion\\ntoken cannot be used."', args=[d.arg(name='minApplyDelay', type=d.T.string)]),
      withMinApplyDelay(minApplyDelay): { spec+: { replica+: { minApplyDelay: minApplyDelay } } },
      '#withPrimary':: d.fn(help='"Primary defines which Cluster is defined to be the primary in the distributed PostgreSQL cluster, based on the\\ntopology specified in externalClusters"', args=[d.arg(name='primary', type=d.T.string)]),
      withPrimary(primary): { spec+: { replica+: { primary: primary } } },
      '#withPromotionToken':: d.fn(help='"A demotion token generated by an external cluster used to\\ncheck if the promotion requirements are met."', args=[d.arg(name='promotionToken', type=d.T.string)]),
      withPromotionToken(promotionToken): { spec+: { replica+: { promotionToken: promotionToken } } },
      '#withSelf':: d.fn(help='"Self defines the name of this cluster. It is used to determine if this is a primary\\nor a replica cluster, comparing it with `primary`"', args=[d.arg(name='Self', type=d.T.string)]),
      withSelf(Self): { spec+: { replica+: { 'self': Self } } },
      '#withSource':: d.fn(help='"The name of the external cluster which is the replication origin"', args=[d.arg(name='source', type=d.T.string)]),
      withSource(source): { spec+: { replica+: { source: source } } },
    },
    '#replicationSlots':: d.obj(help='"Replication slots management configuration"'),
    replicationSlots: {
      '#highAvailability':: d.obj(help='"Replication slots for high availability configuration"'),
      highAvailability: {
        '#withEnabled':: d.fn(help='"If enabled (default), the operator will automatically manage replication slots\\non the primary instance and use them in streaming replication\\nconnections with all the standby instances that are part of the HA\\ncluster. If disabled, the operator will not take advantage\\nof replication slots in streaming connections with the replicas.\\nThis feature also controls replication slots in replica cluster,\\nfrom the designated primary to its cascading replicas."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { spec+: { replicationSlots+: { highAvailability+: { enabled: enabled } } } },
        '#withSlotPrefix':: d.fn(help='"Prefix for replication slots managed by the operator for HA.\\nIt may only contain lower case letters, numbers, and the underscore character.\\nThis can only be set at creation time. By default set to `_cnpg_`."', args=[d.arg(name='slotPrefix', type=d.T.string)]),
        withSlotPrefix(slotPrefix): { spec+: { replicationSlots+: { highAvailability+: { slotPrefix: slotPrefix } } } },
      },
      '#synchronizeReplicas':: d.obj(help='"Configures the synchronization of the user defined physical replication slots"'),
      synchronizeReplicas: {
        '#withEnabled':: d.fn(help='"When set to true, every replication slot that is on the primary is synchronized on each standby"', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { spec+: { replicationSlots+: { synchronizeReplicas+: { enabled: enabled } } } },
        '#withExcludePatterns':: d.fn(help='"List of regular expression patterns to match the names of replication slots to be excluded (by default empty)"', args=[d.arg(name='excludePatterns', type=d.T.array)]),
        withExcludePatterns(excludePatterns): { spec+: { replicationSlots+: { synchronizeReplicas+: { excludePatterns: if std.isArray(v=excludePatterns) then excludePatterns else [excludePatterns] } } } },
        '#withExcludePatternsMixin':: d.fn(help='"List of regular expression patterns to match the names of replication slots to be excluded (by default empty)"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='excludePatterns', type=d.T.array)]),
        withExcludePatternsMixin(excludePatterns): { spec+: { replicationSlots+: { synchronizeReplicas+: { excludePatterns+: if std.isArray(v=excludePatterns) then excludePatterns else [excludePatterns] } } } },
      },
      '#withUpdateInterval':: d.fn(help='"Standby will update the status of the local replication slots\\nevery `updateInterval` seconds (default 30)."', args=[d.arg(name='updateInterval', type=d.T.integer)]),
      withUpdateInterval(updateInterval): { spec+: { replicationSlots+: { updateInterval: updateInterval } } },
    },
    '#resources':: d.obj(help='"Resources requirements of every generated Pod. Please refer to\\nhttps://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\\nfor more information."'),
    resources: {
      '#claims':: d.obj(help='"Claims lists the names of resources, defined in spec.resourceClaims,\\nthat are used by this container.\\n\\nThis is an alpha field and requires enabling the\\nDynamicResourceAllocation feature gate.\\n\\nThis field is immutable. It can only be set for containers."'),
      claims: {
        '#withName':: d.fn(help='"Name must match the name of one entry in pod.spec.resourceClaims of\\nthe Pod where this field is used. It makes that resource available\\ninside a container."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withRequest':: d.fn(help='"Request is the name chosen for a request in the referenced claim.\\nIf empty, everything from the claim is made available, otherwise\\nonly the result of this request."', args=[d.arg(name='request', type=d.T.string)]),
        withRequest(request): { request: request },
      },
      '#withClaims':: d.fn(help='"Claims lists the names of resources, defined in spec.resourceClaims,\\nthat are used by this container.\\n\\nThis is an alpha field and requires enabling the\\nDynamicResourceAllocation feature gate.\\n\\nThis field is immutable. It can only be set for containers."', args=[d.arg(name='claims', type=d.T.array)]),
      withClaims(claims): { spec+: { resources+: { claims: if std.isArray(v=claims) then claims else [claims] } } },
      '#withClaimsMixin':: d.fn(help='"Claims lists the names of resources, defined in spec.resourceClaims,\\nthat are used by this container.\\n\\nThis is an alpha field and requires enabling the\\nDynamicResourceAllocation feature gate.\\n\\nThis field is immutable. It can only be set for containers."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='claims', type=d.T.array)]),
      withClaimsMixin(claims): { spec+: { resources+: { claims+: if std.isArray(v=claims) then claims else [claims] } } },
      '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='limits', type=d.T.object)]),
      withLimits(limits): { spec+: { resources+: { limits: limits } } },
      '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
      withLimitsMixin(limits): { spec+: { resources+: { limits+: limits } } },
      '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='requests', type=d.T.object)]),
      withRequests(requests): { spec+: { resources+: { requests: requests } } },
      '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
      withRequestsMixin(requests): { spec+: { resources+: { requests+: requests } } },
    },
    '#seccompProfile':: d.obj(help='"The SeccompProfile applied to every Pod and Container.\\nDefaults to: `RuntimeDefault`"'),
    seccompProfile: {
      '#withLocalhostProfile':: d.fn(help="\"localhostProfile indicates a profile defined in a file on the node should be used.\\nThe profile must be preconfigured on the node to work.\\nMust be a descending path, relative to the kubelet's configured seccomp profile location.\\nMust be set if type is \\\"Localhost\\\". Must NOT be set for any other type.\"", args=[d.arg(name='localhostProfile', type=d.T.string)]),
      withLocalhostProfile(localhostProfile): { spec+: { seccompProfile+: { localhostProfile: localhostProfile } } },
      '#withType':: d.fn(help='"type indicates which kind of seccomp profile will be applied.\\nValid options are:\\n\\nLocalhost - a profile defined in a file on the node should be used.\\nRuntimeDefault - the container runtime default profile should be used.\\nUnconfined - no profile should be applied."', args=[d.arg(name='type', type=d.T.string)]),
      withType(type): { spec+: { seccompProfile+: { type: type } } },
    },
    '#serviceAccountTemplate':: d.obj(help='"Configure the generation of the service account"'),
    serviceAccountTemplate: {
      '#metadata':: d.obj(help='"Metadata are the metadata to be used for the generated\\nservice account"'),
      metadata: {
        '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be\\nset by external tools to store and retrieve arbitrary metadata. They are not\\nqueryable and should be preserved when modifying objects.\\nMore info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotations(annotations): { spec+: { serviceAccountTemplate+: { metadata+: { annotations: annotations } } } },
        '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be\\nset by external tools to store and retrieve arbitrary metadata. They are not\\nqueryable and should be preserved when modifying objects.\\nMore info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
        withAnnotationsMixin(annotations): { spec+: { serviceAccountTemplate+: { metadata+: { annotations+: annotations } } } },
        '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize\\n(scope and select) objects. May match selectors of replication controllers\\nand services.\\nMore info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
        withLabels(labels): { spec+: { serviceAccountTemplate+: { metadata+: { labels: labels } } } },
        '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize\\n(scope and select) objects. May match selectors of replication controllers\\nand services.\\nMore info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
        withLabelsMixin(labels): { spec+: { serviceAccountTemplate+: { metadata+: { labels+: labels } } } },
        '#withName':: d.fn(help='"The name of the resource. Only supported for certain types"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { serviceAccountTemplate+: { metadata+: { name: name } } } },
      },
    },
    '#storage':: d.obj(help='"Configuration of the storage of the instances"'),
    storage: {
      '#pvcTemplate':: d.obj(help='"Template to be used to generate the Persistent Volume Claim"'),
      pvcTemplate: {
        '#dataSource':: d.obj(help='"dataSource field can be used to specify either:\\n* An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)\\n* An existing PVC (PersistentVolumeClaim)\\nIf the provisioner or an external controller can support the specified data source,\\nit will create a new volume based on the contents of the specified data source.\\nWhen the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,\\nand dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.\\nIf the namespace is specified, then dataSourceRef will not be copied to dataSource."'),
        dataSource: {
          '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
          withApiGroup(apiGroup): { spec+: { storage+: { pvcTemplate+: { dataSource+: { apiGroup: apiGroup } } } } },
          '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
          withKind(kind): { spec+: { storage+: { pvcTemplate+: { dataSource+: { kind: kind } } } } },
          '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { storage+: { pvcTemplate+: { dataSource+: { name: name } } } } },
        },
        '#dataSourceRef':: d.obj(help="\"dataSourceRef specifies the object from which to populate the volume with data, if a non-empty\\nvolume is desired. This may be any object from a non-empty API group (non\\ncore object) or a PersistentVolumeClaim object.\\nWhen this field is specified, volume binding will only succeed if the type of\\nthe specified object matches some installed volume populator or dynamic\\nprovisioner.\\nThis field will replace the functionality of the dataSource field and as such\\nif both fields are non-empty, they must have the same value. For backwards\\ncompatibility, when namespace isn't specified in dataSourceRef,\\nboth fields (dataSource and dataSourceRef) will be set to the same\\nvalue automatically if one of them is empty and the other is non-empty.\\nWhen namespace is specified in dataSourceRef,\\ndataSource isn't set to the same value and must be empty.\\nThere are three important differences between dataSource and dataSourceRef:\\n* While dataSource only allows two specific types of objects, dataSourceRef\\n  allows any non-core object, as well as PersistentVolumeClaim objects.\\n* While dataSource ignores disallowed values (dropping them), dataSourceRef\\n  preserves all values, and generates an error if a disallowed value is\\n  specified.\\n* While dataSource only allows local objects, dataSourceRef allows objects\\n  in any namespaces.\\n(Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.\\n(Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\""),
        dataSourceRef: {
          '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
          withApiGroup(apiGroup): { spec+: { storage+: { pvcTemplate+: { dataSourceRef+: { apiGroup: apiGroup } } } } },
          '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
          withKind(kind): { spec+: { storage+: { pvcTemplate+: { dataSourceRef+: { kind: kind } } } } },
          '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { storage+: { pvcTemplate+: { dataSourceRef+: { name: name } } } } },
          '#withNamespace':: d.fn(help="\"Namespace is the namespace of resource being referenced\\nNote that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.\\n(Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\"", args=[d.arg(name='namespace', type=d.T.string)]),
          withNamespace(namespace): { spec+: { storage+: { pvcTemplate+: { dataSourceRef+: { namespace: namespace } } } } },
        },
        '#resources':: d.obj(help='"resources represents the minimum resources the volume should have.\\nIf RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements\\nthat are lower than previous value but must still be higher than capacity recorded in the\\nstatus field of the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"'),
        resources: {
          '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='limits', type=d.T.object)]),
          withLimits(limits): { spec+: { storage+: { pvcTemplate+: { resources+: { limits: limits } } } } },
          '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
          withLimitsMixin(limits): { spec+: { storage+: { pvcTemplate+: { resources+: { limits+: limits } } } } },
          '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='requests', type=d.T.object)]),
          withRequests(requests): { spec+: { storage+: { pvcTemplate+: { resources+: { requests: requests } } } } },
          '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
          withRequestsMixin(requests): { spec+: { storage+: { pvcTemplate+: { resources+: { requests+: requests } } } } },
        },
        '#selector':: d.obj(help='"selector is a label query over volumes to consider for binding."'),
        selector: {
          '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
          matchExpressions: {
            '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
            withOperator(operator): { operator: operator },
            '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
            withValues(values): { values: if std.isArray(v=values) then values else [values] },
            '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
            withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
          },
          '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
          withMatchExpressions(matchExpressions): { spec+: { storage+: { pvcTemplate+: { selector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } },
          '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
          withMatchExpressionsMixin(matchExpressions): { spec+: { storage+: { pvcTemplate+: { selector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } },
          '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { spec+: { storage+: { pvcTemplate+: { selector+: { matchLabels: matchLabels } } } } },
          '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { spec+: { storage+: { pvcTemplate+: { selector+: { matchLabels+: matchLabels } } } } },
        },
        '#withAccessModes':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"', args=[d.arg(name='accessModes', type=d.T.array)]),
        withAccessModes(accessModes): { spec+: { storage+: { pvcTemplate+: { accessModes: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } },
        '#withAccessModesMixin':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='accessModes', type=d.T.array)]),
        withAccessModesMixin(accessModes): { spec+: { storage+: { pvcTemplate+: { accessModes+: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } },
        '#withStorageClassName':: d.fn(help='"storageClassName is the name of the StorageClass required by the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"', args=[d.arg(name='storageClassName', type=d.T.string)]),
        withStorageClassName(storageClassName): { spec+: { storage+: { pvcTemplate+: { storageClassName: storageClassName } } } },
        '#withVolumeAttributesClassName':: d.fn(help="\"volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.\\nIf specified, the CSI driver will create or update the volume with the attributes defined\\nin the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,\\nit can be changed after the claim is created. An empty string value means that no VolumeAttributesClass\\nwill be applied to the claim but it's not allowed to reset this field to empty string once it is set.\\nIf unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass\\nwill be set by the persistentvolume controller if it exists.\\nIf the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be\\nset to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource\\nexists.\\nMore info: https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\\n(Beta) Using this field requires the VolumeAttributesClass feature gate to be enabled (off by default).\"", args=[d.arg(name='volumeAttributesClassName', type=d.T.string)]),
        withVolumeAttributesClassName(volumeAttributesClassName): { spec+: { storage+: { pvcTemplate+: { volumeAttributesClassName: volumeAttributesClassName } } } },
        '#withVolumeMode':: d.fn(help='"volumeMode defines what type of volume is required by the claim.\\nValue of Filesystem is implied when not included in claim spec."', args=[d.arg(name='volumeMode', type=d.T.string)]),
        withVolumeMode(volumeMode): { spec+: { storage+: { pvcTemplate+: { volumeMode: volumeMode } } } },
        '#withVolumeName':: d.fn(help='"volumeName is the binding reference to the PersistentVolume backing this claim."', args=[d.arg(name='volumeName', type=d.T.string)]),
        withVolumeName(volumeName): { spec+: { storage+: { pvcTemplate+: { volumeName: volumeName } } } },
      },
      '#withResizeInUseVolumes':: d.fn(help='"Resize existent PVCs, defaults to true"', args=[d.arg(name='resizeInUseVolumes', type=d.T.boolean)]),
      withResizeInUseVolumes(resizeInUseVolumes): { spec+: { storage+: { resizeInUseVolumes: resizeInUseVolumes } } },
      '#withSize':: d.fn(help='"Size of the storage. Required if not already specified in the PVC template.\\nChanges to this field are automatically reapplied to the created PVCs.\\nSize cannot be decreased."', args=[d.arg(name='size', type=d.T.string)]),
      withSize(size): { spec+: { storage+: { size: size } } },
      '#withStorageClass':: d.fn(help='"StorageClass to use for PVCs. Applied after\\nevaluating the PVC template, if available.\\nIf not specified, the generated PVCs will use the\\ndefault storage class"', args=[d.arg(name='storageClass', type=d.T.string)]),
      withStorageClass(storageClass): { spec+: { storage+: { storageClass: storageClass } } },
    },
    '#superuserSecret':: d.obj(help='"The secret containing the superuser password. If not defined a new\\nsecret will be created with a randomly generated password"'),
    superuserSecret: {
      '#withName':: d.fn(help='"Name of the referent."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { superuserSecret+: { name: name } } },
    },
    '#tablespaces':: d.obj(help='"The tablespaces configuration"'),
    tablespaces: {
      '#owner':: d.obj(help='"Owner is the PostgreSQL user owning the tablespace"'),
      owner: {
        '#withName':: d.fn(help='', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { owner+: { name: name } },
      },
      '#storage':: d.obj(help='"The storage configuration for the tablespace"'),
      storage: {
        '#pvcTemplate':: d.obj(help='"Template to be used to generate the Persistent Volume Claim"'),
        pvcTemplate: {
          '#dataSource':: d.obj(help='"dataSource field can be used to specify either:\\n* An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)\\n* An existing PVC (PersistentVolumeClaim)\\nIf the provisioner or an external controller can support the specified data source,\\nit will create a new volume based on the contents of the specified data source.\\nWhen the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,\\nand dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.\\nIf the namespace is specified, then dataSourceRef will not be copied to dataSource."'),
          dataSource: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { storage+: { pvcTemplate+: { dataSource+: { apiGroup: apiGroup } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { storage+: { pvcTemplate+: { dataSource+: { kind: kind } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { storage+: { pvcTemplate+: { dataSource+: { name: name } } } },
          },
          '#dataSourceRef':: d.obj(help="\"dataSourceRef specifies the object from which to populate the volume with data, if a non-empty\\nvolume is desired. This may be any object from a non-empty API group (non\\ncore object) or a PersistentVolumeClaim object.\\nWhen this field is specified, volume binding will only succeed if the type of\\nthe specified object matches some installed volume populator or dynamic\\nprovisioner.\\nThis field will replace the functionality of the dataSource field and as such\\nif both fields are non-empty, they must have the same value. For backwards\\ncompatibility, when namespace isn't specified in dataSourceRef,\\nboth fields (dataSource and dataSourceRef) will be set to the same\\nvalue automatically if one of them is empty and the other is non-empty.\\nWhen namespace is specified in dataSourceRef,\\ndataSource isn't set to the same value and must be empty.\\nThere are three important differences between dataSource and dataSourceRef:\\n* While dataSource only allows two specific types of objects, dataSourceRef\\n  allows any non-core object, as well as PersistentVolumeClaim objects.\\n* While dataSource ignores disallowed values (dropping them), dataSourceRef\\n  preserves all values, and generates an error if a disallowed value is\\n  specified.\\n* While dataSource only allows local objects, dataSourceRef allows objects\\n  in any namespaces.\\n(Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.\\n(Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\""),
          dataSourceRef: {
            '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
            withApiGroup(apiGroup): { storage+: { pvcTemplate+: { dataSourceRef+: { apiGroup: apiGroup } } } },
            '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
            withKind(kind): { storage+: { pvcTemplate+: { dataSourceRef+: { kind: kind } } } },
            '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { storage+: { pvcTemplate+: { dataSourceRef+: { name: name } } } },
            '#withNamespace':: d.fn(help="\"Namespace is the namespace of resource being referenced\\nNote that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.\\n(Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\"", args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { storage+: { pvcTemplate+: { dataSourceRef+: { namespace: namespace } } } },
          },
          '#resources':: d.obj(help='"resources represents the minimum resources the volume should have.\\nIf RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements\\nthat are lower than previous value but must still be higher than capacity recorded in the\\nstatus field of the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"'),
          resources: {
            '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='limits', type=d.T.object)]),
            withLimits(limits): { storage+: { pvcTemplate+: { resources+: { limits: limits } } } },
            '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
            withLimitsMixin(limits): { storage+: { pvcTemplate+: { resources+: { limits+: limits } } } },
            '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='requests', type=d.T.object)]),
            withRequests(requests): { storage+: { pvcTemplate+: { resources+: { requests: requests } } } },
            '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
            withRequestsMixin(requests): { storage+: { pvcTemplate+: { resources+: { requests+: requests } } } },
          },
          '#selector':: d.obj(help='"selector is a label query over volumes to consider for binding."'),
          selector: {
            '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
            matchExpressions: {
              '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
              withKey(key): { key: key },
              '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
              withOperator(operator): { operator: operator },
              '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
              withValues(values): { values: if std.isArray(v=values) then values else [values] },
              '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
              withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
            },
            '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressions(matchExpressions): { storage+: { pvcTemplate+: { selector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
            '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
            withMatchExpressionsMixin(matchExpressions): { storage+: { pvcTemplate+: { selector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } },
            '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { storage+: { pvcTemplate+: { selector+: { matchLabels: matchLabels } } } },
            '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { storage+: { pvcTemplate+: { selector+: { matchLabels+: matchLabels } } } },
          },
          '#withAccessModes':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"', args=[d.arg(name='accessModes', type=d.T.array)]),
          withAccessModes(accessModes): { storage+: { pvcTemplate+: { accessModes: if std.isArray(v=accessModes) then accessModes else [accessModes] } } },
          '#withAccessModesMixin':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='accessModes', type=d.T.array)]),
          withAccessModesMixin(accessModes): { storage+: { pvcTemplate+: { accessModes+: if std.isArray(v=accessModes) then accessModes else [accessModes] } } },
          '#withStorageClassName':: d.fn(help='"storageClassName is the name of the StorageClass required by the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"', args=[d.arg(name='storageClassName', type=d.T.string)]),
          withStorageClassName(storageClassName): { storage+: { pvcTemplate+: { storageClassName: storageClassName } } },
          '#withVolumeAttributesClassName':: d.fn(help="\"volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.\\nIf specified, the CSI driver will create or update the volume with the attributes defined\\nin the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,\\nit can be changed after the claim is created. An empty string value means that no VolumeAttributesClass\\nwill be applied to the claim but it's not allowed to reset this field to empty string once it is set.\\nIf unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass\\nwill be set by the persistentvolume controller if it exists.\\nIf the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be\\nset to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource\\nexists.\\nMore info: https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\\n(Beta) Using this field requires the VolumeAttributesClass feature gate to be enabled (off by default).\"", args=[d.arg(name='volumeAttributesClassName', type=d.T.string)]),
          withVolumeAttributesClassName(volumeAttributesClassName): { storage+: { pvcTemplate+: { volumeAttributesClassName: volumeAttributesClassName } } },
          '#withVolumeMode':: d.fn(help='"volumeMode defines what type of volume is required by the claim.\\nValue of Filesystem is implied when not included in claim spec."', args=[d.arg(name='volumeMode', type=d.T.string)]),
          withVolumeMode(volumeMode): { storage+: { pvcTemplate+: { volumeMode: volumeMode } } },
          '#withVolumeName':: d.fn(help='"volumeName is the binding reference to the PersistentVolume backing this claim."', args=[d.arg(name='volumeName', type=d.T.string)]),
          withVolumeName(volumeName): { storage+: { pvcTemplate+: { volumeName: volumeName } } },
        },
        '#withResizeInUseVolumes':: d.fn(help='"Resize existent PVCs, defaults to true"', args=[d.arg(name='resizeInUseVolumes', type=d.T.boolean)]),
        withResizeInUseVolumes(resizeInUseVolumes): { storage+: { resizeInUseVolumes: resizeInUseVolumes } },
        '#withSize':: d.fn(help='"Size of the storage. Required if not already specified in the PVC template.\\nChanges to this field are automatically reapplied to the created PVCs.\\nSize cannot be decreased."', args=[d.arg(name='size', type=d.T.string)]),
        withSize(size): { storage+: { size: size } },
        '#withStorageClass':: d.fn(help='"StorageClass to use for PVCs. Applied after\\nevaluating the PVC template, if available.\\nIf not specified, the generated PVCs will use the\\ndefault storage class"', args=[d.arg(name='storageClass', type=d.T.string)]),
        withStorageClass(storageClass): { storage+: { storageClass: storageClass } },
      },
      '#withName':: d.fn(help='"The name of the tablespace"', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { name: name },
      '#withTemporary':: d.fn(help='"When set to true, the tablespace will be added as a `temp_tablespaces`\\nentry in PostgreSQL, and will be available to automatically house temp\\ndatabase objects, or other temporary files. Please refer to PostgreSQL\\ndocumentation for more information on the `temp_tablespaces` GUC."', args=[d.arg(name='temporary', type=d.T.boolean)]),
      withTemporary(temporary): { temporary: temporary },
    },
    '#topologySpreadConstraints':: d.obj(help='"TopologySpreadConstraints specifies how to spread matching pods among the given topology.\\nMore info:\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/"'),
    topologySpreadConstraints: {
      '#labelSelector':: d.obj(help='"LabelSelector is used to find matching pods.\\nPods that match this label selector are counted to determine the number of pods\\nin their corresponding topology domain."'),
      labelSelector: {
        '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
        matchExpressions: {
          '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
          withKey(key): { key: key },
          '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
          withOperator(operator): { operator: operator },
          '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
          withValues(values): { values: if std.isArray(v=values) then values else [values] },
          '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
          withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
        },
        '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressions(matchExpressions): { labelSelector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
        '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
        withMatchExpressionsMixin(matchExpressions): { labelSelector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } },
        '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { labelSelector+: { matchLabels: matchLabels } },
        '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { labelSelector+: { matchLabels+: matchLabels } },
      },
      '#withMatchLabelKeys':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select the pods over which\\nspreading will be calculated. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are ANDed with labelSelector\\nto select the group of existing pods over which spreading will be calculated\\nfor the incoming pod. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.\\nMatchLabelKeys cannot be set when LabelSelector isn't set.\\nKeys that don't exist in the incoming pod labels will\\nbe ignored. A null or empty list means only match against labelSelector.\\n\\nThis is a beta field and requires the MatchLabelKeysInPodTopologySpread feature gate to be enabled (enabled by default).\"", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
      withMatchLabelKeys(matchLabelKeys): { matchLabelKeys: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
      '#withMatchLabelKeysMixin':: d.fn(help="\"MatchLabelKeys is a set of pod label keys to select the pods over which\\nspreading will be calculated. The keys are used to lookup values from the\\nincoming pod labels, those key-value labels are ANDed with labelSelector\\nto select the group of existing pods over which spreading will be calculated\\nfor the incoming pod. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector.\\nMatchLabelKeys cannot be set when LabelSelector isn't set.\\nKeys that don't exist in the incoming pod labels will\\nbe ignored. A null or empty list means only match against labelSelector.\\n\\nThis is a beta field and requires the MatchLabelKeysInPodTopologySpread feature gate to be enabled (enabled by default).\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='matchLabelKeys', type=d.T.array)]),
      withMatchLabelKeysMixin(matchLabelKeys): { matchLabelKeys+: if std.isArray(v=matchLabelKeys) then matchLabelKeys else [matchLabelKeys] },
      '#withMaxSkew':: d.fn(help="\"MaxSkew describes the degree to which pods may be unevenly distributed.\\nWhen `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference\\nbetween the number of matching pods in the target topology and the global minimum.\\nThe global minimum is the minimum number of matching pods in an eligible domain\\nor zero if the number of eligible domains is less than MinDomains.\\nFor example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same\\nlabelSelector spread as 2/2/1:\\nIn this case, the global minimum is 1.\\n| zone1 | zone2 | zone3 |\\n|  P P  |  P P  |   P   |\\n- if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2;\\nscheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2)\\nviolate MaxSkew(1).\\n- if MaxSkew is 2, incoming pod can be scheduled onto any zone.\\nWhen `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence\\nto topologies that satisfy it.\\nIt's a required field. Default value is 1 and 0 is not allowed.\"", args=[d.arg(name='maxSkew', type=d.T.integer)]),
      withMaxSkew(maxSkew): { maxSkew: maxSkew },
      '#withMinDomains':: d.fn(help="\"MinDomains indicates a minimum number of eligible domains.\\nWhen the number of eligible domains with matching topology keys is less than minDomains,\\nPod Topology Spread treats \\\"global minimum\\\" as 0, and then the calculation of Skew is performed.\\nAnd when the number of eligible domains with matching topology keys equals or greater than minDomains,\\nthis value has no effect on scheduling.\\nAs a result, when the number of eligible domains is less than minDomains,\\nscheduler won't schedule more than maxSkew Pods to those domains.\\nIf value is nil, the constraint behaves as if MinDomains is equal to 1.\\nValid values are integers greater than 0.\\nWhen value is not nil, WhenUnsatisfiable must be DoNotSchedule.\\n\\nFor example, in a 3-zone cluster, MaxSkew is set to 2, MinDomains is set to 5 and pods with the same\\nlabelSelector spread as 2/2/2:\\n| zone1 | zone2 | zone3 |\\n|  P P  |  P P  |  P P  |\\nThe number of domains is less than 5(MinDomains), so \\\"global minimum\\\" is treated as 0.\\nIn this situation, new pod with the same labelSelector cannot be scheduled,\\nbecause computed skew will be 3(3 - 0) if new Pod is scheduled to any of the three zones,\\nit will violate MaxSkew.\"", args=[d.arg(name='minDomains', type=d.T.integer)]),
      withMinDomains(minDomains): { minDomains: minDomains },
      '#withNodeAffinityPolicy':: d.fn(help="\"NodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector\\nwhen calculating pod topology spread skew. Options are:\\n- Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.\\n- Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.\\n\\nIf this value is nil, the behavior is equivalent to the Honor policy.\\nThis is a beta-level feature default enabled by the NodeInclusionPolicyInPodTopologySpread feature flag.\"", args=[d.arg(name='nodeAffinityPolicy', type=d.T.string)]),
      withNodeAffinityPolicy(nodeAffinityPolicy): { nodeAffinityPolicy: nodeAffinityPolicy },
      '#withNodeTaintsPolicy':: d.fn(help='"NodeTaintsPolicy indicates how we will treat node taints when calculating\\npod topology spread skew. Options are:\\n- Honor: nodes without taints, along with tainted nodes for which the incoming pod\\nhas a toleration, are included.\\n- Ignore: node taints are ignored. All nodes are included.\\n\\nIf this value is nil, the behavior is equivalent to the Ignore policy.\\nThis is a beta-level feature default enabled by the NodeInclusionPolicyInPodTopologySpread feature flag."', args=[d.arg(name='nodeTaintsPolicy', type=d.T.string)]),
      withNodeTaintsPolicy(nodeTaintsPolicy): { nodeTaintsPolicy: nodeTaintsPolicy },
      '#withTopologyKey':: d.fn(help="\"TopologyKey is the key of node labels. Nodes that have a label with this key\\nand identical values are considered to be in the same topology.\\nWe consider each \u003ckey, value\u003e as a \\\"bucket\\\", and try to put balanced number\\nof pods into each bucket.\\nWe define a domain as a particular instance of a topology.\\nAlso, we define an eligible domain as a domain whose nodes meet the requirements of\\nnodeAffinityPolicy and nodeTaintsPolicy.\\ne.g. If TopologyKey is \\\"kubernetes.io/hostname\\\", each Node is a domain of that topology.\\nAnd, if TopologyKey is \\\"topology.kubernetes.io/zone\\\", each zone is a domain of that topology.\\nIt's a required field.\"", args=[d.arg(name='topologyKey', type=d.T.string)]),
      withTopologyKey(topologyKey): { topologyKey: topologyKey },
      '#withWhenUnsatisfiable':: d.fn(help="\"WhenUnsatisfiable indicates how to deal with a pod if it doesn't satisfy\\nthe spread constraint.\\n- DoNotSchedule (default) tells the scheduler not to schedule it.\\n- ScheduleAnyway tells the scheduler to schedule the pod in any location,\\n  but giving higher precedence to topologies that would help reduce the\\n  skew.\\nA constraint is considered \\\"Unsatisfiable\\\" for an incoming pod\\nif and only if every possible node assignment for that pod would violate\\n\\\"MaxSkew\\\" on some topology.\\nFor example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same\\nlabelSelector spread as 3/1/1:\\n| zone1 | zone2 | zone3 |\\n| P P P |   P   |   P   |\\nIf WhenUnsatisfiable is set to DoNotSchedule, incoming pod can only be scheduled\\nto zone2(zone3) to become 3/2/1(3/1/2) as ActualSkew(2-1) on zone2(zone3) satisfies\\nMaxSkew(1). In other words, the cluster can still be imbalanced, but scheduler\\nwon't make it *more* imbalanced.\\nIt's a required field.\"", args=[d.arg(name='whenUnsatisfiable', type=d.T.string)]),
      withWhenUnsatisfiable(whenUnsatisfiable): { whenUnsatisfiable: whenUnsatisfiable },
    },
    '#walStorage':: d.obj(help='"Configuration of the storage for PostgreSQL WAL (Write-Ahead Log)"'),
    walStorage: {
      '#pvcTemplate':: d.obj(help='"Template to be used to generate the Persistent Volume Claim"'),
      pvcTemplate: {
        '#dataSource':: d.obj(help='"dataSource field can be used to specify either:\\n* An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot)\\n* An existing PVC (PersistentVolumeClaim)\\nIf the provisioner or an external controller can support the specified data source,\\nit will create a new volume based on the contents of the specified data source.\\nWhen the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef,\\nand dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified.\\nIf the namespace is specified, then dataSourceRef will not be copied to dataSource."'),
        dataSource: {
          '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
          withApiGroup(apiGroup): { spec+: { walStorage+: { pvcTemplate+: { dataSource+: { apiGroup: apiGroup } } } } },
          '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
          withKind(kind): { spec+: { walStorage+: { pvcTemplate+: { dataSource+: { kind: kind } } } } },
          '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { walStorage+: { pvcTemplate+: { dataSource+: { name: name } } } } },
        },
        '#dataSourceRef':: d.obj(help="\"dataSourceRef specifies the object from which to populate the volume with data, if a non-empty\\nvolume is desired. This may be any object from a non-empty API group (non\\ncore object) or a PersistentVolumeClaim object.\\nWhen this field is specified, volume binding will only succeed if the type of\\nthe specified object matches some installed volume populator or dynamic\\nprovisioner.\\nThis field will replace the functionality of the dataSource field and as such\\nif both fields are non-empty, they must have the same value. For backwards\\ncompatibility, when namespace isn't specified in dataSourceRef,\\nboth fields (dataSource and dataSourceRef) will be set to the same\\nvalue automatically if one of them is empty and the other is non-empty.\\nWhen namespace is specified in dataSourceRef,\\ndataSource isn't set to the same value and must be empty.\\nThere are three important differences between dataSource and dataSourceRef:\\n* While dataSource only allows two specific types of objects, dataSourceRef\\n  allows any non-core object, as well as PersistentVolumeClaim objects.\\n* While dataSource ignores disallowed values (dropping them), dataSourceRef\\n  preserves all values, and generates an error if a disallowed value is\\n  specified.\\n* While dataSource only allows local objects, dataSourceRef allows objects\\n  in any namespaces.\\n(Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled.\\n(Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\""),
        dataSourceRef: {
          '#withApiGroup':: d.fn(help='"APIGroup is the group for the resource being referenced.\\nIf APIGroup is not specified, the specified Kind must be in the core API group.\\nFor any other third-party types, APIGroup is required."', args=[d.arg(name='apiGroup', type=d.T.string)]),
          withApiGroup(apiGroup): { spec+: { walStorage+: { pvcTemplate+: { dataSourceRef+: { apiGroup: apiGroup } } } } },
          '#withKind':: d.fn(help='"Kind is the type of resource being referenced"', args=[d.arg(name='kind', type=d.T.string)]),
          withKind(kind): { spec+: { walStorage+: { pvcTemplate+: { dataSourceRef+: { kind: kind } } } } },
          '#withName':: d.fn(help='"Name is the name of resource being referenced"', args=[d.arg(name='name', type=d.T.string)]),
          withName(name): { spec+: { walStorage+: { pvcTemplate+: { dataSourceRef+: { name: name } } } } },
          '#withNamespace':: d.fn(help="\"Namespace is the namespace of resource being referenced\\nNote that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details.\\n(Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled.\"", args=[d.arg(name='namespace', type=d.T.string)]),
          withNamespace(namespace): { spec+: { walStorage+: { pvcTemplate+: { dataSourceRef+: { namespace: namespace } } } } },
        },
        '#resources':: d.obj(help='"resources represents the minimum resources the volume should have.\\nIf RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements\\nthat are lower than previous value but must still be higher than capacity recorded in the\\nstatus field of the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"'),
        resources: {
          '#withLimits':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='limits', type=d.T.object)]),
          withLimits(limits): { spec+: { walStorage+: { pvcTemplate+: { resources+: { limits: limits } } } } },
          '#withLimitsMixin':: d.fn(help='"Limits describes the maximum amount of compute resources allowed.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='limits', type=d.T.object)]),
          withLimitsMixin(limits): { spec+: { walStorage+: { pvcTemplate+: { resources+: { limits+: limits } } } } },
          '#withRequests':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"', args=[d.arg(name='requests', type=d.T.object)]),
          withRequests(requests): { spec+: { walStorage+: { pvcTemplate+: { resources+: { requests: requests } } } } },
          '#withRequestsMixin':: d.fn(help='"Requests describes the minimum amount of compute resources required.\\nIf Requests is omitted for a container, it defaults to Limits if that is explicitly specified,\\notherwise to an implementation-defined value. Requests cannot exceed Limits.\\nMore info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='requests', type=d.T.object)]),
          withRequestsMixin(requests): { spec+: { walStorage+: { pvcTemplate+: { resources+: { requests+: requests } } } } },
        },
        '#selector':: d.obj(help='"selector is a label query over volumes to consider for binding."'),
        selector: {
          '#matchExpressions':: d.obj(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."'),
          matchExpressions: {
            '#withKey':: d.fn(help='"key is the label key that the selector applies to."', args=[d.arg(name='key', type=d.T.string)]),
            withKey(key): { key: key },
            '#withOperator':: d.fn(help="\"operator represents a key's relationship to a set of values.\\nValid operators are In, NotIn, Exists and DoesNotExist.\"", args=[d.arg(name='operator', type=d.T.string)]),
            withOperator(operator): { operator: operator },
            '#withValues':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."', args=[d.arg(name='values', type=d.T.array)]),
            withValues(values): { values: if std.isArray(v=values) then values else [values] },
            '#withValuesMixin':: d.fn(help='"values is an array of string values. If the operator is In or NotIn,\\nthe values array must be non-empty. If the operator is Exists or DoesNotExist,\\nthe values array must be empty. This array is replaced during a strategic\\nmerge patch."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='values', type=d.T.array)]),
            withValuesMixin(values): { values+: if std.isArray(v=values) then values else [values] },
          },
          '#withMatchExpressions':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."', args=[d.arg(name='matchExpressions', type=d.T.array)]),
          withMatchExpressions(matchExpressions): { spec+: { walStorage+: { pvcTemplate+: { selector+: { matchExpressions: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } },
          '#withMatchExpressionsMixin':: d.fn(help='"matchExpressions is a list of label selector requirements. The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchExpressions', type=d.T.array)]),
          withMatchExpressionsMixin(matchExpressions): { spec+: { walStorage+: { pvcTemplate+: { selector+: { matchExpressions+: if std.isArray(v=matchExpressions) then matchExpressions else [matchExpressions] } } } } },
          '#withMatchLabels':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabels(matchLabels): { spec+: { walStorage+: { pvcTemplate+: { selector+: { matchLabels: matchLabels } } } } },
          '#withMatchLabelsMixin':: d.fn(help='"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels\\nmap is equivalent to an element of matchExpressions, whose key field is \\"key\\", the\\noperator is \\"In\\", and the values array contains only \\"value\\". The requirements are ANDed."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
          withMatchLabelsMixin(matchLabels): { spec+: { walStorage+: { pvcTemplate+: { selector+: { matchLabels+: matchLabels } } } } },
        },
        '#withAccessModes':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"', args=[d.arg(name='accessModes', type=d.T.array)]),
        withAccessModes(accessModes): { spec+: { walStorage+: { pvcTemplate+: { accessModes: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } },
        '#withAccessModesMixin':: d.fn(help='"accessModes contains the desired access modes the volume should have.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='accessModes', type=d.T.array)]),
        withAccessModesMixin(accessModes): { spec+: { walStorage+: { pvcTemplate+: { accessModes+: if std.isArray(v=accessModes) then accessModes else [accessModes] } } } },
        '#withStorageClassName':: d.fn(help='"storageClassName is the name of the StorageClass required by the claim.\\nMore info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"', args=[d.arg(name='storageClassName', type=d.T.string)]),
        withStorageClassName(storageClassName): { spec+: { walStorage+: { pvcTemplate+: { storageClassName: storageClassName } } } },
        '#withVolumeAttributesClassName':: d.fn(help="\"volumeAttributesClassName may be used to set the VolumeAttributesClass used by this claim.\\nIf specified, the CSI driver will create or update the volume with the attributes defined\\nin the corresponding VolumeAttributesClass. This has a different purpose than storageClassName,\\nit can be changed after the claim is created. An empty string value means that no VolumeAttributesClass\\nwill be applied to the claim but it's not allowed to reset this field to empty string once it is set.\\nIf unspecified and the PersistentVolumeClaim is unbound, the default VolumeAttributesClass\\nwill be set by the persistentvolume controller if it exists.\\nIf the resource referred to by volumeAttributesClass does not exist, this PersistentVolumeClaim will be\\nset to a Pending state, as reflected by the modifyVolumeStatus field, until such as a resource\\nexists.\\nMore info: https://kubernetes.io/docs/concepts/storage/volume-attributes-classes/\\n(Beta) Using this field requires the VolumeAttributesClass feature gate to be enabled (off by default).\"", args=[d.arg(name='volumeAttributesClassName', type=d.T.string)]),
        withVolumeAttributesClassName(volumeAttributesClassName): { spec+: { walStorage+: { pvcTemplate+: { volumeAttributesClassName: volumeAttributesClassName } } } },
        '#withVolumeMode':: d.fn(help='"volumeMode defines what type of volume is required by the claim.\\nValue of Filesystem is implied when not included in claim spec."', args=[d.arg(name='volumeMode', type=d.T.string)]),
        withVolumeMode(volumeMode): { spec+: { walStorage+: { pvcTemplate+: { volumeMode: volumeMode } } } },
        '#withVolumeName':: d.fn(help='"volumeName is the binding reference to the PersistentVolume backing this claim."', args=[d.arg(name='volumeName', type=d.T.string)]),
        withVolumeName(volumeName): { spec+: { walStorage+: { pvcTemplate+: { volumeName: volumeName } } } },
      },
      '#withResizeInUseVolumes':: d.fn(help='"Resize existent PVCs, defaults to true"', args=[d.arg(name='resizeInUseVolumes', type=d.T.boolean)]),
      withResizeInUseVolumes(resizeInUseVolumes): { spec+: { walStorage+: { resizeInUseVolumes: resizeInUseVolumes } } },
      '#withSize':: d.fn(help='"Size of the storage. Required if not already specified in the PVC template.\\nChanges to this field are automatically reapplied to the created PVCs.\\nSize cannot be decreased."', args=[d.arg(name='size', type=d.T.string)]),
      withSize(size): { spec+: { walStorage+: { size: size } } },
      '#withStorageClass':: d.fn(help='"StorageClass to use for PVCs. Applied after\\nevaluating the PVC template, if available.\\nIf not specified, the generated PVCs will use the\\ndefault storage class"', args=[d.arg(name='storageClass', type=d.T.string)]),
      withStorageClass(storageClass): { spec+: { walStorage+: { storageClass: storageClass } } },
    },
    '#withDescription':: d.fn(help='"Description of this PostgreSQL cluster"', args=[d.arg(name='description', type=d.T.string)]),
    withDescription(description): { spec+: { description: description } },
    '#withEnablePDB':: d.fn(help='"Manage the `PodDisruptionBudget` resources within the cluster. When\\nconfigured as `true` (default setting), the pod disruption budgets\\nwill safeguard the primary node from being terminated. Conversely,\\nsetting it to `false` will result in the absence of any\\n`PodDisruptionBudget` resource, permitting the shutdown of all nodes\\nhosting the PostgreSQL cluster. This latter configuration is\\nadvisable for any PostgreSQL cluster employed for\\ndevelopment/staging purposes."', args=[d.arg(name='enablePDB', type=d.T.boolean)]),
    withEnablePDB(enablePDB): { spec+: { enablePDB: enablePDB } },
    '#withEnableSuperuserAccess':: d.fn(help='"When this option is enabled, the operator will use the `SuperuserSecret`\\nto update the `postgres` user password (if the secret is\\nnot present, the operator will automatically create one). When this\\noption is disabled, the operator will ignore the `SuperuserSecret` content, delete\\nit when automatically created, and then blank the password of the `postgres`\\nuser by setting it to `NULL`. Disabled by default."', args=[d.arg(name='enableSuperuserAccess', type=d.T.boolean)]),
    withEnableSuperuserAccess(enableSuperuserAccess): { spec+: { enableSuperuserAccess: enableSuperuserAccess } },
    '#withEnv':: d.fn(help='"Env follows the Env format to pass environment variables\\nto the pods created in the cluster"', args=[d.arg(name='env', type=d.T.array)]),
    withEnv(env): { spec+: { env: if std.isArray(v=env) then env else [env] } },
    '#withEnvFrom':: d.fn(help='"EnvFrom follows the EnvFrom format to pass environment variables\\nsources to the pods to be used by Env"', args=[d.arg(name='envFrom', type=d.T.array)]),
    withEnvFrom(envFrom): { spec+: { envFrom: if std.isArray(v=envFrom) then envFrom else [envFrom] } },
    '#withEnvFromMixin':: d.fn(help='"EnvFrom follows the EnvFrom format to pass environment variables\\nsources to the pods to be used by Env"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='envFrom', type=d.T.array)]),
    withEnvFromMixin(envFrom): { spec+: { envFrom+: if std.isArray(v=envFrom) then envFrom else [envFrom] } },
    '#withEnvMixin':: d.fn(help='"Env follows the Env format to pass environment variables\\nto the pods created in the cluster"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='env', type=d.T.array)]),
    withEnvMixin(env): { spec+: { env+: if std.isArray(v=env) then env else [env] } },
    '#withExternalClusters':: d.fn(help='"The list of external clusters which are used in the configuration"', args=[d.arg(name='externalClusters', type=d.T.array)]),
    withExternalClusters(externalClusters): { spec+: { externalClusters: if std.isArray(v=externalClusters) then externalClusters else [externalClusters] } },
    '#withExternalClustersMixin':: d.fn(help='"The list of external clusters which are used in the configuration"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='externalClusters', type=d.T.array)]),
    withExternalClustersMixin(externalClusters): { spec+: { externalClusters+: if std.isArray(v=externalClusters) then externalClusters else [externalClusters] } },
    '#withFailoverDelay':: d.fn(help='"The amount of time (in seconds) to wait before triggering a failover\\nafter the primary PostgreSQL instance in the cluster was detected\\nto be unhealthy"', args=[d.arg(name='failoverDelay', type=d.T.integer)]),
    withFailoverDelay(failoverDelay): { spec+: { failoverDelay: failoverDelay } },
    '#withImageName':: d.fn(help='"Name of the container image, supporting both tags (`<image>:<tag>`)\\nand digests for deterministic and repeatable deployments\\n(`<image>:<tag>@sha256:<digestValue>`)"', args=[d.arg(name='imageName', type=d.T.string)]),
    withImageName(imageName): { spec+: { imageName: imageName } },
    '#withImagePullPolicy':: d.fn(help='"Image pull policy.\\nOne of `Always`, `Never` or `IfNotPresent`.\\nIf not defined, it defaults to `IfNotPresent`.\\nCannot be updated.\\nMore info: https://kubernetes.io/docs/concepts/containers/images#updating-images"', args=[d.arg(name='imagePullPolicy', type=d.T.string)]),
    withImagePullPolicy(imagePullPolicy): { spec+: { imagePullPolicy: imagePullPolicy } },
    '#withImagePullSecrets':: d.fn(help='"The list of pull secrets to be used to pull the images"', args=[d.arg(name='imagePullSecrets', type=d.T.array)]),
    withImagePullSecrets(imagePullSecrets): { spec+: { imagePullSecrets: if std.isArray(v=imagePullSecrets) then imagePullSecrets else [imagePullSecrets] } },
    '#withImagePullSecretsMixin':: d.fn(help='"The list of pull secrets to be used to pull the images"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='imagePullSecrets', type=d.T.array)]),
    withImagePullSecretsMixin(imagePullSecrets): { spec+: { imagePullSecrets+: if std.isArray(v=imagePullSecrets) then imagePullSecrets else [imagePullSecrets] } },
    '#withInstances':: d.fn(help='"Number of instances required in the cluster"', args=[d.arg(name='instances', type=d.T.integer)]),
    withInstances(instances): { spec+: { instances: instances } },
    '#withLivenessProbeTimeout':: d.fn(help='"LivenessProbeTimeout is the time (in seconds) that is allowed for a PostgreSQL instance\\nto successfully respond to the liveness probe (default 30).\\nThe Liveness probe failure threshold is derived from this value using the formula:\\nceiling(livenessProbe / 10)."', args=[d.arg(name='livenessProbeTimeout', type=d.T.integer)]),
    withLivenessProbeTimeout(livenessProbeTimeout): { spec+: { livenessProbeTimeout: livenessProbeTimeout } },
    '#withLogLevel':: d.fn(help="\"The instances' log level, one of the following values: error, warning, info (default), debug, trace\"", args=[d.arg(name='logLevel', type=d.T.string)]),
    withLogLevel(logLevel): { spec+: { logLevel: logLevel } },
    '#withMaxSyncReplicas':: d.fn(help='"The target value for the synchronous replication quorum, that can be\\ndecreased if the number of ready standbys is lower than this.\\nUndefined or 0 disable synchronous replication."', args=[d.arg(name='maxSyncReplicas', type=d.T.integer)]),
    withMaxSyncReplicas(maxSyncReplicas): { spec+: { maxSyncReplicas: maxSyncReplicas } },
    '#withMinSyncReplicas':: d.fn(help='"Minimum number of instances required in synchronous replication with the\\nprimary. Undefined or 0 allow writes to complete when no standby is\\navailable."', args=[d.arg(name='minSyncReplicas', type=d.T.integer)]),
    withMinSyncReplicas(minSyncReplicas): { spec+: { minSyncReplicas: minSyncReplicas } },
    '#withPlugins':: d.fn(help='"The plugins configuration, containing\\nany plugin to be loaded with the corresponding configuration"', args=[d.arg(name='plugins', type=d.T.array)]),
    withPlugins(plugins): { spec+: { plugins: if std.isArray(v=plugins) then plugins else [plugins] } },
    '#withPluginsMixin':: d.fn(help='"The plugins configuration, containing\\nany plugin to be loaded with the corresponding configuration"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='plugins', type=d.T.array)]),
    withPluginsMixin(plugins): { spec+: { plugins+: if std.isArray(v=plugins) then plugins else [plugins] } },
    '#withPostgresGID':: d.fn(help='"The GID of the `postgres` user inside the image, defaults to `26`"', args=[d.arg(name='postgresGID', type=d.T.integer)]),
    withPostgresGID(postgresGID): { spec+: { postgresGID: postgresGID } },
    '#withPostgresUID':: d.fn(help='"The UID of the `postgres` user inside the image, defaults to `26`"', args=[d.arg(name='postgresUID', type=d.T.integer)]),
    withPostgresUID(postgresUID): { spec+: { postgresUID: postgresUID } },
    '#withPrimaryUpdateMethod':: d.fn(help='"Method to follow to upgrade the primary server during a rolling\\nupdate procedure, after all replicas have been successfully updated:\\nit can be with a switchover (`switchover`) or in-place (`restart` - default)"', args=[d.arg(name='primaryUpdateMethod', type=d.T.string)]),
    withPrimaryUpdateMethod(primaryUpdateMethod): { spec+: { primaryUpdateMethod: primaryUpdateMethod } },
    '#withPrimaryUpdateStrategy':: d.fn(help='"Deployment strategy to follow to upgrade the primary server during a rolling\\nupdate procedure, after all replicas have been successfully updated:\\nit can be automated (`unsupervised` - default) or manual (`supervised`)"', args=[d.arg(name='primaryUpdateStrategy', type=d.T.string)]),
    withPrimaryUpdateStrategy(primaryUpdateStrategy): { spec+: { primaryUpdateStrategy: primaryUpdateStrategy } },
    '#withPriorityClassName':: d.fn(help='"Name of the priority class which will be used in every generated Pod, if the PriorityClass\\nspecified does not exist, the pod will not be able to schedule.  Please refer to\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass\\nfor more information"', args=[d.arg(name='priorityClassName', type=d.T.string)]),
    withPriorityClassName(priorityClassName): { spec+: { priorityClassName: priorityClassName } },
    '#withSchedulerName':: d.fn(help='"If specified, the pod will be dispatched by specified Kubernetes\\nscheduler. If not specified, the pod will be dispatched by the default\\nscheduler. More info:\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/"', args=[d.arg(name='schedulerName', type=d.T.string)]),
    withSchedulerName(schedulerName): { spec+: { schedulerName: schedulerName } },
    '#withSmartShutdownTimeout':: d.fn(help='"The time in seconds that controls the window of time reserved for the smart shutdown of Postgres to complete.\\nMake sure you reserve enough time for the operator to request a fast shutdown of Postgres\\n(that is: `stopDelay` - `smartShutdownTimeout`)."', args=[d.arg(name='smartShutdownTimeout', type=d.T.integer)]),
    withSmartShutdownTimeout(smartShutdownTimeout): { spec+: { smartShutdownTimeout: smartShutdownTimeout } },
    '#withStartDelay':: d.fn(help='"The time in seconds that is allowed for a PostgreSQL instance to\\nsuccessfully start up (default 3600).\\nThe startup probe failure threshold is derived from this value using the formula:\\nceiling(startDelay / 10)."', args=[d.arg(name='startDelay', type=d.T.integer)]),
    withStartDelay(startDelay): { spec+: { startDelay: startDelay } },
    '#withStopDelay':: d.fn(help='"The time in seconds that is allowed for a PostgreSQL instance to\\ngracefully shutdown (default 1800)"', args=[d.arg(name='stopDelay', type=d.T.integer)]),
    withStopDelay(stopDelay): { spec+: { stopDelay: stopDelay } },
    '#withSwitchoverDelay':: d.fn(help='"The time in seconds that is allowed for a primary PostgreSQL instance\\nto gracefully shutdown during a switchover.\\nDefault value is 3600 seconds (1 hour)."', args=[d.arg(name='switchoverDelay', type=d.T.integer)]),
    withSwitchoverDelay(switchoverDelay): { spec+: { switchoverDelay: switchoverDelay } },
    '#withTablespaces':: d.fn(help='"The tablespaces configuration"', args=[d.arg(name='tablespaces', type=d.T.array)]),
    withTablespaces(tablespaces): { spec+: { tablespaces: if std.isArray(v=tablespaces) then tablespaces else [tablespaces] } },
    '#withTablespacesMixin':: d.fn(help='"The tablespaces configuration"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tablespaces', type=d.T.array)]),
    withTablespacesMixin(tablespaces): { spec+: { tablespaces+: if std.isArray(v=tablespaces) then tablespaces else [tablespaces] } },
    '#withTopologySpreadConstraints':: d.fn(help='"TopologySpreadConstraints specifies how to spread matching pods among the given topology.\\nMore info:\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/"', args=[d.arg(name='topologySpreadConstraints', type=d.T.array)]),
    withTopologySpreadConstraints(topologySpreadConstraints): { spec+: { topologySpreadConstraints: if std.isArray(v=topologySpreadConstraints) then topologySpreadConstraints else [topologySpreadConstraints] } },
    '#withTopologySpreadConstraintsMixin':: d.fn(help='"TopologySpreadConstraints specifies how to spread matching pods among the given topology.\\nMore info:\\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='topologySpreadConstraints', type=d.T.array)]),
    withTopologySpreadConstraintsMixin(topologySpreadConstraints): { spec+: { topologySpreadConstraints+: if std.isArray(v=topologySpreadConstraints) then topologySpreadConstraints else [topologySpreadConstraints] } },
  },
  '#mixin': 'ignore',
  mixin: self,
}
